\documentclass{cslthse-msc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{algorithm, algorithmicx}
\usepackage{algpseudocode}
\usepackage[titletoc, header, page]{appendix}
\usepackage{hyperref}
\usepackage{cleveref}

\crefname{app}{Appendix}{Appendices}

\newtheorem{definition}{Definition}[chapter]
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

%\geometry{showframe}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\author{
	Philip Ståhl \\
	{\normalsize \href{mailto:ada10pst@student.lu.se}{\texttt{ada10pst@student.lu.se}}}
	\and
	Jonatan Broberg \\
    {\normalsize \href{mailto:elt11jbr@student.lu.se
}{\texttt{elt11jbr@student.lu.se}}}
}

\title{Dynamic Fault-Tolerance and Task Scheduling in Distributed Systems}
\subtitle{}
\company{Mobile and Pervasive Computing Institute (MAPCI), Lund University}
\supervisors{Björn Landfeldt, \href{mailto:bjorn.landfeldt@eit.lth.se}{\texttt{bjorn.landfeldt@eit.lth.se}}}{}
\examiner{Christian Nyberg, \href{mailto:christian.nyberg@eit.lth.se}{\texttt{christian.nyberg@eit.lth.se}}}

\date{\today}
%\date{January 16, 2015}

\acknowledgements{
We would like to thank our supervisor Björn Landfeldt and MAPCI for there input.

Maybe also:
Shubhabatra, Ericsson, Jörn, Doan, examiner Christian Nyberg. (Or simply all the people at Mapci)

%If you want to thank people, do it here, on a separate right-hand page. Both the U.S. \textit{acknowledgments} and the British \textit{acknowledgements} spellings are acceptable.
}

\theabstract{
This document describes the Master's Thesis format for the theses carried out at the Department of Computer Science, Lund University. 

%Your abstract should capture, in English, the whole thesis with focus on the problem and solution in 150 words. It should be placed on a separate right-hand page, with an additional \textit{1cm} margin on both left and right. Avoid acronyms, footnotes, and references in the abstract if possible.

%Leave a \textit{2cm} vertical space after the abstract and provide a few keywords relevant for your report. Use five to six words, of which at most two should be from the title.

}

\keywords{reliability, distributed computing, dynamic fault-tolerance, task scheduling, Poisson}

%% Only used to display font sizes
\makeatletter
\newcommand\thefontsize[1]{{#1 \f@size pt\par}}
\makeatother
%%%%%%%%%%


\begin{document}
\makefrontmatter

\chapter{Introduction} \label{ch:introduction} 
\section{Background and Motivation} \label{sec:introduction_backgroud_motivation}
Ensuring a certain level of reliability is of major concern for many cloud system providers. As cloud computing is growing rapidly and users are demanding more services and higher performance, providing fault tolerant systems and improving reliability by more sophisticated task scheduling, has become a very important and interesting research area. 

Cloud systems often consists of heterogeneous hardware and any of the often vast number of components may fail at any time. Therefore, ensuring reliability raises complexity to the resource allocation decisions and fault-tolerance mechanisms in highly dynamic distributed systems. For cloud service providers, it is necessary that this increased complexity is taken care of without putting extra burden on the user. The system should therefore ensure these properties in a seamless way.

As computational work is distributed across multiple resources, the overall reliability of the application decreases. To cope with this issue, a fault-tolerant design or error handling mechanism need to be in place. This is of particular interest for cloud service providers, as users often desire a certain level of reliability. In some cases, vendors, for example carrier providers, are obliged by law to achieve a certain level of availability or reliability. In these cases, one may need to sacrifice other quality aspects such as latency and resource usage. By using static and dynamic analysis of the infrastructure and the mean-time-between-failure for the given resources, a statistical model can be used in order to verify  that the desired level is reached. However, such a model should be dynamic, as failure rates are likely to vary over time, for example due to high or low load on the system. This is of particular importance for long running applications when requiring a certain level of reliability. Despite fulfilling the required level at the time of deployment, as the state of the system changes, the level may no longer be fulfilled.

Reliability can be increased by replicating application tasks, where all replicas perform the same computations on the same input. This allows for both increased redundancy and an easy way of detecting errors. This allows for continuing execution of the application even in case of a filing replica. Seamlessly being able to continue the execution, without losing any data is of particular interest in data stream processing.

One drawbacks of replicating a task n times, is that the resources needed increases. With $n$ replicas, all performing the same computation on the same input, one need $n$ times as much resources, hence a lot of computational resources is thus wasted. Dynamic analysis of the system and an adaptive scheduling technique could help in determining on which resources one should assign the tasks to for optimal resource usage and load balancing. 

Throughout this master thesis, extensive literature studies has been made, first in order to find out what has already been done in the area and later in order to get valuable help in developing a reliability model.

\section{Related work} \label{sec:introduction_related_work}
The interest in reliability in distributed systems has gain increased knowledge ~\cite{replicationManagement}. Due to the uncertain heterogeneous environment of cloud and grid systems, increasing reliability is a complex task [TODO]. %TODO add reference

A lot of scheduling techniques has been designed, aiming at maximizing reliability of jobs in distributed environments, under various constraints such as meeting task deadlines or minimizing execution time~\cite{algoOptTimeMaxRel} \cite{optTaskAllocationForMaxRel} \cite{taskAllocation} \cite{taskAllocationSwarm} \cite{algoMaxRelEndToEndConstraint} \cite{algoMinExTime} ~\cite{schedReplicas}. Maximizing reliability for these algorithms are a secondary concern, while meeting the constraints are the primary. Other algorithms have been developed which put greater focus on increasing the reliability~\cite{optResourceAllMaxPerformance} ~\cite{matchSchedAlgoMinFailure}, and some have increased reliability as the primary objective ~\cite{safetyRelTaskAllocation} ~\cite{improvedTaskAllMaxRel}. Common for these scheduling techniques are that while they try to maximize reliability, they do not ensure a certain level of reliability to the user. Furthermore, the algorithms are usually static in the way that they do not account for the dynamic behavior of distributed system, and they make assumptions such as known execution times of tasks.

A lot of work has been done in the area of designing fault-tolerant systems by using checkpoint/restart techniques [TODO]. %TODO add reference
These techniques relies of the notion of a stable storage, such as a hard-drive, which is persistent even in the case of a system failure.

Some attempts at designing fault-tolerant systems by the use of replication has been made ~\cite{designFaultTolerantSched} \cite{evalReplicationSched} \cite{taskSchedulingReplication} \cite{effTaskReplMobGrid} ~\cite{relGridServicePredConstraint}. \cite{evalReplicationSched} assumes a static number of replicas, which is used for every application being deployed. Furthermore, they do not guarantee that all replicas are deployed, instead they use a best-effort approach, where replicas are deployed when resources are available. While both~\cite{effTaskReplMobGrid}, \cite{taskSchedulingReplication} and~\cite{designFaultTolerantSched} all dynamically determines the number of replicas based on the state of the system, it is static in the way that failed replicas are not restarted. The reliability level for long running applications are therefore decreased if replicas fail. Furthermore, while ~\cite{designFaultTolerantSched} dynamically determines the number of replicas to use, the selection of resources is done after the number of replicas has been determined. In order to ensure a certain level of reliability, these two parts need to be combined into one, because the resources selected affects the number of replicas needed in order to achieve the desired reliability.

A quite old but still relevant work is found in ~\cite{dynAdaptRepl} where they present a framework for dynamic replication with an adaptive control in an multi-agent system. They introduce the software architecture which can act as support for building reliable multi-agent systems. Since the available resources often are limited they say that it isn't feasible to replicate all components. Therefore they use a criticality for each agent which is allowed to evolve during runtime. The proposed solution allows for dynamically adapt the number of replicas and the replication strategy itself (passive/active). The number of replicas is partly based on the agents critically and a predefined minimum number of replicas. From CPU usage time and communication activity an agent activity is calculated which is used in calculating the agents critically. One restriction they do in their fault model is that processes can only fail by permanent crashes. %TODO Do they meet a desrided level of reliablity?

Other approaches to improve reliability in Multi-Agent Systems (MAS) by the use of replication is presented in ~\cite{replicatingAgents} ~\cite{adaptiveMASReplication} ~\cite{adaptiveAgentReplication}. While being adaptive to system state, the solution presented in ~\cite{replicatingAgents} still faces the problem of having a single point of failure due to a communication proxy. This problem is avoided in ~\cite{adaptiveMASReplication}, where a decentralized solution is proposed, where the number of replicas and their placement depends on the system state. %TODO need to read \cite{adaptiveMASReplication} again...
The solution proposed in ~\cite{adaptiveAgentReplication} involves distributed monitoring system...  %TODO need to read this again...

\cite{adaptiveCheckPointAndRep} proposes an algorithm based on replication which dynamically varies the number of replicas depending on system load. However, the algorithm reduces the number of replicas during peak hours, in order to reduce system load. Since the reliability of system decreases during higher load ~\cite{studyOfFailures} ~\cite{studyOfFailures} ~\cite{implicationsOfFailures}, one should increase the number of replicas to keep the desired level of reliability.

A fault-tolerant scheduling technique incorporating a replication scheme is presented in ~\cite{faultTolerantSchedPolicy}. While being dynamic in that failed replicas are restarted, it is static in that the user defines the number of replicas to use.

The techniques used in ~\cite{selfAdaptRel} ~\cite{dynAdaptRepl} ~\cite{relModelWebServices} are more dynamic and adaptive to the dynamic behavior of distributed systems. However, reliability is defined as producing the correct result, and achieved by techniques like voting and \emph{k-modular redundancy}.
%TODO  (above sentence) However?
An adaptive approach, which adapts to changes in the execution environment is presented in ~\cite{imprRelAdaptRL}. In it, they present an adaptive scheduling model based on reinforcement learning, aiming at increasing the reliability. However, they assume a task's profile is available.


\section{Our contributions} \label{sec:introduction_contributions}
To our knowledge, no previous attempt has been made which in a fully dynamic manner ensures a certain level of reliability for long running applications. Some previous work dynamically calculates the number of replicas, but are static in that failed replicas are not restarted, while others use a static number of replicas, and dynamically restart failed ones.

We propose a framework which ensures an user determined level of reliability for long running applications by the use of replication. Furthermore, the method ensures a minimized use of resource by not using more replicas than needed. This is achieved by scheduling replicas to the most reliable resources first and foremost. Furthermore, the system is continuously monitored in order to adapt the number of replicas as the state of the system changes.

The framework is not limited to a specific type of distributed environment, and its key concepts may be used in both grid and cloud systems.

%TODO
%Finally, our solution is fully distributed and thereby avoids having a single point of failure, which may be the case in grid environments with a single Resource Management System. (THIS CONTRADICTS NOT BEING LIMITED TO A SPECIFIC DISTRIBUTED ENVIRONMENT)

Our model is implemented by extending the actor-based application environment \emph{Calvin} \cite{calvin}, developed by Ericsson. While \emph{Calvin} is mainly an environment for IoT applications, it suites our purpose well. The model is evaluated by running a set of experiments in a small cluster. %TODO

The report is structured as follows: in \cref{ch:background} all necessary background theory is provided, in \cref{ch:design} we present our model and contribution in more detail, \cref{ch:evaluation} aims at evaluating our solution while \cref{ch:future_work} presents future work. \Cref{ch:conclusions} concludes the report. 

\section{Goal} \label{sec:introduction_goals}
The goal of this thesis is to devise a method for dynamically ensuring a certain level of reliability for distributed applications or services. Reliability will be achieved through replication of tasks.

First, a reliability model will be designed, describing the reliability for an application running in a distributed environment.

Secondly, a framework will be designed which will automatically detect node failures and based on the reliability model spawns enough replicas to reach the desired reliability level above.

Lastly, the system will by periodically monitored in order to adapt the reliability model and the replicas needed as the properties of the system varies over time.

The model will be implemented and tested using the IoT application framework \emph{Calvin}.

\chapter{Background} \label{ch:background}
In this chapter we provide all the necessary background theory to fully understand the rest of the report.
\section{Computational Environment} \label{sec:background_comp_env}
The computational environment used in this thesis is distributed computing, i.e. several resources working together towards a common goal.
\subsection{Types of distributed computing} \label{subsec:background_types_of_distr_comp}
Distributed computing systems (DCS) are composed of a number of components or subsystems interconnected via an arbitrary communication network ~\cite{relModelDistSimSystem} ~\cite{efficientRelAnalysisAlgo}. There are a number of different types of distributed environments, e.g. grid, clusters, cloud and HDCS.

\subsubsection{Heterogeneous distributed computing systems}
Heterogeneous Distributed Computing Systems, HDCS is a system of numerous high-performance machines connected in a high-speed network. Therefore high-speed processing of heavy applications is possible ~\cite{algoMinExTime}. %TODO add info

The majority of distributed service systems can be viewed as CHDS, (Centralized Heterogeneous Distributed System). A CHDS consists of heterogeneous sub-systems which a managed by a centralized control center. The sub-systems have various operating platforms and are connected in diverse topological networks ~\cite{studyServiceRel}.

%COPIED: Most of the distributed service systems can be modeled as a CHDS. This type of distributed systems consists of heterogeneous sub-systems with various operating platforms on different computers in diverse topological networks, which are managed by a control center. The heterogeneous sub-distributed systems are composed of different types of computers with various operating systems connected by diverse topologies of networks. These sub-systems exchange data with virtual machine through system service provider interface. They are connected with virtual nodes by routers \cite{studyServiceRel}.

\subsubsection{Grid computing}
A grid is a collection of autonomous resources that are distributed geographically and across several administrative domains, and work together to achieve a common goal, i.e. to solve a single task ~\cite{compStudyLoadAndCloud} ~\cite{relAndPerfGridServices} ~\cite{evalOfGridRel}.

Each domain in a grid usually has a centralized scheduling service called Resource Management System (RMS) which accepts job execution requests and sends the job's tasks to the different resources for execution ~\cite{evalOfGridRel}.

The reliability of the cloud computing is very critical but hard to analyze due to its characteristics of massive-scale service sharing, wide-area network, heterogeneous software/hardware components and complicated interactions among them ~\cite{cloudServiceRel}.

%COPIED:The grid computing system has emerged as an important new field, distinguished from conventional distributed computing systems by its focus on large-scale resource sharing, innovative applications, and, in some cases, high performance orientation. \cite{hierarchicalRelModeling}.

\subsubsection{Cluster}
A cluster system is usually a number of identical units managed by a central manager. It is similar to a grid, but differ in that resources are geographically located at the same place. The resources work in parallel under supervision of a single administrative domain. From the outside it looks like a single computing resource ~\cite{compStudyLoadAndCloud}.

\subsubsection{Cloud}
A cloud has been described as the next generation of grids and clusters. While it is similar to grid and clusters, for example parallel and distributed, the important difference is that cloud has multiple domains ~\cite{compStudyLoadAndCloud}. Machines can be geographically distributed, and software and hardware components are often heterogeneous, and therfore analyzing and predicting workload and reliability is usually very challenging ~\cite{surveyReliabilityDistr}. 

%COPIED: "Cloud computing is a style of computing where service is provided across the Internet using different models and layers of abstraction [4]. It refers to the applications delivered as services [5] to the mass, " \cite{faultToleranceChallenges}

%COPIED: Cloud computing is different from but related with grid computing, utility computing and transparent computing. Grid computing [1] is a form of distributed computing whereby a "super and virtual computer" composed of a cluster of networked, loosely-coupled computers acts in concert to perform very large tasks.”

\subsection{Dynamic versus static environments} \label{subsec:background_dyn_stat_env}
A distributed computing environment can be either static or dynamic.

In a static environment only homogenous resources are installed ~\cite{compStudyLoadAndCloud}. Prior knowledge of node capacity, processing power, memory, performance and statistics of user requirements are required. Changes in load during run time is not taken into account which makes environment easy to simulate but not well suited for heterogeneous resources.

In a dynamic environment heterogeneous resources are installed ~\cite{compStudyLoadAndCloud}. In this scenario prior knowledge isn't enough since the requirements of the user can change during run time. Therefore run time statistics is collected and taken into account. The dynamic environment is difficult to simulate but there are algorithms which easily adopt to run time changes in load.


\section{Faults in distributed environments} \label{sec:background_faults_distr_env}
In a distributed environment a larger number of faults can occur due to the higher complexity. Therefore a fault model is usually employed, describing what kind of faults which is considered.
\subsection{Types of Faults} \label{subsec:background_types_of_faults}
%TODO Perhaps research more which definition is the most usual. Per Runesson, head of Computer Science at LTH uses a different definition
A fault is usually used to describe a defect at the lowest level of abstraction ~\cite{faultTolerantFundamentals}. A fault may cause an error which in turn may lead to a failure, which is when a system has not behaved according to its specification.

In distributed environments, especially with heterogeneous commodity hardware, several types of failures can take place, which may affect the running applications in the environment. These failure include, but are not limited to, overflow failure, timeout failure, resource missing failure, network failure, hardware failure, software failure, and database failure ~\cite{cloudServiceRel}. Failures are usually considered to be either ~\cite{evalOfGridRel}:

\begin{itemize}
	\item Job related, or
	\item System related, or
	\item Network related.
\end{itemize}

The possible errors in a Grid environment can be divided into the following three categorize ~\cite{effTaskReplMobGrid}:
\begin{itemize}
	\item Crash failure - When a correctly working server comes to a halt.
	\item Omission failure - When a server fails to respond to incoming requests and to send messages
	\item Timing failure - When a server responds correctly but beyond the specified time interval
\end{itemize}

In ~\cite{studyOfFailures}, almost ten years of real-world failure data of 22 high performance computing systems is studied and concluded hardware failures to be the single most common type of failure, ranging from 30 to more than 70 \% depending on hardware type, while 10 - 20 \% of the failures were software failures.

\subsection{Fault models} \label{subsec:background_fault_models}
When studying the reliability of distributed systems or reliability of applications running in distributed computing environments, one usually starts with specifying which fault model is used, and the model developed is then proved with respect to this fault model ~\cite{faultTolerantFundamentals}.

\subsubsection{Byzantine Faults} \label{subsub:background_byzantine}
The Byzantine fault model allows nodes to continue interaction after failure. Correctly working nodes cannot automatically detect if a failure has occurred and even if it was known that a failure has occurred they cannot detect which nodes has failed. The systems behaviour can be inconsistent and arbitrary ~\cite{surveyFaultParallel}. Nodes can fail (become Byzantine) at any point of time and stop being Byzantine at any time. A Byzantine node can send no response at all or it can try to send an incorrect result. All Byzantine nodes might send the same incorrect result making it hard to identify malicious nodes ~\cite{selfAdaptRel}. %Kan man detektera malicious nodes? 
The Byzantine fault model is very broad since it allows failed nodes to continue interacting, therefore it is very difficult to analyze.

% COPIED: in which processors may behave in arbitrary, even malevolent, ways). System correctness was always proved with respect to a specific fault model \cite{faultTolerantFundamentals}

\subsubsection{Fail-stop faults} \label{subsub:background_fail_stop}
The fail-stop model, also called the crash-stop model, is in comparison to the Byzantine fault model much simpler. When a node fails it stops producing any output and stops interacting with the other nodes ~\cite{faultTolerantFundamentals}. This allows for the rest of the system to automatically detect when a node has failed. Due to its simplicity, it does not handle subtle failures such as memory corruption but rather failures such as a system crash or if the system hangs ~\cite{surveyFaultParallel}. The fail-stop model has been criticized for not representing enough real-world failures.

%COPIED: This failure model has the practical implication that avoids the use of complicated software and/or hardware replication systems for recovering tasks from failed server. Consequently, the application cannot be successfully serviced by the system if at least one task remains unprocessed at a failed server. \cite{perfRelNonMarkovian}

\subsubsection{Fail-stutter faults}
Since the Byzantine model is very broad and complicated and the fail-stop model doesn't represent enough real-world failures, a third middle ground model has been developed. It is an extension of the fail-stop model but it also allows for performance fault, such as unexpectedly low performance of a node ~\cite{surveyFaultParallel}.
% fail-stutter modellen är knappt använd...

\subsubsection{Crash-failure model}
%TODO Källa
The crash failure model is quite alike the fail-stop model with the difference that the other nodes do not automatically detect that a node has failed ~\cite{faultTolerantFundamentals} ~\cite{adaptiveAgentReplication}.

\subsection{Failure distribution} \label{subsec:background_failure_distribution}
Models describing the nature of failures in distributed computing environments, are usually based on certain assumptions, and are usually only valid under those assumptions. Common assumptions are stated in \cref{def:failure_assumptions} ~\cite{relModelDistSimSystem} \cite{relModelAnalysis}  \cite{cloudServiceRel} \cite{studyServiceRel} \cite{hierarchicalRelModeling} ~\cite{selfAdaptRel}:

\begin{definition} \label{def:failure_assumptions}
Common assumptions when modeling failures are:
\begin{itemize}
	\item Each component in the system has only two states: operational or failed
	\item Failures of components are statistically independent
	\item The network topology is cycle free
	\item Components have a constant failure rate, i.e. the failure of a component follows a Poisson process
	\item Fully reliable network
\end{itemize}
\end{definition}

For reliability modelling for the grid, it is common to also assume a fully reliable Resource Management System (RMS) ~\cite{relAndPerfGridServices} ~\cite{relGridServicePredConstraint}, which is an non-neglect-able assumption since the RMS in this case in fact is a single point of failure.

%TODO definera "failures", vilka brukar inkluderas?
%When modelling system reliability, failures are usually assumed to follow a Poisson process with a constant failure rate ~\cite{algoMaxRelEndToEndConstraint} \cite{algoMinExTime} \cite{relModelDistSimSystem} \cite{optTaskAllocationForMaxRel} \cite{perfImplPerCheckPoint} ~\cite{optCheckpointInterval} ~\cite{realTimeSchedAlgo}. For such a model to be valid, it is assumed that failures between resources are statistically independent with a constant failure rate ~\cite{algoMaxRelEndToEndConstraint}. %we use Poisson and have a non-constant failure rate?

Constant failure rates are not likely to model the actual failure scenario of a dynamic heterogeneous distributed system ~\cite{algoMinExTime}. The failure rate for a hardware component usually follows a bathtub shaped curve ~\cite{surveyReliabilityDistr}. The failure rate is usually higher in the beginning due to that the probability that a manufacture failure would affect the system is higher in the beginning of the systems lifetime. After a certain time the failure rate drops and later increases again due to that the component gets worn out.

Statistically independent failures is also not very likely to reflect the real dynamic behaviour of distributed systems ~\cite{surveyReliabilityDistr} ~\cite{cloudServiceRel}. Faults may propagate throughout the system, thereby affecting other components as well ~\cite{relGridSystems}. In grid environments, a sub-system may consist of resources using a common gateway to communicate with the rest of the system. In such a scenario, the resources do not use independent links ~\cite{optResourceAllMaxPerformance}. As failures are likely to be correlated ~\cite{perfImplPerCheckPoint}, the probability of failure increases with the number of components on which the job is running.

Other factors affect the likelihood of resource failures as well. Several work has concluded a relationship between failure rate and the load of the system ~\cite{studyOfFailures} ~\cite{implicationsOfFailures}. Furthermore, studies show that failures are more likely to occur during daytime than at night ~\cite{implicationsOfFailures} ~\cite{studyOfFailures}. Finally, components which have failed in the past are more likely to fail again ~\cite{implicationsOfFailures}.

While a Poisson process is commonly used to describe failures, ~\cite{studyOfFailures} showed failures are better modelled by a Weibull distribution with a shape parameter of 0.7 - 0.8. However, despite not always reflecting the true dynamic failure behaviour of a resource, a Poisson process has been experimentally shown to be reasonable useful in mathematical models ~\cite{experimentalFailureAssessment}.

\section{Reliability} \label{sec:background_reliability}
Reliability in the context of software applications can have several meanings, especially for applications running in distributed systems. Often, reliability is defined as the probability that the system can run an entire task successfully ~\cite{taskAllocation} \cite{relModelDistSimSystem} \cite{studyServiceRel} \cite{hierarchicalRelModeling} \cite{generalAlgoRelEval} \cite{realTimeRelAnalysis} \cite{selfAdaptRel} ~\cite{perfRelNonMarkovian}. A similar definition of reliability, usually used for applications in distributed environments, is the probability of a software application, running in a certain environment, to perform its intended functions for a specified period of time ~\cite{surveyReliabilityDistr} ~\cite{surveyRelPrediction} ~\cite{relDistApplications}, and is common for application with time constraints. Finally, reliability can also be defined as the probability that a task produces the correct result ~\cite{surveyRelPrediction} \cite{relAndPerfGridServices} \cite{relGridServicePredConstraint} \cite{relModelWebServices} ~\cite{selfAdaptRel}. This definition is usually used together with the Byzantine fault model. 

%Onödig definition??
\cite{surveyReliabilityDistr} defines a software application reliable if the following is achieved:
\begin{itemize}
\item Perform well in specified time t without undergoing halting state
\item Perform exactly the way it is designed
\item Resist various failures and recover in case of any failure that occurs during system execution without proceeding any incorrect result.
\item Successfully run software operation or its intended functions for a specified period of time in a specified environment.
\item Have probability that a functional unit will perform its required function for a specified interval under stated conditions.
\item Have the ability to run correctly even after scaling is done with reference to some aspects.
\end{itemize}

In this paper, we use the following definitions of reliability:
\begin{definition} \label{def:single_task_reliability}
The reliability of a process is the probability that the resource on which the process is running is functioning during the time of execution.
\end{definition}

For multi-task applications, where the tasks use more than one resource, reliability is defined as
\begin{definition} \label{def:multi_task_reliability}
The reliability of a multi-task process is the probability that the tasks being executed within a given time without experiencing any type of failure (internal or external) during the time of execution.
\end{definition}

Finally, for long running applications, where a replication scheme is used, the reliability can be defined as \cite{effTaskReplMobGrid}
\begin{definition} \label{def:task_replica_reliability}
The reliability of a process, with $n$ task replicas, is the probability that at least one replica is always running. This can be expressed as the probability that not all replicas fail during the time from that an actor dies until a new replica is up and running.
\end{definition}

\subsection{Modelling reliability} \label{subsec:background_modelling_rel}
Reliability of a system highly depends on how the system is used ~\cite{surveyRelPrediction}. In order to determine the reliability of a system, one need to take all factors affecting the reliability into account ~\cite{surveyReliabilityDistr}. However, including all factors if unfeasible. \cite{factorsAffectingRel} lists 32 factors affecting the reliability of software, excluding environmental factors such as hardware and link failure. Other environmental conditions affecting reliability include the amount of data being transmitted, available bandwidth and operation time ~\cite{cloudServiceRel} ~\cite{hierarchicalRelModeling}.

For distributed applications, the probability of failure increases since it is dependent on more components ~\cite{relModelDistSimSystem}. Most models used to model the reliability of a system is based on the mean time to failure, or the mean time between failures, of components ~\cite{relModelAnalysis}. Conventionally, Mean-Time-To-Failure (MTTF) refers to non-repairable resources, while Mean-Time-Between-Failures (MTBF) refers to repairable objects ~\cite{effTaskReplMobGrid}.

\begin{definition} \label{def:mttf}
The Mean-Time-To-Failure for a component is the average time it takes for a component to fail, given that it was operational at time zero.
\end{definition}

\begin{definition} \label{def:MTBF}
The Mean-Time-Between-Failure for a component is the average time between successive failures for that component.
\end{definition}

The MTBF can be calculated as

\begin{equation} \label{eq:MTBF}
MTBF = (total\ time) / (number\ of\ failures)
\end{equation}

From \cref{eq:MTBF}, a reliability function for a component can be expressed as 
\begin{equation} \label{eq:resource_reliability}
R(t) = e^{-t/MTBF}
\end{equation}

\Cref{eq:resource_reliability} expresses the probability that a given resource will work for a time $t$. Correspondingly, the probability that a resource will fail during a time $t$ is
\begin{equation} \label{eq:resource_failure_prob}
F(t) = 1- e^{-t/MTBF}
\end{equation}

\iffalse
While redundancy and diversity have been employed as popular methods to attain better reliability [4][5][6][7][8][9][10], they impose extra hardware or software costs.
optimal task allocation. This method does not require additional hardware or software and improves system reliability just by using proper allocation of the tasks among the nodes [8][11][12] [13] [14] \cite{optTaskAllocationForMaxRel}
\\\\
\cite{discContRelModel}
\\\\
System reliability: Due to independence of node and path failures, system reliability can be formulated as: Rs(X) = RS’(X) * Rs” (X) \cite{optTaskAllocationForMaxRel}
\\\\
the reliability model of the sub-distributed systems inherits the traditional models’ characters [19], [20], [21], [22] and has certain limitations. Those traditional models have a common assumption that the operational probabilities of the nodes and links are constant. However, this assumption is unrealistic for the grid, so this assumption was relaxed in [15] by assuming that the failures of nodes and links followed their respective Poisson processes so that their operational probabilities decrease with their working time instead of the constant values 
There are also many other reliability models for software, hardware, or small-scale distributed systems, see, e.g., [14], [23], [24], [25], [26], which cannot be directly implemented for studying grid service reliability. \cite{hierarchicalRelModeling}
\\\\
Reliability evaluation of the hyper-cubes has been addressed recently [7]-[9]. Najjar and Gaudiot have modeled hyper-cube reliability assuming that the system works as long as there is no disconnected working node(s) [7]. This implies that even if a task requirement is satisfied, the system is considered failed whenever there is a disconnected node(s). This reliability is not based on task requirement, but on system managemen.  [ A unified task-based dependability model for hyper-cube computers]
\\\\
(the processing element that runs the program under consideration) to some other nodes such that its vertices hold all the needed files for the program under consideration. \cite{relAnalysisFRA}
\\\\
We focus on the study of task allocation decision to achieve maximal distributed system reliability (DSR) under processor resource constraints and total system cost constraint. the task allocation problem has been studied to achieve various goals, such as reliability maximization, safety maximization, fault tolerance increasing, and cost minimization. Their processor reliability was computed from the processor failure rate and the elapsed execution time. As far as we know, the effects of module software reliabilities and module execution frequencies on the optimal task allocation decision have not been studied before. To pursue this topic, we divide the processor reliability into two parts in our system model: the processor hardware reliability and the module software reliability \cite{decisionModelTaskAllocation}
\fi

\section{Fault tolerance techniques} \label{sec:background_fault_tol_tech}
Fault tolerance techniques are used to either predict failures and take an appropriate action before they actually occur
~\cite{faultToleranceChallenges} or to prepare the system that failures might occur and take an appropriate action first when they occur. Considering the whole life-span of a software application, fault-tolerant techniques can be divided into four different categories ~\cite{surveyReliabilityDistr}:

\begin{enumerate}
\item Fault prevention - elimination of errors before they happen, e.g. during development phase
\item Fault removal - elimination of bugs or faults after repeated testing phases
\item Fault tolerance - provide service complying with the specification in spite of faults
\item Fault forecasting - Predicting or estimating faults at architectural level during design phase or before actual deployment
\end{enumerate}

Limited to already developed application, fault tolerance techniques can be divided into reactive and proactive techniques ~\cite{faultToleranceChallenges}. A reactive fault tolerant technique reacts when a failure occur and tries to reduce the effect of the failure. They therefore consists of detecting fault and failures, and recovering from them to allow computations to continue ~\cite{relGridSystems}. The proactive technique on the other hand tries to predict failures and proactively replace the erroneous components.

Common fault tolerance techniques include \emph{checkpointing}, \emph{rollback recovery} and \emph{replication} ~\cite{relGridSystems}.

% Checkpointing, job migration, voting etc
\subsection{Checkpoint/Restart} \label{subsec:background_checkpoint}
Fault tolerance by the use of periodic checkpointing and rollback recovery is the most basic form of fault tolerance ~\cite{surveyFaultParallel}.

Checkpointing is a fault tolerance technique which periodically saves the state of a computation to a persistent storage ~\cite{relGridSystems} ~\cite{surveyFaultParallel}. In the case of failure, a new process can be restarted from the last saved state, thereby reducing the amount of computations needed to be redone.

%TODO
%The basics, however, are checkpoint recovery and task replication. The former is a common method for ensuring the progress of a long-running application by taking a checkpoint, i.e. saving its state on permanent storage periodically. A checkpoint recovery is an insurance policy against failures. In the event of a failure, the application can be rolled back and restarted from its last checkpoint—thereby bounding the amount of lost work to be re-computed \cite{effTaskReplMobGrid}

\subsection{Rollback recovery} \label{subsec:background_rollback}
Rollback recovery is a technique in which all actions taken during execution are written to a log. At the event of failure, the process is restarted, and the log is read and the actions replayed, which will reconstruct the previous state ~\cite{surveyFaultParallel}. In contrast to checkpointing, rollback recovery returns the state to the most recent state, not only the last saved one. Rollback recovery can however be used in combination with checkpointing in order to decrease recovery time, not needing to replay all actions, but only those from the latest checkpoint.

\subsection{Replication} \label{subsec:background_replication}
Job replication is a commonly used fault tolerance technique, and is based on the assumption that the probability of a single resource failing is greater than the probability of multiple resources failing simultaneously ~\cite{faultToleranceGrid}.

Using replication, several identification processes are scheduled on different resources and simultaneously perform the same computations ~\cite{relGridSystems}. With the increased redundancy, the probability of at least one replica finishing increases at the cost of more resources being used. Furthermore, the use of replication effectively protects against having a single point of failure ~\cite{faultToleranceGrid}.

Replication also minimizes the risk of failures affecting the execution time of jobs, since it avoids recomputation typically necessary when using checkpoint/restart techniques ~\cite{designFaultTolerantSched}.

There are three different strategies for replicating a task: \emph{Active}, \emph{Semi-active} and \emph{Passive}.
\begin{itemize}

\item In active replication, one or several replicas are run on a other machine and receive an exact copy of the primary nodes input. From the input they perform the same calculations as if they were the primary node. The primary node is monitored for incorrect behaviour and in event that the primary node fails or behaves in an unexpected way, one of the replicas will promote itself as the primary node ~\cite{surveyFaultParallel}. This type of replication is feasible only if by assumption that the two nodes receives exactly the same input. Since the replica already is in an identical state as the primary node the transition will take negligible amount of time. A drawback with active replication is that all calculations are ran twice, thus a waste of computational capacity. 

Active replication can also be used in consensus algorithms such as majority voting or k-modular redundancy, where one need to determine the correct output ~\cite{surveyFaultParallel}. In this case, there is no primary and backup replicas, instead every replica acts as a primary.

\item Semi-Active replication is very similar to active replication but with the difference that decisions common for all replicas are taken by one site.

\item Passive replication is the case when a second machine, typically in idle or power off state has a copy of all necessary system software as the primary machine. If the primary machine fails the "spare" machine takes over, which might incur some interrupt of service. This type of replication is only suitable for components that has a minimal internal state, unless additional checkpointing is employed.
\end{itemize}

A replica, whether active, semi-active or passive replication is used, is defined as ~\cite{effTaskReplMobGrid}:
\begin{definition} \label{def:replica}
The term replica or task replica is used to denote an identical copy of the original task
\end{definition}

While replication increases the system load, is may help improving performance by reducing task completion time ~\cite{improvingPerformanceReplication}.

\subsubsection*{Consensus} \label{subsub:consensus}
Replication is usually used with some form of consensus algorithm. The consensus problem can be viewed as a form of agreement. A consensus algorithm lets several replicas execute in parallel, independent of each other and afterwards they agree on which result is considered correct. These algorithms are usually used with the Byzantine fault model \cref{subsub:background_byzantine}, where a resource can produce an incorrect result.

Based on achieving consensus two different redundancy strategies can be identified, traditional and progressive redundancy~\cite{selfAdaptRel}. In traditional redundancy an odd number of replicas are executed simultaneously and afterwards they vote for which result is correct. The result with the highest number of votes are considered correct and consensus is reached.

In progressive redundancy on the other hand the number of replicas needed is minimized. Assume that with traditional redundancy $k \in {3,5,7...}$ replicas is executed, progressive redundancy only executes $(k+1)/2$ replicas and reaches consensus if all replicas return the same result. If some replica return a deviant result an additional number of replicas is executed until enough replicas has return the same result, i.e. consensus is reached. In worst case $k$ replicas are executed, the same as for traditional redundancy. A disadvantage with progressive consensus is that it might take longer time if consensus is not reached after the first iteration.

Furthermore ~\cite{selfAdaptRel} present a third strategy, an iterative redundancy alternative which focus is more on reaching a required level of reliability in comparison to reaching a certain level of consensus.

%TODO ref to \cite{selfAdaptRel} \cite{dynAdaptRepl} \cite{relModelWebServices}?

\iffalse
%Delete? We don't use load balancing at all
\section{Load balancing} \label{sec:background_load_balancing}
The term load balancing is generally used for the process of transferring load from overloaded nodes to under loaded nodes and thus improving the overall performance. Load balancing techniques for a distributed environment must take two tasks into account, one part is the resource allocation and the other is the task scheduling. 
%Efficient load balancing will ensure that resources are easily available on demand and efficiently utilized under condition of high/low load, that energy is saved at low load and that the cost of resources is reduced \cite{compStudyLoadAndCloud}.

The load balancing algorithms can be divided into three categorize based on the initiation of the process:
\begin{itemize}
\item Sender Initiated - An overloaded node send requests until it find a proper node which can except its load.
\item Receiver Initiated - An under loaded node sends a message for requests until it finds an overloaded node.
\item Symmetric - A combination of sender initiated and receiver initiated. 
\end{itemize}

Load balancing is often divided into two categorize, namely static and dynamic algorithms. The difference is that the dynamic algorithm takes into account the nodes previous states and performance whilst the static doesn't. The static load balancing simply looks at things like processing power and available memory which might lead to the disadvantage that the selected node gets overloaded ~\cite{perfAnalysisLoadCloud}

A dynamic load balancing can work in two ways, either distributed or non-distributed. In the distributed case the load balancing algorithm is run on all the nodes and the task of load  balancing is shared among them. This implies that each node has to communicate with all the others, effecting the overall performance of the network. % true also for static load balancing? i.e. distr/non-distr

In the non-distributed case the load balancing algorithms is done by only a single node or a group-node. Non-distributed load balancing can be run in semi-distributed form, where the nodes are grouped into clusters and each such cluster has a central node performing the load balancing. Since there is only one load balancing node the number of messages between the nodes are decreased drastically but instead we get the disadvantage of the central node becoming a single-point of failure and a bottleneck in the system. Therefore this centralized form of load balancing is only useful for small networks ~\cite{perfAnalysisLoadCloud}.
\fi

\iffalse
We will briefly mention some dynamic load balancing algorithms: % skip static load balancing algorithms or not?

\begin{itemize}
\item Central Queue Algorithm A non-distributed algorithm where the central manager maintains a cyclic FIFO-queue. Whenever a new activity arrives the manager inserts it into the queue and whenever a new request arrives it simply picks the first activity in the queue. 

\item Local Queue Algorithm When a new task is created on the main host it will be allocated on under-loaded nodes. Afterwards all new tasks are allocated locally since ... \cite{perfAnalysisLoadCloud}. The algorithm is receiver initiated since when a node is under-loaded it randomly sends requests to remote load managers

\item Ant Colony Optimization Algorithm As the name implies this algorithm is inspired by the behavior of real ants to find a optimal solution. Whenever an ant find food it moves back to the colony while leaving "markers", i.e. laying pheromone on the way. When more ants find the same place the path the pheromone will become denser. 
%Förtydliga om det ska ingå i rapporten.

\item Honey Bee Foraging Algorithm This algorithm is quite similar to the Ant Colony Optimization algorithm. When bees find food they return to the bee's colony and use special dance movements for informing the other bees of how much food there is and where it's located. When the forager bees find more food a more energetic dance takes place. This phenomenon can be applied to servers, when an overloaded server receives a request it redirects it to other under loaded servers.

% De fyra ovan är från \cite{perfAnalysisLoadCloud}

% Static algorithms: (?)
\item Bidding An overloaded node request bids from other nodes. The node with the best bid (i.e. lowest load) wins the job.
\item Max-Min 
\item Min-Min 
\end{itemize}

\fi

\section{Task scheduling} \label{sec:background_task_sched}
Task scheduling is the process of mapping tasks to available resource. A scheduling algorithm can be divided into three simple steps ~\cite{optSchedCloud}:

\begin{enumerate}
	\item Collect the available resources
	\item Based on task requirements and resource parameters choose resources to schedule the task on
	\item Send the task to the selected resources
\end{enumerate}

Based on which requirements and parameters are considered in step 2 above a task scheduling algorithm can achieve different goals. They can aim at maximizing the total reliability, minimizing the overall system load or meeting all the tasks deadlines \cite{schedulingSurvey}. 

Task scheduling algorithms can be divided into static and dynamic algorithms depending on if the scheduling mapping is based on pre-defined parameters or if the parameters might change during runtime ~\cite{schedReplicas}.

Many studies have been recently done to improve reliability by proper task allocation in distributed systems, but they have only considered some system constraints such as processing load, memory capacity, and communication rate ~\cite{optTaskAllocationForMaxRel}. In fact finding an optimal solution and maximizing the overall system reliability at the same time is a NP-hard problem ~\cite{optTaskAllocationForMaxRel} ~\cite{taskAllocationSwarm} \cite{schedulingSurvey}.

Taking many factors in account when scheduling tasks results in a big overhead in the cloud, thus decreasing performance. 

%COPIED:
%The problem of finding an optimal task allocation with maximum system reliability has been shown to be NP-hard; thus, existing approaches to finding exact solutions are limited to the use in problems of small size. This paper presents a hybrid particle swarm optimization (HPSO) algorithm for finding the near-optimal task allocation within reasonable time. \cite{taskAllocationSwarm}.

\section{Monitoring} \label{sec:background_monitoring}
Monitoring is common in distributed system for detecting when resources fail. The most common technique is based on heartbeats, i.e. the resources send "I'm still alive"-messages with a certain pulse to each other and if a resource stops receiving heartbeats from a resource it is assumed to have died. Another technique is to send a test message and wait for a reply and after a certain timeout assume that the resource has died \cite{probabilistic_recovery}.

\iffalse
COPIED:
Heartbeat-based monitors in which lack of a timely heartbeat message from the target indicates failure.
Test-based monitors which send a test message to the target and wait for a reply. Messages may range from OSlevel pings and SNMP queries to application level testing, e.g., a test query to a database object.
End-to-end monitors which emulate actual user requests. Such monitors can identify that a problem is somewhere along the request path but not its precise location.
Error logs of different types that are often produced by software components. Some error messages can be modeled as monitors which alert when the error message is produced.
Statistical monitors which track auxiliary system attributes such as load, throughput, and resource utilization at various measurement points, and alarm if the values fall outside historical norms.
Diagnostic tools such as file system and memory checkers (e.g., fsck) that are expensive to run continuously, but can be invoked on demand. [Probabilistic Model-Driven Recovery in Distributed Systems]

COPIED:
Globus [9] provides a heartbeat service to monitor running processes to detect faults  \cite{effTaskReplMobGrid}

COPIED
Nodes can alternate between working correctly and being crashed in our model. Hence, the status of a node is modeled by a state machine with two states, failed and working. Failed nodes do not send messages nor do they perform any computation. Working nodes execute faithfully the diagnosis procedure. In this section, we derive lower bounds on the diagnostic latency, start-up time, and state holding time achievable by any heartbeat-based diagnosis algorithm in completelyconnected networks. The maximum time between two consecutive heartbeats arriving from a continuously working node at any other node in the system sets a limit on how early failed nodes can be identified by the absence of a heartbeat. \cite{distDiagnosis}

COPIED: "“The most basic form of monitoring is a simple heartbeat system. A monitor process listens for periodic messages from the monitored components. The message simply indicates that the component continues to function correctly enough to send messages. " \cite{surveyFaultParallel}.
\fi

\section{Virtualization and containers} \label{sec:background_virtualization}
Virtualization is a broad term of creating a virtual version of something, e.g. a server, a hard drive or a private network (VPN). The advantages of virtualization are savings in hardware cost and space and in that the system becomes more dynamic. For instance, since hardware servers usually has a lot of free capacity it is possible to run two virtual servers on each physical server. Another advantage with virtualization is that it separates the software from the hardware which enables the possibility of moving a virtual machine from one physical location to another.

A container is very similar to a virtual machine but more light-weight. Containers running on the same physical machine share the machines operating system and binaries while virtual machines running on a single physical machine have their own operating systems and dedicated resources, isolating them from each other \cite{vm_vs_container}.

%TODO first mention of calvin?
Calvin, an actor-based application environment developed by Ericsson, use a concept of runtimes. A runtime contains all necessary components to run various kinds of actors, by taking care of things such as scheduling and communication between runtimes. The runtimes work as containers and separate the hardware from the software, resulting in that it is possible to use Calvin on different platforms. In developing phase of new applications the developer does not need to consider on which operating systems the application should run.

\iffalse
%COPIED
Virtualization for cloud
What makes virtualization so important for the cloud is that it decouples the software from the hardware. [dummies.com] 

%COPIED
Docker (Most popular container)
Docker containers wrap up a piece of software in a complete file system that contains everything it needs to run:  code, runtime, system tools, system libraries – anything you can install on a server. This guarantees that it will always run the same, regardless of the environment it is running in
\fi

\chapter{Design of a dynamic fault-tolerant system} \label{ch:design}
\section{Methodology} \label{sec:design_methodology}
%TODO
The methodology for devising our model is mainly literature study as well as meetings and discussions with both our supervisor but also with colleges at MAPCI.

To validate our model and show its usefulness, it was implemented in Ericsson's actor-based application environment Calvin \cite{calvin}. A somewhat simplistic reliability model was made, and more time spent on implementation, in order to be able to conduct various kinds of experiments. 

An alternative approach would have been to make a more sophisticated reliability model, which maps better to real-world characteristics of distributed environments. However, while a more sophisticate theoretical model may map better to the real-world, it is impossible to include all parameters affecting the reliability of an application or service running in an distributed environment. Furthermore, with no actual implementation, only simulations could be conducted to verify the model, and even though simulations can take a lot of parameters into account, it would have to be based on failure data, which is usually collected from in-house testing, and thus cannot be compared with failures that can occur in an actual operational environment~\cite{surveyReliabilityDistr}.

Since we limit ourselves to testing only in Calvin we lose some of the understanding of the results but we still believe that it can give a good first interpretation of the possibilities, see~\cref{ch:discussion}. 

Since a lot of test were already written to the Calvin system a test-driven development of our code came natural.

\section{Introduction} \label{sec:design_intro}
In the case of stream processing, reliability is of particular interest in order not to lose any valuable data. In this thesis, focus is on the case when a producer of some kind produces data which needs to be transformed by a service $S$ before being sent to a consumer. The service $S$ is running within a cluster of some service provider, and the user of this service demands a certain level of reliability, and active replication is used to ensure this.	

Choosing streaming services with deterministic processing\footnote{A deterministic function always produces the same output given a certain input.} allows us to avoid timing issues and cases when the consumer receives different results. However, since using active replication the consumer will receive a result from each replica and our model could therefore easily be extended with consensus algorithms such as those presented in \cref{subsub:consensus} to determine whether or not the correct result was received. 

Furthermore, long running applications or services, such as streaming applications, allows us to focus on ensuring a desired reliabity even in the case of failures. Therefore, in contrast to the various scheduling algorithms in ~\cite{algoOptTimeMaxRel} \cite{optTaskAllocationForMaxRel} \cite{taskAllocation} \cite{taskAllocationSwarm} \cite{algoMaxRelEndToEndConstraint} \cite{algoMinExTime}~\cite{schedReplicas} aiming at meeting task deadlines or minimizing execution time, a greedy scheduling algorithm will be presented which solely aims at reaching a desired level of reliability with the minimum number of replicas.

Unlike ~\cite{designFaultTolerantSched}  \cite{evalReplicationSched} \cite{taskSchedulingReplication} \cite{effTaskReplMobGrid} \cite{relGridServicePredConstraint}, we propose a fully dynamic model which monitors the system with its running applications and services, and create new replicas in order to ensure that the desired reliability is met. Determining the number of replicas needed only when deploying and application or service may suffer if the system is in a reliable state at the time, but later becomes more unreliable. As mentioned in~\cref{subsec:background_failure_distribution}, the reliability of the resources vary over time, why one must periodically monitor the system and vary the number of replicas over time. In our model, this is achieved by monitoring both how many replicas are operational, on which nodes they are executing, as well as node failures, in order to dynamically adapt the reliability model as the properties of the system changes.

The rest of this chapter is composed as follows, in \cref{sec:design_limitations} we go through what limitations we have done. \Cref{sec:design_system_model} gives a description of the system model we have used, e.g. in which computational environment we test our model, how we model the faults and their distribution and what kind of applications we use. In~\cref{sec:design_reliability_model} we present our reliability model, which is our main contribution with this paper. We start by giving diverse definitions and a scheduling algorithm and then go through our self-adapting model. Last in this chapter, \cref{sec:design_implementation} we briefly describe how Calvin works and present the main parts of the implementation of our model.

\section{Limitations} \label{sec:design_limitations}
By assuming fully reliable links between nodes, we limit our model to only consider node failures. However, the reliability model used in this thesis could easily be extended to consider more parameters, or be replaced by another reliability model, and the scheduling algorithm presented in~\cref{subsec:design_sched_alg} is independent of the reliability model used. Also, we do not distinguish between different kinds of failures, hence the reason for why a node failed is not important in our model, but could be taken into account in a more sophisticated reliability model.

Furthermore, we assume the replicated tasks/services are deterministic and always produces the correct result for a given input. This allows us to focus on the reliability of producing a result and not lose any data, instead of the reliability of producing the correct result. As mentioned before this can quite easily be solved by extending the model with consensus such as majority voting. Due to that we don't gain any extra useful information we have chosen to leave this a future work.

We also assume the case where all nodes are within the same cluster, connected with high-bandwidth and low-latency links, and all nodes are reachable from every other node. We therefore assume that the time it takes to send a message between two nodes is the same for all nodes.

By the use of these limitations we avoid unnecessary complexity in form of checkpointing and consensus. The impact of this will be discussed later in report, \cref{ch:discussion}.

%\subsection{Reliability model}
%\label{sec:limitations_reliability_model}
\iffalse
COPIED:

Many engineers and researchers base their reliability models on the assumption that components of a system fail in a statistically independent manner. This assumption is often violated in practice because environmental and system specific factors contribute to correlated failures, which can lower the reliability of a fault tolerant system. A simple method to quantify the impact of correlation on system reliability is needed to encourage models explicitly incorporating correlated failures. Previous approaches to model correlation are limited to systems consisting of two or three components or assume that the majority of the subsets of component failures are statistically independent. \cite{discContRelModel}

the interrelationship constraint between task modules has not been taken into account in calculation of distributed system reliability (DSR). In distributed simulation, the LPs simulation advances are bound by synchronization constraint, which will affect the executive time of system components. \cite{relModelDistSimSystem}

We do not take into account parameters such as X and Y [TODO].

Furthermore failure data which they have collected for their experiments through in-house testing cannot be compared with failures that can occur under actual operational environment \cite{surveyReliabilityDistr}

A lot of researchers are of the view that service providers must provide some other detail to compute reliability of both type of services i.e. atomic service and composite service. Such as, external services it uses, how service are glued together in composite service, how frequently they call each other, and flow graph describing behavior of service [2]. Service Oriented Reliability Model (SORM) [18] computed reliability of atomic and composite services exploiting distinct technique \cite{surveyReliabilityDistr}

Given the failure probability Pfik of each one of the mi replicas Ti k of task Ti , the new failure probability P fi0 for task Ti is:
0 mi Y
Pfi =Pfi· Pfik. (2) k=1
The above corresponds to the probability of the event “all the replicas and the original task fail”. Respectively, the success probability is equal to the probability of the event “the original task or at least one of its replicas executes successfully”. The number of replicas issued depends on the failure probabilities of the original tasks and on the desired fault tolerance level in the Grid infrastructure.  \cite{effTaskReplMobGrid}
\fi

\section{System model} \label{sec:design_system_model}
In this section, we describe the system and application models employed in this work.

\subsection{Computational environment} \label{subsec:design_comp_env}
In this paper we assume that all resources are within the same cluster, with low latency connections between them. The resources consist of heterogeneous hardware with high bandwidth low latency redundant links between them. Due to its heterogeneous properties, nodes can not be assumed to have the same failure rate.

A small example of four interconnected nodes  is shown in \cref{fig:computational_environment}. We refer to a computational resource as a \emph{node}. Due to the lack of access to a cluster consisting of heterogeneous hardware components, the cluster used for experiments consists of homogeneous hardware components, and varying behavior in terms of failure is simulated in the experiments. This if further described in~\cref{ch:evaluation}.

Since all nodes are interconnected we have redundant paths which results in that all nodes are directly reachable even if a node fails. In~\cref{appendix:figures} a system of one replicated actor is shown before and after a node failure. Finally, we assume that the time to send a message between two nodes is the same for all nodes.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/computational_environment.pdf}
\caption{Computational environment, interconnected nodes} \label{fig:computational_environment}
\end{figure}

\subsection{Monitoring} \label{subsec:design_monitoring}
Monitoring the system is crucial to achieve an entirely dynamic system, in which certain reliability is dynamically ensured. One must know both where replicas are executing and be able to detect node failures.

\subsubsection{Heartbeats}
The ability to detect node failures is crucial for knowing when a new replica is needed in order to keep the desired reliability. Furthermore, since the reliability model presented in ~\cref{sec:design_reliability_model} is based on a \emph{mean-time-between-failures} for the nodes in the system, it is based on the assumption that node failures is detectable. 

To achieve this, a heartbeat system is used, where nodes periodically send UDP messages, so called heartbeats, containing a node identifier to the other nodes in the system. If no heartbeat is received from a node for a certain period of time, the node is assumed dead. The heartbeats are further described in~\cref{sec:background_monitoring}.

From that a node on which a replica is running fails, until a new replica is created, the reliability level may be lower than the desired level, hence the system in a vulnerable state. As further described in~\cref{sec:design_reliability_model}, the time it takes to detect node failures is therefore an important part of the reliability model.

In our model, heartbeats are sent to all nodes in the system every 200 ms. Furthermore, a node is considered dead when no heartbeat is received for 500 ms, which is an upper bound to how long it takes to detect a node failure. 

The frequency of the heartbeats, and the timeout time, is configurable and should be set depending on the desired behavior. A higher frequency and lower timeout means a shorter time in which the system is in a vulnerable state, and therefore a lower number of replicas may be needed to reach the desired reliability. On the other hand, highest frequency means more packages being sent thereby increasing network traffic in the system.

\subsection{Storage of data} \label{subsec:design_storage}
All information, such as how many replicas are currently executing, and on which nodes they are running, must be globally available and accessible from every node in the system. If stored on a single node, that information is lost in case that node fails. Also, if a remote database is used, we introduce a single-point-of-failure, resulting in a system where the reliability is no more than the reliability of the database. 

Instead, to achieve a redundant storage in our model, Distributed Hash Table (DHT) is used, which efficiently avoids having a single-point-of-failure. More specifially, the DHT implementation used is Kademlia, and we refer to~\cite{kademlia} for further info about DHT and Kademlia. The reason for chosing this approach is a result of Calvin, the framework in the model was implemented and evaluated, already used Kademlia. Calvin is further described in~\cref{subsec:design_calvin}.

\subsubsection{Updating storage}
As mentioned, in order to know where replicas are executing, this information must be stored and globally available. When a task is replicated to another node, the replication must only be considered successful if the storage is properly updated. Otherwise, the replication must be partially reflected in the storage and will become useless. Using DHT, the storage is properly updated when a node has sent the newly stored values to the other nodes. 
%TODO this text needs work, I just put it there not to forget about it, not sure it should be here
%TODO not sure this section is even needed...	

\subsection{Application model} \label{subsec:design_app_model}
The fault-tolerant framework presented in this paper is general and may be used in various contexts. However, it is particularly of value for long running applications and services running dynamic environments, and where a certain level of reliability must be met. Long running applications are particularly vulnerable to failure because they usually require many resources and usually must produce precise results ~\cite{relGridSystems}.

In this paper we use a simple example application in our experiments. The application can be modelled as shown in~\cref{fig:app_model}. The node $A$ shown in the figure could for example be a service for which we require a certain reliability. No assumptions are made about the execution time of the application. However, our model is particularly beneficial for long running applications and services, as they in large-scale distributed environments are required to stay operational even in the case of unpredictable failures~\cite{imprRelAdaptRL}.

%TODO actually... in the tests, there is no "A", but instead the producer is replicated... should update the tests to say we have this situations.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/app_model.pdf} 
\caption{An application model where a producer transmits data to a task $S$, which transforms the data, and sends the result to a consumer. $S$ may be seen as a task or service running in a cluster and requiring a certain level of reliability.}\label{fig:app_model}
\end{figure}

%TODO
COMMENT: we should have some real examples of applications which would benefit from using our framework. ?
\begin{itemize}
\item telephone system?
\item video trans-coding when streaming video to mobile phone? In this case one could imagine a case where a video file is located on a server with limited processing power. The video could therefore be streamed to another server in the cluster, with more processing power, which encodes it on the fly and stream the result to the user. To keep a continuous stream of data to the user, even in the case of failure of the encoding server, one could replicate the task and stream the video to several servers which encode and transmit the results to the user. While this would increase the amount of transmitted data to the user's mobile, it would also increase the user experience.
\item Long running simulations? \cite{relModelDistSimSystem}
\end{itemize}

\iffalse

COPIED:
“Services in large-scale distributed environments (e.g., The Internet of Things [15]) are required to stay and continue operating even in the presence of malicious and unpredictable circumstances; that is, their processing capacity must not be significantly affected by the user requirements [11,8] “ \cite{imprRelAdaptRL}.
\\\\
COPIED: “Most workloads are large-scale long-running 3D scientific simulations, e.g. for nuclear stockpile stewardship. These applications perform long periods (often months) of CPU computation, interrupted every few hours by a few minutes of I/O for checkpointing. “  \cite{studyOfFailures}
\\\\
COPIED: “given the long execution times of many of the parallel applications that we are targeting – those in the scientific domain at national laboratories and supercomputing centers. “  \cite{implicationsOfFailures}. 
\\\\
COPIED: "The end result is that long-running, distributed applications are interrupted by hardware failures with increasing frequency"  \cite{surveyFaultParallel}.

\fi

\subsection{Replication scheme} \label{subsec:design_repl_scheme}
As previously mentioned, reliability will be ensured by the use of replication. More specifically, active replication is used, where each replica receives the same input, and performs the same calculations. \Cref{fig:app_model_replication} shows how the application in \cref{fig:app_model} looks after replicating task $S$ 4 times.

In contrast to the case with one \emph{primary} replica and several \emph{secondary} replicas, as described in \cref{subsec:background_replication}, we will adapt a fan-in fan-out model, where all replicas both receive the same input, but also all transmit its result to the consumer. We do not count for whether or not the correct result is received at the consumer, but our model allows for easy extension to also include a majority decision at the consumer to determine whether or not the correct result was received. %TODO do we have to have this? I don't think it really affects our model right?

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/app_model_replication.pdf} 
\caption{An application model where a task or service $S$ has been replicated 4 times.}\label{fig:app_model_replication}
\end{figure}

\subsection{Fault model} \label{subsec:design_fault_model}
In this paper, we adapt the \emph{fail-stop} fault model, which is commonly used when presenting fault-tolerance techniques ~\cite{surveyFaultParallel}. Nodes in the system have one of two states: \emph{operational} or \emph{failed}. If a node fails, all running tasks on that node are dead. Furthermore, after a node has died, it will be restarted. However, the tasks that were being executed before it died will not be restarted when the node is restarted. The reason for why a node died is irrelevant. Our model do not care whether a link died, or it was a hardware failure. 

Like ~\cite{selfAdaptRel}, we assume failures depends on the nodes, not the jobs running on them and the computations they perform.

Unlike the reliability models presented in XXX TODO, we do not consider link failures in our model. However, in our experiments, link failures are indeed possible, but in the case of a link failure a node will be assumed dead, and a new replica created. Therefore, the desired reliability will still be met. %TODO we should have something here about links right? 


\subsubsection{Failure distribution}
%TODO do we want to assume Poisson process with constant failure rates? Maybe we should start with this and then later on use a more sophisticated model which for example takes the time a day into acocunt...
We assume failures follow a Poisson process, as this seems to be widely accepted in the research community. However, while most assumes constant failure rates, our failure distributions are dynamic and monitoring of the system resources and events allows for the framework to be self-adaptive and dynamic in terms of resource failure rates.

The Poisson distribution is defined as follows:

\begin{equation} \label{eq:Poisson}
P(k failures) = \dfrac{\lambda^k \cdot e^{-\lambda}}{k!}
\end{equation}

where $\lambda$ is the failure rate, i.e. the average number of failures occurring in time \emph{t}. In our case \emph{t} represents the time it take to detect a node failure and spawn a new replica. From \emph{t} and \emph{MTBF} we can calculate the failure rate $\lambda$ as $\lambda = t/MTBF$. In our case \emph{k} will always equal zero since we are interesting in knowing the probability that no failure occur during time \emph{t}. 

However, in order to use~\cref{eq:Poisson}, the MTBF of a node must be available. To calculate a node's MTBF at least two failures most have occurred since not all nodes are alive at time 0 which is assumed by~\cref{def:mttf}. However, the system still need to be able to calculate the MTBF for a node, even if the node has not yet failed. Therefore, a default value for the MTBF will be used in the case when no failure data is available for a node, or if it has not yet failed twice.

As time goes and nodes fail, the time of failure for nodes are stored, the system will get more precise values for the MTBF for the nodes in the system. The time it takes to detect node failure and spawn a new replica is further discussed in~\cref{sec:design_time_t}.

\section{Reliability model} \label{sec:design_reliability_model}
%TODO Maybe change section name
Here we present our main contribution, the self-adapting reliability model. But first we state some definitions of reliability.

\subsection{Definitions} \label{subsec:design_definitions}
Using replication of a task, and reliability defined as in \cref{def:task_replica_reliability}, the probability that a task $T$ with $n$ replicas is successful during a time $t$, corresponding to at least one replica survives, i.e. not all fail, can be expressed as 

\begin{equation} \label{eq:task_reliability}
R_{T}(t) =  1 - \prod\limits_{k=1}^n F_{Rep_k}(t)
\end{equation}

where $F_{Rep_k}(t)$ is the probability that replica $k$ fails during time $t$. \Cref{eq:task_reliability} corresponds to \cref{def:task_replica_reliability}, i.e. at least one replica is always running.

Since we assume tasks themselves do not fail unless the resources they use fail, the reliability of a task replica is dependent on the reliability of the resources it uses. Limiting the model to node failures, either in terms of hardware failure or lost connectivity, \cref{eq:task_reliability} can with \cref{eq:resource_reliability} be re-written as

\begin{equation} \label{eq:task_reliability_2}
R_{T}(t) = 1 - \prod\limits_{k=1}^m  F_{Res_k}(t)
\end{equation}

where $F_{Res_1}(t) \cdots F_{Res_m}(t)$ are the failure probability functions of the $m$ resources on which the $n$ replicas are running. Using this model, reliability is only increased through replication if the replicas are scheduled on separate nodes.

Given a time $t$, a desired reliability level $\lambda$, and assuming the replicas are running on separate nodes, we get

\begin{equation} \label{eq:desired_rel}
\lambda \leq 1 - \prod\limits_{k=1}^n  F_{Res_k}(t)
\end{equation}

which must be fulfilled by the system. Assuming that failure rates differ among resources, fulfilling \cref{eq:desired_rel} is a scheduling problem, since the number of replicas needed is dependent on which resources the replicas are executed on.

The scheduling problem to fulfill a reliability $\lambda$ refers to selecting $n$ nodes on which to place $n$ replicas such as the reliability level of the task, expressed in \cref{eq:task_reliability_2}, exceeds $\lambda$.

\subsection{Expressing time t} \label{sec:design_time_t}
The time $t$ used in \cref{eq:desired_rel} consists of the time it takes from that a failure happened, until a new replica is operational. $t$ can therefore be expressed as 

\begin{equation} \label{eq:rep_time}
	t = Tf + Tr
\end{equation}

where $Tf$ is the time to detect that a node has failed, and $Tr$ is the time it takes to spawn a new replica.

\subsubsection{Node failure detection time} \label{sec:node_failure_detection_time}
The time to detect a failed node depends on the frequency of the heartbeats. In our model, heartbeats are sent to all nodes in the system every 200 ms. A node is considered dead when no heartbeat is received for 500 ms, which is an upper bound to how long it takes to detect a node failure.

500 ms is only an upper bound since a node may fail just before sending a heartbeat, or directly after, which will affect the actual time it took to detect the failure. This corresponds to the best and worst case scenario. 

If a node A dies before sending a heartbeat to B, the last received heartbeat from A was for B received 200 ms ago. The timeout on that heartbeat will timeout after 500 ms, resulting in the node A has been dead for close to 300 ms before detecting it. 

In the latter case, node A dies directly after sending a heartbeat, since it will no longer send heartbeats, that one will timeout, and the time to detect it will therefore be 500 ms, which is thus an upper bound of detecting node failures.

It is safe to assume that the probability that a node dies just before sending a heartbeat and just after is the same we get a theoretical average detection time of 400 ms.

%TODO
"""Should we use the upper bound in our model, or the 'average value'???"""

\subsubsection{Replication time} \label{sec:replication_time}
The time it takes to replicate a task depends on the time to find out on which nodes current replicas are running, and sending a replicate request to one of these nodes. Since the reliability is dependent on which nodes the replicas are running on, not solely on the number of replicas, we must also find a node which has no replica already and include this in the request. The receiving node will then send a request to that node including the current state of the task to replicate. See~\cref{appendix:figures}.

\begin{definition}
Latency = propagation time + transmission time + queuing time + processing delay
\end{definition}

\begin{definition}
Propagation time = distance / propagation speed
\end{definition}

\begin{definition}
Handshake time = (ACK size / bandwidth) + (SYN+ACK size / bandwidth) + (ACK size / bandwidth)	
\end{definition}

\begin{definition}
Transmission time = package size / bandwidth
\end{definition}

\begin{definition}
Package size = handshake + message size
Handshake size = ACK + (SYN+ACK) + ACK
\end{definition}

\begin{definition}
Replication time = “node dead discovery time” + “time to retrieve data from storage” + “time to send data to node” + “time to replicate”
\end{definition}

\subsection{Scheduling algorithm} \label{subsec:design_sched_alg}
A simple greedy scheduling, similar to one presented in ~\cite{effTaskReplMobGrid}, which fulfills \cref{eq:desired_rel} is shown below:

\begin{algorithm} 
	\caption{Greedy scheduling algorithm} \label{alg:scheduling}
	\begin{algorithmic}[1]
	\State $n\gets 0$
	\State $nodes\gets available\ nodes$\Comment{sorted by reliability, highest first}
	\While {$\lambda \geq \prod\limits_{k=1}^n  R_{Res_k}(t)$}
		\State $node\gets nodes.pop$\Comment{take the node with highest reliability}
		\State{$replica\gets new\ replica$}
		\State
		\Call{place replica on node}{$replica$, $node$}
		\State $n\gets n + 1$
	\EndWhile
	\end{algorithmic}
\end{algorithm}

This algorithm is run by a master node, selected when failure is detected. The selection process is further described in~\cref{subsec:design_handling_failure}.

\subsection{Handling node failure} \label{subsec:design_handling_failure}
%TODO not sure where to put this section
In the case of a node failure, one must first determine whether or not any replicas were running on the failed node, and if so, determine whether or not any new replicas are needed by running the algorithm in~\cref{subsec:design_sched_alg}. However, when a node fails and thereby stop sending heartbeats to the other nodes in the system, the other nodes will all be aware of the failure. If all other nodes start running the algorithm, one could end up in a situation where every remaining node creates a new replica. This will of course only increase the reliability, but we will no longer ensure the optimal number of replicas being used. 

To cope with this, selection process must take place to select a single node which will be responsible for deciding what actions to take. Since assuming a dynamic system where nodes fail and new nodes are introduced, this selection must be done every time a node dies. Furthermore, the selected node may also die before it managed to create any new replica. Therefore, all other nodes will send a \emph{lost node} request to the selected node. When finished, the selected node sends a reply back whether or not the it managed to create a new replica if this was required to fulfill the required reliability. If the sending nodes does not receive a reply for some time, they assume the selected node died, and a new selection process will begin.

The algorithm is shown below: % Om vi ska ha ":" får vi tvinga algoritmen att ligga här relativt till texten. Går att göra på något sätt, kommer inte ihåg just nu.

\begin{algorithm}\label{alg:node_failure}
	\caption{Handling a failed node}
	\begin{algorithmic}[1]
	\State $nodes\gets live\ nodes$\Comment{Alive nodes sorted by ID}
	\Do
		\State $node\gets nodes.pop$\Comment{take the node with highest id}
		\State
		\Call{lost node request}{$node$}
		\State {wait for reply}
		\State $reply\gets reply\ from\ node$
	\doWhile {$reply\ is\ not\ successful\ OR\ request\ timeout$}
	\end{algorithmic}
\end{algorithm}

A small example is presented in~\cref{fig:handling_node_failure}. In the situation shown in the figure, there are 4 connected nodes, A, B, C and D, and the node C has just failed. When the other nodes (A, B and D) detect the failure they will select the node with the highest ID, in this case node A, and inform it that node C has failed. Node A will then determine which actions to take.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/handling_node_failure.pdf}
\caption{4 interconnected nodes after nodes A, B and D have detected the failure of node C.}\label{fig:handling_node_failure}
\end{figure}

\subsection{Self-adapting} \label{subsec:design_self_adapting}
Adaption refers to changing the behaviour depending on the changing state of the system. For a distributed system, resources most be continously monitored in order to adapt to changing behaviour ~\cite{imprRelAdaptRL}.

To ensure a certain level of reliability, the framework must ensure both that the current state of the system is taken into account when calculating the number of replicas needed. Furthermore, the system and the running jobs must be continuously monitored, and the number of replicas must be increased is the reliability of a running job decreases below the desired level, or, the number of replicas must be decreased in order not to waste resources.

When monitoring the system, one must monitor all parameters affecting the reliability. In our case, we consider only the nodes' mean-time-between-failures, the time to detect node failures and the time it takes to spawn a new replica.

The replication time is stored per actor type. Since the state of different types of actors vary, and thereby the size of the states, the time to send the state from one node to another will also vary. 

% TODO machine learning section?
TODO: If we have time:
Furthermore, we will take node CPU usage, and the time of day of the event into account in our fault model. This is based on that resource failure depends on both system load and time of day ~\cite{implicationsOfFailures} \cite{studyOfFailures}.

\iffalse
COPIED: “adaptation to changing conditions is achieved by both adaptive scheduling and adaptive execution” \cite{evalOfGridRel}.

COPIED:
"the distributed systems require consistent and iterative monitoring for valuation resources’ behaviors and processing requirements. Therefore, an autonomous, scalable and highly dynamic learning approach is deserved \cite{imprRelAdaptRL}

%TODO predict future MTBF? machine learning? cpu, memory usage, network load, etc.

COPIED:
“The individual component can be monitored in real time and updates the parameters dynamically for the exponential distribution. The monitored information is simple: just the number of failures over the total running time of this component which has actually been recorded by log files in today’s grids. Such a dynamic updating scheme can further validate the exponential assumption, though we may relax the above assumption somewhat to allow reasonable or gradual change in the failure rate (such as wear out) because, during a short enough period of service time, the parameter cannot change too much and using the latest value should be a good approximation “ \cite{hierarchicalRelModeling}.

COPIED: “in the proposed system, we need to determine the degree of over-provisioning or job replicas as small as possible in order to minimize the system overhead “ \cite{designFaultTolerantSched}.
\fi

\section{Implementation} \label{sec:design_implementation}
For implementing our model, we use an actor based application environment called \emph{Calvin} ~\cite{calvin}.

\subsection{Calvin} \label{subsec:design_calvin}
Calvin is a light-weight actor-based application environment for IoT applications, and is written in Python. It was developed by \emph{Ericsson} and made open source in the summer of 2015. While Calvin is an application environment for IoT applications, is suites well for implementation of our model.

\subsubsection{Storage} \label{sec:calvin_storage}
Calvin uses the Kademlia implementation of a DHT. This enables that any data stored by a node is distributed and available for all nodes in the network. DHT will not be further explained in this thesis, instead we refer to~\cite{TODO Kademlia} for further information about DHT and Kademlia especially.

\subsubsection{Actor model}
An application in Calvin consist of a set of connected \emph{actors}. An actor usually represents part of a device, service or computation. Actors communicate by sending data on their out-ports to other actors in-ports. The runtime, see~\cref{sec:calvin_runtime}, in which an actor is executing is responsible for the communication between runtimes and between actors.

The state of an actor is needed when replicating an actor. In Calvin, the state of an actor consists mainly of actor type, in-port and out-port connections, and for each port a queue with data to process or to send, and read and write positions for the incoming and outgoing queues. The queues is the major part of the message size.

\subsubsection{Runtimes} \label{sec:calvin_runtime}
The Calvin framework use a concept of \emph{runtimes}. A mesh of connected runtimes makes up the distributed execution environment on which one can deploy application. A runtime is a self-managed container for application actors and provides data transport between runtimes and actors. Each runtime has a \emph(storage), all the information stored in storage is first stored locally and later flushed, i.e. distributed in a multicast approach.

\subsection{Our contributions} \label{subsec:design_contributions} % ish?
Here follows a small description of the most important parts of our implementation.

\subsubsection{Actor replication}
We extended the Calvin framework to allow a fan-out/fan-in connectivity model where actors can have multiple producers (fan-in) and multiple consumers (fan-out). 

Furthermore, we implemented functionality for dynamically connecting and disconnecting actors, which allowed for dynamically adding and removing replicas of an actor.

%Maybe mention that due to no consensus actors only use the first received value

\subsubsection{Node Resource Reporter}
We have added an actor which automatically deploys on a runtime at runtime started-up. It reports the node's CPU usage once every second to the runtime which distributes it to all connected nodes. Each node has a resource manager which stores the other nodes' usages.

%TODO we currently do not use the CPU usage, this is therefore not neccessary?

\subsubsection{Resource Manager}
Besides storing the connected nodes usages the resource manager stores information about node failures. If a node fails a time stamp and the failed node's usage is stored, among with the time it took to replicate a new replica for each of the actors which were running on the failed node.


\subsubsection{Heartbeat Actor}
As each node periodically sends updates to the other nodes, it could be used as a heartbeat system. However, the messages and sent using TCP, which involved a initial handshake to setup the communication channel. For a heartbeat system, this is unnecessary overhead. Fort this reason, every runtime also starts a heartbeat actor, which periodically sends heartbeats in form of UDP messages to the other nodes. It also listens on a port and registers incoming heartbeats.

For every heartbeat sent, a timeout of 500 ms it set. When it timeouts, the node to which the heartbeat was sent is assumed dead, and when a heartbeat is received, all timeouts related to that node is canceled.

\subsubsection{Application monitor}
Each runtime also periodically monitors the applications deployed to it. We have also added an application monitor on each node that once every five seconds checks the current reliability for all actors in the applications started on the node. 

% Mention any time at all? Five seconds??
If the current reliability is not achieved more replicas is created and if the reliability is unnecessary high replicas are deleted.

\subsubsection{Failed node mechanism}
When the monitor reports that a node has failed, a ... is executed. First information about all actors that were running on the failed node are fetched from storage, the actors marked for achieving a certain reliability is replicated to new nodes until the required level of reliability is reached. 


\chapter{Evaluation} \label{ch:evaluation}
In order to validate our model, a set of experiments was conducted. The goal of the experiments were to show that our model dynamically ensures the desired level of reliability is met, despite the event of node failures and changing properties of the system, and doing so in an optimal way by choosing the most reliable nodes and thereby the minimum number of replicas.

\section{Computational environment} \label{sec:eval_comp_env}
As mentioned in \cref{ch:design}, the model was implemented in the IoT application environment Calvin. In the experiments, a cluster consisting of 6 inter-connected servers with homogeneous hardware components were used. A laptop was also used when measuring the replication time. The specification of the servers and the laptop are described in~\cref{sec:server_spec} and \cref{sec:laptop_spec}. The various servers will be referred to as \emph{Gru} (stable), \emph{Dave}, \emph{Jerry}, \emph{Mark}, \emph{Jerry} and \emph{Tim}.

On five of the six servers used, one or two (depending on experiment) Calvin runtimes were started and periodically killed and restarted, thereby simulating node failures. As described in~\cref{sec:TODO HERE}, our model needs at least two failure times for a given node to calculate its MTBF and thereby its reliability. In the experiments, before two failures has happened, all nodes will use a default value of 10 seconds as MTBF. On the sixth server a single runtime was started, which was not killed.

The reason for having a stable runtime was to have the producer and consumer deployed to this runtime, while a service actor were deployed to the failing runtimes. The application is further described in~\cref{sec:eval_application}.

\subsection{Simulating node failure} \label{sec:simulating_node_failure}
As mentioned, the runtimes were periodically killed and restarted to simulate node failures. The time between starting and killing the runtimes varied for the experiments and in some cases even between the runtimes as well.

The process of killing and restarting is shown in~\cref{alg:simulating_node_failures}. If a node was to have a MTBF of 15 seconds, the time between failures followed a normal distribution with mean 15 and standard deviation of 1. Since the tests only ran for a couple of minutes, the actual MTBF for a given node may during the duration of a test be either lower or higher than the given MTBF.

\begin{algorithm} 
	\caption{Simulating node failures} \label{alg:simulating_node_failures}
	\begin{algorithmic}[1]
	\While {$true$}
		\State
		\Call{start runtime}{}
		\State $t_{s}\gets \mathcal{N} (MTBF,1)$
		\State
		\Call{sleep $t_{s}$}{}
		\State
		\Call{kill runtime}{}
	\EndWhile
	\end{algorithmic}
\end{algorithm}

\subsection{Server specification} \label{sec:server_spec}
The servers used in the experiments all have a Intel(R) Xeon(R) CPU E5-2420 v2 of 2.20GHz and 24 GB RAM. Furthermore, they are all connected with a 1000 Mb/s link with a latency of less than 0.2 ms. 

\subsection{Laptop specification} \label{sec:laptop_spec}
The laptop used in the experiments is a Dell Vostro v131 with a Intel i5, 2.3 GHz processor, 4 GB 1333 MHz RAM. It was equipped with a Samsung SSD with a read speed of 540 MB/s and a write speed of 520 MB/s. 
%TODO mention SSD speed? maybe of value when talking about swap memory... 

\subsection{Application} \label{sec:eval_application}
The application used in the experiments is of simplest form, consisting only of one consuming, one producing actor, and one service actor for which a certain reliability is desired. 

The producing actor produced integer numbers and sent those to the  service actor which simply forwarded them to the consuming actor, which printed the values to standard out. Since both the producing and the consuming actor are on a stable server, only the service actor was to be replicated by the system.

The computational environment and the application is shown in~\cref{fig:evaluation_application}.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/evaluation_application.pdf} 
\caption{The computational environment used in the experiments. In this example, there is a producer and consumer on Gru, and two replicas of the service located on Dave and Mark.} \label{fig:evaluation_application}
\end{figure}

%REPLICATION TIME:
%An average of the last "self.history" (default is 5) times. We consider these values to be most relevant. 

\section{Experiments}

\subsection{Measurement of time $t$} \label{sec:eval_time_t}
As mentioned in~\cref{sec:design_time_t}, the time $t$ is the time it takes from that a node on which a replica dies, until a new replica is up and running, and depends on the time it takes to detect that the node died, and the time it takes to create a new replica.

\subsection{Measurement of node failure detection time} \label{subsec:eval_node_fail_time}
% Is this needed really?
As described in~\cref{subsec:design_monitoring}, heartbeats are used to decide whether or not another node is healthy. If no heartbeat is received from a node within 500 ms, it is assumed dead. 

In order to measure the time it takes to detect a failed node, an experiment was conducted in which two servers were used, each with a Calvin runtime. One of the runtimes was periodically killed and restarted. Before killing the runtime, the current time was logged, and on the other node, the time was logged when it detected the loss of connectivity to the killed node. The failing node was killed and restarted 50 times.

\subsubsection*{Result}
The average of the 50 measured times was 483.8 ms. This above the theoretical average of 400 ms, but lower than the upper bound of 500 ms.

% corresponds well to the theoretical average of 500 ms. %TODO this experiment is not needed if we set a timeout of 500 ms instead of increasing a count everytime we send a heartbeat and assume lost when we have sent 3 without receiving a reply. Atm, sending every 200 ms and assuming dead when count > 2, the average case is 500 ms. But using a timeout one can have a deterministic timeout.

\subsection{Measurement of replication time} \label{sec:eval_repl_time}
The time it takes to replicate a task, depends on the size of the task state, which have to be sent to the new node. Using Calvin, the state of a task corresponds to the state of the actor representing the task. In Calvin, the state consists mainly of the queues of incoming and outgoing data for the actor's in- and out-ports.

In this experiment, the actor had only one out port and no in port. The state of the actor was incrementally increased by increasing the size of its output queue, and the time it took to replicate the actor to another runtime was measured.

The experiment was conducted in two settings. In one, two servers were used with a runtime on each of them, while in the other, a laptop was used on which both the two runtimes were started. The specification of the servers and the laptop are found in~\cref{sec:server_spec} and ~\cref{sec:laptop_spec}.

After starting the actor on one of the two runtimes, it was replicated to the other one 10 times and the time each replication took was measured. 

\subsubsection*{Result}
\Cref{fig:replication_time} shows how the replication time vary depending on the state size. \Cref{fig:replication_time_less_than_40} show replication time for messages of size less than 40 MB. 

Replicating an actor consists of getting the state of the actor, sending the state to another node, starting a new actor with that state, and sending the reply. \Cref{fig:replication_time_percentages_server} and ~\cref{fig:replication_time_percentages_laptop} show how much time is spent on each part of the replication when replicating from server to another server, and from laptop to laptop respectively.

As seen in the figure, the replication time is exponential. For the laptop, there is a big increase in time when increasing the state size from 160 MB to 240 MB. This is due to running out of memory, and the process starting using the system's SWAP memory, which significantly reduced the performance. This is also why the time to setup the new actor takes a significantly larger part of the total time for 160 MB than for 80 MB, shown in ~\cref{fig:replication_time_percentages_laptop}. A 1333 MHz ram memory has a speed of approximately 21 GB/s, which compared to the SSD speed of 540 MB/s explains the significantly drop in performance.

As shown in~\cref{fig:replication_time_percentages_server}, as long as one does not run out of memory, the time to send the state is clearly the bottleneck, despite the two servers in the experiment were connected with a 1000 Mb/s link with a latency of less than 0.2 ms. With a slower connection, sending the state would probably be ever larger part of the total replication time.


\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/replication_time.pdf} 
\caption{Result for test~\ref{sec:eval_repl_time}. The average replication time as a function of state size.}\label{fig:replication_time}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/replication_time_percentages_server.pdf} 
\caption{Result for test~\ref{sec:eval_repl_time}. Part of total replication time spent on fetching the actor state, sending the state, setting up the new actor, and sending reply when replicating from server to another server.}\label{fig:replication_time_percentages_server}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/replication_time_percentages_laptop.pdf} 
\caption{Result for test~\ref{sec:eval_repl_time}. Part of total replication time spent on fetching the actor state, sending the state, setting up the new actor, and sending reply when replicating from laptop to laptop.}\label{fig:replication_time_percentages_laptop}
\end{figure}

\subsection{Ensuring a certain reliability level} \label{sec:eval_rel_level}
In this experiment, each runtime was given pseudo-random numbers according to a normal distribution with mean of 20 and a standard deviation of 1, i.e. the 6 runtimes all had the same MTBF, 20 seconds. Since nothing is known about failure times when the system first was started, it was assumed the MTBF for a node was 10 seconds.

After the application was started, the reliability level of the nodes on which replicas were running was periodically measured. The desired level of reliability for the producing actors were 0.98.

The test was considered successful if the average reliability level during the duration of the test exceeded 0.98.

\subsubsection*{Result}
The experiment ran for 5 minutes, and \cref{fig:exp_reliability_level} shows how the reliability vary during this period. The average reliability for the duration of the whole test was 0.996, clearly above the desired level of 0.98.

Every drop in reliability corresponds to a failure of one of the nodes on which a replica is executing.

As shown in the figure, the reliability was lower in the beginning of the test. This is due the lack of failure data for nodes, therefore assuming a MTBF of 10 seconds. As the nodes starts failing, the system learns their actual MTBF, which in the test was 20 seconds.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/reliability.pdf}
\caption{Result for test~\ref{sec:eval_rel_level}. Reliability over time in a system with failing nodes.} \label{fig:exp_reliability_level}
\end{figure}

\subsection{Optimal number of replicas}
\subsubsection{Chosing the most reliable} \label{sec:eval_opt_nbr_replicas}
Similar to the previous experiment, each runtime was given pseudo-random numbers according to a normal distribution. However, in this experiment, the mean value varied among the runtimes. The mean values given to the different nodes are presented in \cref{table:exp_nodes_means}, and all had a standard deviation of 1. Since nothing is known about the nodes when the test starts, it was assumed the mean-time-between-failure were 10. 

\begin{table}[h]
	\begin{center}
	\begin{tabular}{| c  | c |}
	 \hline
	 node & MTBF (s)  \\
	 \hline		
	  $dave_1$ & 7.5 \\
	  $dave_2$ & 7.5 \\
	  $tim_1$ & 7.5 \\
	  $tim_2$ & 7.5 \\
	  $kevin_1$ & 15 \\
	  $kevin_2$ & 15 \\
	  $mark_1$ & 15 \\
	  $mark_2$ & 40 \\
	  $jerry_1$ & 40 \\
	  $jerry_2$ & 40 \\
	   \hline
	\end{tabular}
	 \caption{Mean-time-between-failures for the eight non-stable runtimes in the experiment ~\ref{sec:eval_opt_nbr_replicas}.}
	 \label{table:exp_nodes_means}
	 \end{center}
 \end{table}


After the application was started, the reliability level of the nodes on which replicas were running was periodically measured. The desired level of reliability for the producing actors were 0.999.

The test was considered successful if the number of replicas used decreased over time, as the system learned the actual MTBF for the nodes, and consequently more reliable nodes could be chosen. Furthermore, for the test to be successful, the average reliability for the duration of the test should exceed 0.999.

\subsubsection*{Result}
%TODO
The experiment ran for 5 minutes, and ~\cref{fig:exp_opt_replicas_total} shows the number of replicas over time, while~\cref{fig:exp_opt_replicas_MTBF_30}, ~\cref{fig:exp_opt_replicas_MTBF_15}, and ~\cref{fig:exp_opt_replicas_MTBF_75} show the number of replicas per node over time.

As shown in the figures, the number of replicas decreased over time, from three replicas at the start of the test, to later only needing two. Furthermore, ~\cref{fig:exp_opt_replicas_MTBF_30} shows that the more reliable nodes, $mark_2$, $jerry_1$, and $jerry_2$, were chosen more often than the less reliable ones, shown in ~\cref{fig:exp_opt_replicas_MTBF_30} and ~\cref{fig:exp_opt_replicas_MTBF_75}.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/optimal_replicas/total.pdf}
\caption{Result for test~\ref{sec:eval_opt_nbr_replicas}. Total number of replicas.} \label{fig:exp_opt_replicas_total}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/optimal_replicas/MTBF_30.pdf}
\caption{Result for test~\ref{sec:eval_opt_nbr_replicas}. Number of replicas per node, for the three nodes with a MTBF of 30 seconds.} \label{fig:exp_opt_replicas_MTBF_30}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/optimal_replicas/MTBF_15.pdf}
\caption{Result for test~\ref{sec:eval_opt_nbr_replicas}. Number of replicas per node, for the three nodes with a MTBF of 15 seconds.} \label{fig:exp_opt_replicas_MTBF_15}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/optimal_replicas/MTBF_75.pdf}
\caption{Result for test~\ref{sec:eval_opt_nbr_replicas}. Number of replicas per node, for the four nodes with a MTBF of 7.5 seconds.} \label{fig:exp_opt_replicas_MTBF_75}
\end{figure}

\subsubsection{Optimal replicas}
\label{sec:eval_opt_nbr_replicas_2}
In order to show that the system automatically reduces the number of replicas in case more reliable nodes are available, an experiment was conducted in which five computing nodes were used. Two of these nodes were given a MTBF of 25 seconds, while the other three did not fail. Since those three nodes did not fail, the system used the default MTBF of 10 seconds. Despite not failing, the system considered them less reliable than the two failing nodes with MTBF of 25 seconds. The reason for not killing those three nodes were simply in order to show that the system automatically reduces the number of replicas, not only when a node fails. The MTBF for the nodes used in this experiment are shown in \cref{table:exp_nodes_means}.

The required reliability in the experiment was set to 0.999. This results in that as long as the two reliable nodes are available, only two replicas are needed, but if one of them or both are dead, three replicas are needed to reach the required reliability. Finally, a delay of 5 seconds were used between killing and restarting nodes \emph{kevin} and \emph{jerry}.


\begin{table}[h]
	\begin{center}
	\begin{tabular}{| c  | c |}
	 \hline
	 node & MTBF (s)  \\
	 \hline		
	  $dave$ & 10 \\
	  $tim$ & 10 \\
	  $mark$ & 10 \\
	  $kevin$ & 25 \\
	  $jerry$ & 25 \\
	   \hline
	\end{tabular}
	 \caption{Mean-time-between-failures for the five runtimes in the experiment ~\ref{sec:eval_opt_nbr_replicas_2}.}
	 \label{table:exp_nodes_means_2}
	 \end{center}
 \end{table}


\subsubsection*{Result}
As mentioned, the default MTBF is 10 seconds when no failure data is known for a node. This means when the more unreliable nodes have failed twice, their MTBF will be 25 seconds, higher than the default value. But before the reliable nodes have failed twice, their mtbf will be 10 seconds. This is why there is a period during which none of the reliable nodes are given any replicas. After their seconds failure however, it is known that they are actually more reliable than the other ones, why they are then given replicas.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/optimal_replicas/2/total.pdf}
\caption{Result for test~\ref{sec:eval_opt_nbr_replicas_2}. Total number of replicas.} \label{fig:exp_opt_replicas_total_2}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/optimal_replicas/2/MTBF_25.pdf}
\caption{Result for test~\ref{sec:eval_opt_nbr_replicas_2}. Number of replicas per node, for the two nodes with a MTBF of 25 seconds.} \label{fig:exp_opt_replicas_MTBF_25_2}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/optimal_replicas/2/MTBF_10.pdf}
\caption{Result for test~\ref{sec:eval_opt_nbr_replicas_2}. Number of replicas per node, for the three nodes which were not killed, and therefore were assumed to have a MTBF of 10 seconds.} \label{fig:exp_opt_replicas_MTBF_10_2}
\end{figure}


\subsection{Self-adaptive reliability model} \label{sec:eval_adaptive_rel_model}
%TODO the mean-time-between-failure does not vary, really... must rephrase this sentence
In this experiment, the MTBF for the various nodes varied over time, following a $sinus-curve$. The pseudo-random numbers for the nodes followed a normal distribution with mean 25 and standard deviation of 1. However, the time to sleep between failures were calculated by ~\ref{eq:eval_sleep_time}. This means the time between failures varied between 5 and 45 seconds. The required reliability was set to 0.99999. This value was chosen so that the number of replicas needed would vary between 3 and 5, depending on the current state of the system.

%TODO how long should before we're back? 60 sec?
\begin{equation} \label{eq:eval_sleep_time}
T_{sleep} = t_i + 20 * \sin{(2\*\pi\*T_{elapsed} / 300)}
\end{equation}

Where $t_i$ are the numbers following a normal distribution with mean 25 and standard deviation of 1, and $T_{elapsed}$ is the total elapsed time for the test. Given $t_i=25$, \cref{fig:eval_sleep_time} in ~\cref{appendix:figures} shows how the actual time to sleep varies over time.

The test was considered successful if the number of replicas used increased as the time between failures decreased for the various nodes, after which it increased as the time between failures increased.

\subsubsection*{Result}
The test ran for 30 minutes. \Cref{fig:eval_self_adaptive_rel} shows how the number of replicas varied over time, and ~\cref{fig:eval_self_adaptive_node_rels} shows how the calculated reliability for the various nodes vary over time. 

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/self_adaptive_replicas.pdf}
\caption{Result for test ~\ref{sec:eval_adaptive_rel_model}. Reliability over time with failing nodes and varying time between failures.} \label{fig:eval_self_adaptive_rel}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/self_adaptive_node_rels.pdf}
\caption{Result for test ~\ref{sec:eval_adaptive_rel_model}. Reliability over time with failing nodes and varying time between failures.} \label{fig:eval_self_adaptive_node_rels}
\end{figure}


\chapter{Discussion} \label{ch:discussion}
%TODO
\iffalse
Större drag, vad kan vi få ut av alla experimenten?
Hur mycket kan vi lite på resulaten?
Är det worst-case/best-case?
Kommer det bara bli bättre/snabbare med tiden?
Generalla slutsater
Vi kan ej generalisera våra resultat. typ.
\fi

When using a fail-stop fault model (see \cref{subsub:background_fail_stop}), we assume that when a node dies all other nodes are aware of this. This is clearly limiting, as in the case of a link failure. Assume we have three nodes $A$, $B$ and $C$ and there is one replica on $A$ and one on $B$. In case of a link failure between $A$ and $B$, $A$ will assume that $B$ has failed and vice versa and both will ask the node with highest id to replicate the lost actors. If $C$ has the highest id of all three then nothing will happen since $C$ is aware of the replicas on both $A$ and $B$. But if $A$ has the highest id it will send the replication request to itself and since it's not aware of that there is a running replica on $B$, it will replicate its actor to $C$. Thereafter we have an unnecessary high reliability and, from a system perspective, there is a unnecessary high impact on the network load.

Our definition of reliability excludes the possibility of tasks calculating incorrect results, which is the case when using a Byzantine fault model. Since already using active replication to ensure reliability, it would be trivial to extend it to use consensus techniques such as majority voting or \emph{k-modular redundancy} in order to extend the reliability definition to also include that the job calculates the correct result.

Our primary objective is to ensure a certain level of reliability. By using active replication, we require a lot more resources, which puts an extra burden on the system. In addition, the extra load on the system may affect the execution time, thus decreasing task performance.

Our model is yet to be evaluated on highly unreliable system during extreme load. In this case, due to the unreliability of the system, the number of replicas needed to ensure the desired reliability level will increase. This will further increase the system load, thereby decreasing the reliability of the system even further. This may turn into a vicious circle.

Our model ensures that the reliability is higher than the required as long as we don't experience a node failure. After a node failure and before the system has recovered (replicated the lost actors) the reliability is lower than the required. Our model does not guarantee that the average reliability is higher than the required, it depends on the relationship between the MTBF and the time it takes to replicate an actor.

The reliability is dependent on the replication time, which depends on the state of the task to replicate. The Calvin framework is only designed for light-weight IoT applications, thus not for applications where the actor state is several megabytes, and not optimized for this kind of work. Despite this, the experiments show that it takes less than one hour to replicate an actor with a state of 1 gigabyte. Assuming a mean-time-between-failures of one year (31536000 s), and an replication time of 1 hour (3600 s), having only two replicas still according to \cref{eq:task_reliability_2} gives a reliability of

\begin{equation*}
\begin{split}
R(3600) = e^{3600/31536000}\\
\\
R_{T}(3600) = 1 - \prod\limits_{k=1}^2  F_{Res_k}(3600)\\
= 1 - \prod\limits_{k=1}^2  (1 - R(3600))\\
= 1 - (1 - e^{3600/31536000})^2\\
=  0.999999986302961
\end{split}
\end{equation*}

This means despite having a large replication time, a high reliability can be achieved by only having two replicas.

If our model is to be used in an environment where not all nodes are located within the same cluster, one must take into account that the replication time will be higher when replicating to a node in another cluster than within the same cluster.

Finally, since using a \emph{default MTBF} of nodes for which we have not yet experienced failure, our model is obviously limited, as this default value may be incorrect. To use in a real-world situation by a service provider, the reliability model must be extended. To cope with the MTBF, one could use past failure data from their cluster. Furthermore, the model should be extended to also take into account parameters like system load, link failures, etc. 

% Should we afford losing the most reliable node? According to definition rel = prob that we don't loose all OTHER nodes during a node failure. What if we dont have a node failure? Simulate one by not including the most reliable node in the calculations of rel.

\chapter{Future Work} \label{ch:future_work}
To the best of our knowledge our model is the first of it's kind, due to its fully dynamic behavior. Therefore we had to limit our scope a lot and considering these limitations there is a lot of possible future work. Our model could be extended by considering more types of failures, e.g. link failure and soft failures (e.g. an actor dies but not the node or the actor produce an incorrect result). A consensus algorithm should be implemented in order to hamper the risk of using incorrect results. Link failures could, for instance be detected if the node responsible for replicating actors after a node failure awaits request from several nodes. If it only receives one node fail request among ten nodes its safe to assume that the node is still alive and the link to the requesting node has failed.

There are a lot of different kinds of failures which could be taken into account, see \cref{subsec:future_extended_model} for an extended reliability model.

Our model could also be extended with some sort of machine learning in order to predict future failures. In the case of a video service its likely that the system load is higher in the evening and therefore also the probability of a failure. By predicting these failures replicas an be created in advance, i.e. when the probability of failure reaches above a certain threshold.

In our experiments we have had only one actor to replicate on the lost node, in bigger systems with many actors on each node we get the complexity of choosing which actor to replicate first. A solution could be to have a criticality of each actor type and start replicating the most critical actor on the lost node. However, replication time is defined as the time it takes from that we start replicating actors at all after a node failure to that the actor is up and running, i.e. all actors replication times have the same start time. This will result in that the least critical actor will have the largest replication time and thereby being having the most amount of replicas. 
Another, perhaps better solution would be to replicate the actors with the biggest states first in order to achieve an uniformly distribution of the replication times as possible. We leave the investigation of which algorithm is the optimal to future work.

%TODO
\iffalse
NOTES:
Replication impose extra burden on the system as additional resources are needed and computational power is wasted. By combining checkpointing techniques with active replication, the number of replicas needed could possible be decreased.(~\cite{adaptiveCheckPointAndRep} combines checkpointing and replication)
\\\\
Predict future failures - machine learning (many parameters could be taken into account), and when the probability of failure reaches above a certain threshold, the running tasks on that node could be migrated. A high reliability in failure prediction allows for fewer replicas to be needed.
\\\\
Implement consensus in Calvin system
\\\\
Accounting for link failures
\\\\
Test on highly unreliable systems during extreme load
\fi

%Should we have this section?
\section{Extended model} \label{subsec:future_extended_model}
Reliability of server’s availability and scalability (such as File Server, DB servers, Web servers, and email servers, etc.), communication infrastructure, and connecting devices~\cite{surveyReliabilityDistr}.

For measuring reliability more accurate, more factors must be included (not only node failures). The overall reliability can then be expressed as:

\begin{equation} \label{eq:overall_reliability}
R(t) = R_{1}(t) \cdot R_{2}(t) \cdots R_{n}(t)
\end{equation}
where $R_{k}(t)$ is the probability that factor $k$ is free from failures during time $t$. Some factors to consider are software (the program itself), OS (the device executing the program), hardware, network, electrical supply and load. The factors can be divided into static and dynamic factors. The static factors are those which does not change that frequently, such as electrical supply or hardware/software while the dynamic factors are those changing more frequently, for instance the current load (or load average for last 5 minutes etc). 


\chapter{Conclusions} \label{ch:conclusions}

\begin{thebibliography}{50}

\bibitem{taskAllocation}
	Sol M. Shatz, Jia-Ping Wang and Masanori Goto,
	\emph{Task Allocation for Maximizing Reliability of Distributed Computer Systems},
	Computers, IEEE Transactions on, Volume 41  Issue 9,
	Sep 1992
	
\bibitem{surveyReliabilityDistr}
	Waseem Ahmed and Yong Wei Wu,
	\emph{A survey on reliability in distributed systems},
	Journal of Computer and System Sciences Volume 78 Issue 8,
	December 2013, Pages 1243–1255
	
\bibitem{surveyRelPrediction}
	A. Immonen, E. Niemelä,
	\emph{Survey of reliability and availability prediction methods from the viewpoint of software architecture},
	Software \& Systems Modeling, p.49-65,
	February 2008

\bibitem{relDistApplications}
	C. A. Tănasie, S. Vîntutis, A. Grigorivici,
	\emph{Reliability in Distributed Software Applications},
	Informatica Economică vol. 15, no. 4/2011,

\bibitem{relGridSystems}
	Christopher Dabrowski,
	\emph{Reliability in grid computing systems},
	Concurrency Computation: Practice Experience,
	2009

\bibitem{compStudyLoadAndCloud}
	Mayanka Katyal and Atul Mishra,
	\emph{A Comparative Study of Load Balancing Algorithms in Cloud Computing Environment},
	International Journal of Distributed and Cloud Computing, Volume 1 Issue 2,
	2013
	
\bibitem{relAndPerfGridServices}
	Y. Dai, G. Levitin,
	\emph{Reliability and Performance of Tree-Structured Grid Services},
	IEEE transactions on reliability, Vol. 55, No. 2, 
	June 2006

\bibitem{surveyFaultParallel}
	Michael Treaster,
	\emph{A Survey of Fault-Tolerance and Fault-Recovery Techniques in Parallel Systems},
	Cornell University Library,
	Jan 2005

\bibitem{faultTolerantFundamentals}
	F. C. Gärtner,
	\emph{Fundamentals of Fault-Tolerant Distributed Computing in Asynchronous Environments},
	ACM Computing Surveys Vol. 31 Issue 1 p. 1-26, 
	March 1999 

\bibitem{gridWorkflow}
	Soonwook Hwang and Carl Kesselman,
	\emph{Grid Workflow: A Flexible Failure Handling Framework for the Grid},
	High Performance Distributed Computing, 2003. Proceedings. 12th IEEE International Symposium on, p. 	126-137, 
	June 2003

\bibitem{perfAnalysisLoadCloud}
	Prashant D. Maheta , Kunjal Garala and Namrata Goswami,
	\emph{A Performance Analysis of Load Balancing Algorithms in Cloud Environment},
	Computer Communication and Informatics (ICCCI), 2015 International Conference on,
	Jan 2015
	
\bibitem{taskSchedulingReplication}
	Shuli Wang et. al.
	\emph{A Task Scheduling Algorithm Based on Replication for Maximizing Reliability on Heterogeneous Computing Systems},
	Parallel \& Distributed Processing Symposium Workshops (IPDPSW), 2014 IEEE International, p. 1562 1571, 
	May 2014

\bibitem{softRelRoadmap}
	Michael R. Lyu
	\emph{Reliability Engineering: A Roadmap},
	Future of Software Engineering, FOSE ’07, p. 153-170,
	23–25 May 2007
	
\bibitem{dynAdaptRepl}
	Guessoum Z. et. al.
	\emph{Dynamic and Adaptive Replication for Large-Scale Reliable Multi-agent Systems},
	Lecture Notes in Computer Science pp 182-198,
	April 2003

\bibitem{algoMaxRelEndToEndConstraint}
	F. Cao , M. M. Zhu,
	\emph{Distributed workflow mapping algorithm for maximized reliability under end-to-end delay constraint},
	The Journal of Supercomputing, Vol. 66, Issue 3, p. 1462-1488,
	December 2013

\bibitem{algoMinExTime}
	A. Dogan, F. Özüner,
	\emph{Matching and Scheduling Algorithms for Minimising Execution Time and Failure Probability of Applications in Heterogeneous Computing},
	IEEE Transactions on parallel and distributed systems, Vol. 13, No.3,
	March 2002

\bibitem{relModelDistSimSystem}
	H. Wan, H. Z. Huang, J. Yang, Y. Chen,
	\emph{Reliability model of distributed simulation system},
	Quality, Reliability, Risk, Maintenance, and Safety Engineering (ICQR2MSE), 2011 International Conference,
	June 2011

\bibitem{relModelAnalysis}
	C. S. Raghavendra, S. V. Makam,
	\emph{Reliability Modeling and Analysis of Computer Networks},
	IEEE Transactions on Reliability  Vol. 35,  Issue. 2,
	June 1986

\bibitem{cloudServiceRel}
	Y. Dai, B. Yang, J. Dongarra, G. Zhang
	\emph{Cloud Service Reliability: Modeling and Analysis},
	in PRDC, 2009	

\bibitem{optTaskAllocationForMaxRel}
	H. R. Faragardi, R. Shojaee, M. A. Keshtkar, H. Tabani,
	\emph{Optimal task allocation for maximizing reliability in distributed real-time systems},
	Computer and Information Science (ICIS), 2013 IEEE/ACIS 12th International Conference,
	June 2013

\bibitem{perfImplPerCheckPoint}
	A. J. Oliner, R. K. Sahoo, J. E. Moreira, M. Gupta,
	\emph{Performance Implications of Periodic Checkpointing on Large-scale Cluster Systems},
	Parallel and Distributed Processing Symposium, 2005. Proceedings. 19th IEEE International,
	April 2005

\bibitem{studyOfFailures}
	B. Schroeder, G. Gibson,
	\emph{A large-scale study of failures in high-performance computing systems},
	IEEE Transactions on Dependable and Secure Computing (Volume:7,  Issue: 4),
	November 2010

\bibitem{implicationsOfFailures}
	Y. Zhang, M. S. Squillante, A. Sivasubramaniam, R. K. Sahoo,
	\emph{Performance Implications of Failures in Large-Scale Cluster Scheduling},
	Job Scheduling Strategies for Parallel Processing, Volume 3277 of the series Lecture Notes in Computer Science pp 233-252,
	2005

\bibitem{discContRelModel}
	L. Fiondella, L. Xing,
	\emph{Discrete and continuous reliability models for systems with identically distributed correlated components},
	Reliability Engineering \& System Safety p. 1-10,
	Jan 2015

\bibitem{effTaskReplMobGrid}
	Antonios Litke et. al.,	
	\emph{Efficient task replication and management for adaptive fault tolerance in Mobile Grid environments},
	Future Generation Computer Systems Vol 23 Issue 2 p. 163-178,
	February 2007

\bibitem{realTimeSchedAlgo}
Real-time fault-tolerant scheduling algorithm for distributed computing systems

\bibitem{distDiagnosis}
Distributed Diagnosis in Dynamic Fault Environments

\bibitem{optCheckpointInterval}
A higher order estimate of the optimum checkpoint interval for restart dumps

\bibitem{factorsAffectingRel}
An analysis of factors affecting software reliability

\bibitem{SLASched}
SLA-aware Resource Scheduling for Cloud Storage

\bibitem{algoOptTimeMaxRel}
An Algorithm for Optimized Time, Cost, and Reliability in a Distributed Computing System

\bibitem{imprRelAdaptRL}
Improving reliability in resource management through adaptive reinforcement learning for distributed systems

\bibitem{selfAdaptRel}
Self-Adapting Reliability in Distributed Software Systems

\bibitem{schedReplicas}
Scheduling Fault-Tolerant Distributed Hard Real-Time Tasks Independently of the Replication Strategies

\bibitem{calvin}
	Per Persson, Ola Angelsmark,
	\emph{Calvin - Merging Cloud and IoT},
	Procedia Computer Science, Volume 52, p. 210–217,
	2015

\bibitem{taskAllocationSwarm}
Task allocation for maximizing reliability of a distributed system using hybrid particle swarm optimization

\bibitem{optResourceAllMaxPerformance}
Optimal Resource Allocation for Maximizing Performance and Reliability in Tree-Structured Grid Services 

\bibitem{matchSchedAlgoMinFailure}
Matching and Scheduling Algorithms for Minimizing Execution Time and Failure Probability of Applications in Heterogeneous Computing

\bibitem{safetyRelTaskAllocation}
Safety and Reliability Driven Task Allocation in Distributed Systems

\bibitem{improvedTaskAllMaxRel}
Improved Task-Allocation Algorithms to Maximize Reliability of Redundant Distributed Computing Systems 

\bibitem{designFaultTolerantSched}
Design of a Fault-Tolerant Scheduling System for Grid Computing

\bibitem{evalReplicationSched}
Evaluation of replication and rescheduling heuristics for grid systems with varying resource availability

\bibitem{faultTolerantSchedPolicy}
Fault-Tolerant Scheduling Policy for Grid Computing Systems

\bibitem{adaptiveCheckPointAndRep}
Adaptive Task Checkpointing and Replication: Toward Efficient Fault-Tolerant Grids

\bibitem{decisionModelTaskAllocation}
The decision model of task allocation for constrained stochastic distributed systems

\bibitem{perfRelNonMarkovian}
Performance and Reliability of Non-Markovian Heterogeneous Distributed Computing Systems

\bibitem{hierarchicalRelModeling}
A Hierarchical Modeling and Analysis for Grid Service Reliability

\bibitem{relAnalysisFRA}
Reliability analysis of distributed systems based on a fast reliability algorithm

\bibitem{relGridServicePredConstraint}
Reliability and Performance of Star Topology Grid Service with Precedence Constraints on Subtask Execution

\bibitem{relModelWebServices}
A Software Reliability Model for Web Services

\bibitem{studyServiceRel}
A study of service reliability and availability for distributed systems

\bibitem{efficientRelAnalysisAlgo}
Efficient algorithms for reliability analysis of distributed computing systems

\bibitem{evalOfGridRel}
Evaluating the reliability of computational grids from the end user’s point of view

\bibitem{generalAlgoRelEval}
A Generalized Algorithm for Evaluating Distributed-Program Reliability

\bibitem{realTimeRelAnalysis}
Real-Time Distributed Program Reliability Analysis

\bibitem{collaborativeReliability}
Collaborative Reliability Prediction of Service-Oriented Systems

\bibitem{faultToleranceGrid}
Fault Tolerance Techniques in Grid Computing Systems

\bibitem{faultToleranceChallenges}
Fault Tolerance Challenges, Techniques and Implementation in Cloud Computing

\bibitem{improvingPerformanceReplication}
Improving Performance via Computational Replication on a Large-Scale Computational Grid

\bibitem{adaptiveMASReplication}
Adaptive Replication in Fault-Tolerant Multi-Agent Systems

\bibitem{replicatingAgents}
Improving Fault-Tolerance by Replicating Agents

\bibitem{adaptiveAgentReplication}
Towards Reliable Multi-Agent Systems: An Adaptive Replication Mechanism

\bibitem{replicationManagement}
Fault Tolerant Algorithm for Replication Management in Distributed Cloud System

\bibitem{experimentalFailureAssessment}
Experimental Assessment of Workstation Failures and Their Impact on Checkpointing Systems

\bibitem{schedulingSurvey}
A Survey on Scheduling and the Attributes of Task Scheduling in the Cloud

\bibitem{optSchedCloud}
Scheduling Optimization in Cloud Computing

\bibitem{vm_vs_container}
	David Strauss,
	\emph{Containers—Not Virtual Machines—Are the Future Cloud},
	http://www.linuxjournal.com/content/containers\%E2\%80\%94not-virtual-machines\%E2\%80\%94are-future-cloud,
	Jun 2013
	
\bibitem{probabilistic_recovery}
	\emph{Probabilistic Model-Driven Recovery in Distributed Systems}
	
\bibitem{kademlia}
	Maymounkov, P., Mazières, D.
	\emph{Kademlia: A Peer-to-peer Information System Based on the XOR Metric}, 2012
	


\end{thebibliography}

\begin{appendices}
\chapter{Figures} \label[app]{appendix:figures}
\Cref{fig:before_node_failure} and \cref{fig:after_node_failure} shows the computational environment of four nodes and one actor between a producer and a consumer before and after a node failure. The required reliability is 0.995 and for a certain replication time and failure rate the nodes reliability is given under their name. \cref{fig:replication_flow} shows the communication flow after a node failure.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/before_node_failure.pdf}
\caption{Before a node failure. Here two replicas (placed on Node A and C) is required to achieve required reliability.} \label{fig:before_node_failure}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/after_node_failure.pdf}
\caption{After Node C has failed. Now three replicacs (placed on Node A, B and D) is reuiqred to achieve reuiqred reliability.} \label{fig:after_node_failure}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/replication_flow.pdf}
\caption{The most important parts of the communication after a node has failed.} \label{fig:replication_flow}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/results/replication_time_less_than_40.pdf} 
\caption{The average replication time as a function of state size.}\label{fig:replication_time_less_than_40}
\end{figure}


\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/sinus_failure_times.pdf}
\caption{Time between failures as of~\cref{eq:eval_sleep_time} used in test ~\ref{sec:eval_adaptive_rel_model}.} \label{fig:eval_sleep_time}
\end{figure}

\chapter{Code} \label{appendix:code}

\end{appendices}









\iffalse

% Below is the report template found online

\chapter[Short on Formatting]{Formatting}
Avoid empty spaces between \textit{chapter}-\textit{section}, \textit{section}-\textit{sub-section}. For instance, a very brief summary of the chapter would be one way of bridging the chapter heading and the first section of that chapter.
\section{Page Size and Margins}
Use A4 paper, with the text margins given in Table \ref{tab:margins}.
\begin{table}[!hbt]
\centering
\caption{Text margins for A4.}\label{tab:margins}
\begin{tabular}{cc}
\hline
\textbf{margin} & \textbf{space} \\
\hline 
top &  3.0cm\\ 

bottom & 3.0cm \\ 
 
left (inside) & 2.5cm \\ 

right (outside) & 2.5cm \\ 

binding offset & 1.0cm \\ 
\hline 
\end{tabular} 
\end{table}

\section{Typeface and Font Sizes}
The fonts to use for the reports are \textbf{TeX Gyre Termes} (a \textbf{Times New Roman} clone) for serif fonts, \textsf{\textbf{TeX Gyre Heros}} (a \textsf{\textbf{Helvetica}} clone) for sans-serif fonts, and finally \texttt{\textbf{TeX Gyre Cursor}} (a \texttt{\textbf{Courier}} clone) as mono-space font. All these fonts are included with the TeXLive 2013 installation. Table \ref{tab:fonts} lists the most important text elements and the associated fonts.
\begin{table}[!hbt]
\caption{Font types, faces and sizes to be used.}\label{tab:fonts}

 \begin{tabular}{ l c c c}
\hline 
\textbf{Element} & \textbf{Face} & \textbf{Size}  & \textbf{\LaTeX size}  \\ 
\hline 
{\huge \textbf{Ch. label}} & {\huge \textbf{serif, bold}} & \thefontsize\huge & \verb+\huge+ \\ 
{\Huge \textbf{Chapter}} & {\Huge \textbf{serif, bold}} & \thefontsize\Huge & \verb+\Huge+ \\ 
{\LARGE \textsf{\textbf{Section}}} & {\Large \textsf{\textbf{sans-serif, bold}}} & \thefontsize\LARGE &  \verb+\LARGE+  \\ 
{\Large \textsf{\textbf{Subsection}}} & {\Large \textsf{\textbf{sans-serif, bold}}} & \thefontsize\Large & \verb+\Large+ \\ 
{\large \textsf{\textbf{Subsubsection}}} & {\Large \textsf{\textbf{sans-serif, bold}}} & \thefontsize\large &  \verb+\large+ \\ 
Body & serif & \thefontsize\normalsize & {\footnotesize \verb+\normalsize+} \\
%{\footnotesize Footnote} & serif  & \thefontsize\footnotesize & {\footnotesize \verb+\footnotesize+} \\
{\footnotesize \textsc{Header}} & {\footnotesize \textsc{serif, SmallCaps}} & \thefontsize\footnotesize &  \\
Footer (page numbers) & serif, regular & \thefontsize\normalsize &  \\
\hline
\textbf{Figure label} & \textbf{serif, bold} & \thefontsize\normalsize & \\
Figure caption & serif, regular & \thefontsize\normalsize & \\
\textsf{In figure} & \textsf{sans-serif} & \textit{any} & \\
\textbf{Table label} & \textbf{serif, bold} & \thefontsize\normalsize & \\
Table caption and text & serif, regular & \thefontsize\normalsize & \\
\texttt{Listings} & \texttt{mono-space} & $\le$ \thefontsize\normalsize & \\
\hline 
\end{tabular} 
\end{table}

\subsection{Headers and Footers}
Note that the page headers are aligned towards the outside of the page (right on the right-hand page, left on the left-hand page) and they contain the section title on the right and the chapter title on the left respectively, in \textsc{SmallCaps}. The footers contain only page numbers on the exterior of the page, aligned right or left depending on the page. The lines used to delimit the headers and footers from the rest of the page are $0.4 pt$ thick, and are as long as the text.

\subsection{Chapters, Sections, Paragraphs}
Chapter, section, subsection, etc. names are all left aligned, and numbered as in this document. 

Chapters always start on the right-hand page, with the label and title separated from the rest of the text by a $0.4 pt$ thick line.

Paragraphs are justified (left and right), using single line spacing. Note that the first paragraph of a chapter, section, etc. is not indented, while the following are indented.

\subsection{Tables}
Table captions should be located above the table, justified, and spaced 2.0cm from left and right (important for very long captions). Tables should be numbered, but the numbering is up to you, and could be, for instance:
\begin{itemize}
\item \textbf{Table X.Y} where X is the chapter number and Y is the table number within that chapter. (This is the default in \LaTeX. More on {\LaTeX} can be found on-line, including whole books, such as \cite{goossens93}.) or
\item \textbf{Table Y} where Y is the table number within the whole report
\end{itemize}
As a recommendation, use regular paragraph text in the tables, bold headings and avoid vertical lines (see Table \ref{tab:fonts}). 

\subsection{Figures}
Figure labels, numbering, and captions should be formed similarly to tables. As a recommendation, use vector graphics in figures (Figure \ref{fig:vectorg}), rather than bitmaps (Figure \ref{fig:rasterg}). Text within figures usually looks better with sans-serif fonts.
\begin{figure}[!hbt]
\centering
\includegraphics[scale=2.5]{images/examplepic1.pdf} 
\caption{A PDF vector graphics figure. Notice the numbering and placement of the caption. The caption text is indented 2.0cm from both left and right text margin.}\label{fig:vectorg}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=2.5]{images/examplepic3.jpg} 
\caption{A JPEG bitmap figure. Notice the bad quality of such an image when scaling it. Sometimes bitmap images are unavoidable, such as for screen dumps.}\label{fig:rasterg}
\end{figure}
For those interested in delving deeper into the design of graphical information display, please refer to books such as \cite{Tufte:1986, few2012show}.

\section{Mathematical Formulae and Equations}
You are free to use in-text equations and formulae, usually in \textit{italic serif} font. For instance: $S = \sum_i a_i$. We recommend using numbered equations when you do need to refer to the specific equations:
\begin{equation}
E = \int_0^{\delta} P(t) dt \quad \longleftrightarrow \quad E = m c^2
\end{equation}
The numbering system for equations should be similar to that used for tables and figures.

\section{References}
Your references should be gathered in a \textbf{References} section, located at the end of the document (before \textbf{Appendices}). We recommend using number style references, ordered as appearing in the document or alphabetically. Have a look at the references in this template in order to figure out the style, fonts and fields. Web references are acceptable (with restraint) as long as you specify the date you accessed the given link \cite{fontspec, CTAN}. You may of course use URLs directly in the document, using mono-space font, i.e. \url{http://cs.lth.se/}.

\section{Colours}
As a general rule, all theses are printed in black-and-white, with the exception of selected parts in selected theses that need to display colour images essential to describing the thesis outcome (\textit{computer graphics}, for instance).

A strong requirement is for using \textbf{black text on white background} in your document's main text. Otherwise we do encourage using colours in your figures, or other elements (i.e. the colour marking internal and external references) that would make the document more readable on screen. You may also emphasize table rows, columns, cells, or headers using white text on black background, or black text on light grey background.

Finally, note that the document should look good in black-and-white print. Colours are often rendered using monochrome textures in print, which makes them look different from on screen versions. This means that you should choose your colours wisely, and even opt for black-and-white textures when the distinction between colours is hard to make in print. The best way to check how your document looks, is to print out a copy yourself.

\chapter{Language}

You are strongly encouraged to write your report in English, for two reasons. First, it will improve your use of English language. Second, it will increase visibility for you, the author, as well as for the Department of Computer Science, and for your host company (if any).

However, note that your examiner (and supervisors) are not there to provide you with extensive language feedback. We recommend that you check the language used in your report in several ways:
\begin{description}
\item[Reference books] dedicated to language issues can be very useful. \cite{heffernan2000writing} 
\item[Spelling and grammar checkers] which are usually available in the commonly used text editing environments.
\item[Colleagues and friends] willing to provide feedback your writing.
\item[Studieverkstaden] is a university level workshop, that can help you with language related problems (see \href{http://www.lu.se/studera/livet-som-student/service-och-stod/studieverkstaden}{Studieverkstaden}'s web page).
\item[Websites] useful for detecting language errors or strange expressions, such as
\begin{itemize}
\item \url{http://translate.google.com}
\item \url{http://www.gingersoftware.com/grammarcheck/}
\end{itemize}
\end{description}

\section{Style Elements}
Next, we will just give some rough guidelines for good style in a report written in English. Your supervisor and examiner as well as the aforementioned \textbf{Studieverkstad} might have a different  take on these, so we recommend you follow their advice whenever in doubt. If you want a reference to a short style guide, have a look at \cite{shortstyleguide}.

\subsubsection{Widows and Orphans}

Avoid \textit{widows} and \textit{orphans}, namely words or short lines at the beginning or end of a paragraph, which are left dangling at the top or bottom of a column, separated from the rest of the paragraph.

\subsubsection{Footnotes}

We strongly recommend you avoid footnotes. To quote from \cite{OGSW}, \textit{Footnotes are frequently misused by containing information which should either be placed in the text or excluded altogether. They should be avoided as a general rule and are acceptable only in exceptional cases when incorporation of their content in the text  [is] not possible.} 

\subsubsection{Active vs. Passive Voice}

Generally active voice (\textit{I ate this apple.}) is easier to understand than passive voice (\textit{This apple has been eaten (by me).}) In passive voice sentences the actor carrying out the action is often forgotten, which makes the reader wonder who actually performed the action. In a report is important to be clear about who carried out the work. Therefore we recommend to use active voice, and preferably the plural form \textit{we} instead of \textit{I} (even in single author reports).

\subsubsection{Long and Short Sentences}
A nice brief list of sentence problems and solutions is given in \cite{yalesentences}. Using choppy sentences (too short) is a common problem of many students. The opposite, using too long sentences, occurs less often, in our experience.

\subsubsection{Subject-Predicate Agreement}
A common problem of native Swedish speakers is getting the subject-predicate (verb) agreement right in sentences. Note that a verb must agree in person and number with its subject. As a rough tip, if you have subject ending in \textit{s} (plural), the predicate should not, and the other way around. Hence, \textit{only one s}. Examples follow:
\begin{description}
\item[incorrect] He have to take this road.
\item[correct] He has to take this road.
\end{description}
\begin{description}
\item[incorrect] These words forms a sentence.
\item[correct] These words form a sentence.
\end{description}
\noindent In more complex sentences, getting the agreement right is trickier. A brief guide is given in  the \textit{20 Rules of Subject Verb Agreement} \cite{subjectverb}.

\chapter{Structure}
It is a good idea to discuss the structure of the report with your supervisor rather early in your writing. Given next is a generic structure that is a starting point, but by no means the absolute standard. Your supervisor should provide a better structure for the specific field you are writing your thesis in. Note also that the naming of the chapters is not compulsory, but may be a helpful guideline.
\begin{description}
\item[Introduction] should give the background of your work. Important parts to cover:
\begin{itemize}
\item Give the context of your work, have a short introduction to the area.
\item Define the problem you are solving (or trying to solve).
\item Specify your contributions. What does this particular work/report bring to the research are or to the body of knowledge? How is the work divided between the co-authors? (This part is essential to pinpoint individual work. For theses with two authors, it is compulsory to identify which author has contributed with which part, both with respect to the work and the report.)
\item Describe related work (literature study). Besides listing other work in the area, mention how is it related or relevant to your work. The tradition in some research area is to place this part at the end of the report (check with your supervisor).
\end{itemize}
\item[Approach] should contain a description of your solution(s), with all the theoretical background needed. On occasion this is replaced by a subset or all of the following:
\begin{itemize}
\item \textbf{Method}: describe how you go about solving the problem you defined. Also how do you show/prove that your solution actually works, and how well does it work.
\item \textbf{Theory}: should contain the theoretical background needed to understand your work, if necessary.
\item \textbf{Implementation}: if your work involved building an artefact/implementation, give the details here. Note, that this should not, as a rule, be a chronological description of your efforts, but a view of the result. There is a place for insights and lamentation later on in the report, in the Discussion section.
\end{itemize}
\item[Evaluation] is the part where you present the finds. Depending on the area this part contains a subset or all of the following: 
\begin{itemize}
\item \textbf{Experimental Setup} should describe the details of the method used to evaluate your solution(s)/approach. Sometimes this is already addressed in the \textbf{Method}, sometimes this part replaces \textbf{Method}.
\item \textbf{Results} contains the data (as tables, graphs) obtained via experiments  (benchmarking, polls, interviews).
\item \textbf{Discussion} allows for a longer discussion and interpretation of the results from the evaluation, including extrapolations and/or expected impact. This might also be a good place to describe your positive and negative experiences related to the work you carried out.
\end{itemize} 
Occasionally these sections are intermingled, if this allows for a better presentation of your work. However, try to distinguish between measurements or hard data (results) and extrapolations, interpretations, or speculations (discussion).
\item[Conclusions] should summarize your findings and possible improvements or recommendations.
\item[Bibliography] is a must in a scientific report. {\LaTeX} and \texttt{bibtex} offer great support for  handling references and automatically generating bibliographies.
\item[Appendices] should contain lengthy details of the experimental setup, mathematical proofs, code download information, and shorter code snippets. Avoid longer code listings. Source code should rather be made available for download on a website or on-line repository of your choosing.

\end{description}
\makebibliography{MyMSc}

\begin{appendices}
\chapter{About This Document}
The following environments and tools were used to create this document:
\begin{itemize}
\item operating system: Mac OS X 10.10.1
\item tex distribution: MacTeX-2014, \url{http://www.tug.org/mactex/}
\item tex editor: Texmaker 4.4.1 for Mac, \url{http://www.xm1math.net/texmaker/} for its XeLaTeX flow (recommended) or pdfLaTeX flow
\item bibtex editor: BibDesk 1.6.3 for Mac, \url{http://bibdesk.sourceforge.net/}
\item fonts \texttt{cslthse-msc.cls} document class): 
\begin{description}
\item{for XeLaTeX}: TeX Gyre Termes, \textsf{TeX Gyre Heros}, \texttt{TeX Gyre Cursor} (installed from the TeXLive 2013)
\item{for pdfLaTeX}: TeX Gyre font packages: tgtermes.sty, tgheros.sty, tgcursor.sty, gtxmath.sty (available through TeXLive 2013) 
\end{description} 
\item picture editor: OmniGraffle Professional 5.4.2
\end{itemize}

\noindent A list of the essential \LaTeX packages needed to compile this document follows (all except \texttt{hyperref} are included in the document class):
\begin{itemize}
\item \texttt{fontspec}, to access local fonts, needs the XeLaTeX flow
\item \texttt{geometry}, for page layout
\item \texttt{titling}, for formatting the title page
\item \texttt{fancyhdr}, for custom headers and footers
\item \texttt{abstract}, for customizing the abstract
\item \texttt{titlesec}, for custom chapters, sections, etc.
\item \texttt{caption}, for custom tables and figure captions
\item \texttt{hyperref}, for producing PDF with hyperlinks
\item \texttt{appendix}, for appendices
\item \texttt{printlen}, for printing text sizes
\item \texttt{textcomp}, for text companion fonts (e.g. bullet)
\end{itemize}

\noindent Other useful packages:
\begin{itemize}
\item \texttt{listings}, for producing code listings with syntax colouring and line numbers
\end{itemize}

\chapter{List of Changes}
\subsubsection{Since 2015/04/27}
\begin{itemize}
\item Improved the \textbf{Structure} chapter and added more detailed comments for each part.
\end{itemize}

\subsubsection{Since 2014/02/18}
\begin{itemize}
\item Added the possibility to specify two supervisors. Use either of the \verb+\supervisor{}+ or \verb+\supervisors{}{}+ commands to set the names and contacts on the first page.
\end{itemize}

\subsubsection{Since 2013/09/23}
\begin{itemize}
\item Added missing colon ":" after \textit{Examiner} on the front page. 
\end{itemize}

\subsubsection{Since 2013/08/30}
\begin{itemize}
\item Changed fonts from Garamond (Times New Roman), Helvetica (Arial), Courier (Source Code Pro) to Tex Gyre fonts, namely Termes, Heros, Cursor, which are freely available with TexLive 2013 installation. These are all clones of Times New Roman, Helvetica and Courier, respectively. Garamond is problematic on some systems, being a non-freely available font.
\item Corrected the \textit{Face} column in Table \ref{tab:fonts} to correctly depict the font face.
\end{itemize}

\subsubsection{Since 2013/02/22}
\begin{itemize}
\item Number of words required in the abstract changed to 150 (from 300).
\end{itemize}

\subsubsection{Since 2013/02/15}
\begin{itemize}
\item Made a separate document class, for clarity.
\item made it work with pdfLaTeX and garamond.sty, in addition to XeLaTeX and true type fonts. It is up to the user to get the hold of the garamond.zip from \url{http://gael-varoquaux.info/computers/garamond/index.html}.
\end{itemize}
\end{appendices}

\fi

\end{document}
