\documentclass{cslthse-msc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage[titletoc, header, page]{appendix}

\usepackage{hyperref}

\newtheorem{definition}{Definition}[chapter]

%\geometry{showframe}

\author{
	Philip Ståhl \\
	{\normalsize \href{mailto:ada10pst@student.lu.se}{\texttt{ada10pst@student.lu.se}}}
	\and
	Jonatan Broberg \\
    {\normalsize \href{mailto:elt11jbr@student.lu.se
}{\texttt{elt11jbr@student.lu.se}}}
}

\title{Dynamic Fault-Tolerance and Task Scheduling in Distributed Systems}
\subtitle{}
\company{Mobile and Pervasive Computing Institute (MAPCI), Lund University}
\supervisors{Björn Landfeldt, \href{mailto:bjorn.landfeldt@eit.lth.se}{\texttt{bjorn.landfeldt@eit.lth.se}}}{}
\examiner{Christian Nyberg, \href{mailto:christian.nyberg@eit.lth.se}{\texttt{christian.nyberg@eit.lth.se}}}

\date{\today}
%\date{January 16, 2015}

\acknowledgements{
We would like to thank our supervisor Björn Landfeldt and MAPCI for there input.

Maybe also:
Shubhabatra, Ericsson, Jörn, Duan, examiner Christian Nyberg.

%If you want to thank people, do it here, on a separate right-hand page. Both the U.S. \textit{acknowledgments} and the British \textit{acknowledgements} spellings are acceptable.
}

\theabstract{
This document describes the Master's Thesis format for the theses carried out at the Department of Computer Science, Lund University. 

%Your abstract should capture, in English, the whole thesis with focus on the problem and solution in 150 words. It should be placed on a separate right-hand page, with an additional \textit{1cm} margin on both left and right. Avoid acronyms, footnotes, and references in the abstract if possible.

%Leave a \textit{2cm} vertical space after the abstract and provide a few keywords relevant for your report. Use five to six words, of which at most two should be from the title.

}

\keywords{reliability, distributed computing, dynamic fault-tolerance, task scheduling, Poisson}

%% Only used to display font sizes
\makeatletter
\newcommand\thefontsize[1]{{#1 \f@size pt\par}}
\makeatother
%%%%%%%%%%


\begin{document}
\makefrontmatter

\chapter{Introduction} \label{ch:introduction} 
\section{Background and Motivation} 
Ensuring a certain level of reliability is of major concern for many cloud system providers. As cloud computing is growing rapidly and users are demanding more services and higher performance, providing fault tolerant systems and improving reliability by more sophisticated task scheduling, has become a very important and interesting research area. 

Cloud systems often consists of heterogeneous hardware and any of the often vast number of components may fail at any time. Therefore, ensuring reliability raises complexity to the resource allocation decisions and fault-tolerance mechanisms in highly dynamic distributed systems. For cloud service providers, it is necessary that this increased complexity is taken care of without putting extra burden on the user. The system should therefore ensure these properties in a seamless way.

As computational work is distributed across multiple resources, the overall reliability of the application decreases. To cope with this issue, a fault-tolerant design or error handling mechanism need to be in place. This is of particular interest for cloud service providers, as users often desire a certain level of reliability. In some cases, vendors, for example carrier providers, are obliged by law to achieve a certain level of availability or reliability. In these cases, one may need to sacrifice other quality aspects such as latency and resource usage. By using static and dynamic analysis of the infrastructure and the mean-time-between-failure for the given resources, a statistical model can be used in order to verify  that the desired level is reached. However, such a model should be dynamic, as failure rates are likely to vary over time, for example due to high or low load on the system. This is of particular importance for long running applications when requiring a certain level of reliability. Despite fulfilling the required level at the time of deployment, as the state of the system changes, the level may no longer be fulfilled.

Reliability can be increased by replicating application tasks, where all replicas perform the same computations on the same input. This allows for both increased redundancy and an easy way of detecting errors. This allows for continuing execution of the application even in case of a filing replica. Seamlessly being able to continue the execution, without losing any data is of particular interest in data stream processing.

One drawbacks of replicating a task n times, is that the resources needed increases. With $n$ replicas, all performing the same computation on the same input, one need $n$ times as much resources, hence a lot of computational resources is thus wasted. Dynamic analysis of the system and an adaptive scheduling technique could help in determining on which resources one should assign the tasks to for optimal resource usage and load balancing. 

Throughout this master thesis, extensive literature studies has been made, first in order to find out what has already been done in the area and later in order to get valuable help in developing a reliability model.

\section{Related work} \label{sec:related_work}
The interest in reliability in distributed systems has gain increased knowledge \cite{replicationManagement}. Due to the uncertain heterogeneous environment of cloud and grid systems, increasing reliability is a complex task [TODO]. %TODO add reference

A lot of scheduling techniques has been designed, aiming at maximizing reliability of jobs in distributed environments, under various constraints such as meeting task deadlines or minimizing execution time \cite{algoOptTimeMaxRel} \cite{optTaskAllocationForMaxRel} \cite{taskAllocation} \cite{taskAllocationSwarm} \cite{algoMaxRelEndToEndConstraint} \cite{algoMinExTime}  \cite{schedReplicas}. Maximizing reliability for these algorithms are a secondary concern, while meeting the constraints are the primary. Other algorithms have been developed which put greater focus on increasing the reliability \cite{optResourceAllMaxPerformance} \cite{matchSchedAlgoMinFailure}, and some have increased reliability as the primary objective \cite{safetyRelTaskAllocation} \cite{improvedTaskAllMaxRel}. Common for these scheduling techniques are that while they try to maximize reliability, they do not ensure a certain level of reliability to the user. Furthermore, the algorithms are usually static in the way that they do not account for the dynamic behaviour of distributed system, and they make assumptions such as known execution times of tasks.

A lot of work has been done in the area of designing fault-tolerant systems by using checkpoint/restart techniques [TODO]. %TODO add reference
These techniques relies of the notion of a stable storage, such as a hard-drive, which is persistent even in the case of a system failure.

Some attempts at designing fault-tolerant systems by the use of replication has been made \cite{designFaultTolerantSched}  \cite{evalReplicationSched} \cite{taskSchedulingReplication} \cite{effTaskReplMobGrid} \cite{relGridServicePredConstraint}. \cite{evalReplicationSched} assumes a static number of replicas, which is used for every application being deployed. Furthermore, they do not guarantee that all replicas are deployed, instead they use a best-effort approach, where replicas are deployed when resources are available. While both \cite{effTaskReplMobGrid}, \cite{taskSchedulingReplication} and \cite{designFaultTolerantSched} all dynamically determines the number of replicas based on the state of the system, it is static in the way that failed replicas are not restarted. The reliability level for long running applications are therefore decreased if replicas fail. Furthermore, while  \cite{designFaultTolerantSched} dynamically determines the number of replicas to use, the selection of resources is done after the number of replicas has been determined. In order to ensure a certain level of reliability, these two parts need to be combined into one, because the resources selected affects the number of replicas needed in order to achieve the desired reliability.

A quite old but still relevant work is found in \cite{dynAdaptRepl} where they present a framework for dynamic replication with an adaptive control in an multi-agent system. They introduce the software architecture which can act as support for building reliable multi-agent systems. Since the available resources often are limited they say that it isn't feasible to replicate all components. Therefore they use a criticality for each agent which is allowed to evolve during runtime. The proposed solution allows for dynamically adapt the number of replicas and the replication strategy itself (passive/active). The number of replicas is partly based on the agents critically and a predefined minimum number of replicas. From CPU usage time and communication activity an agent activity is calculated which is used in calculating the agents critically. One restriction they do in their fault model is that processes can only fail by permanent crashes. %TODO Do they meet a desrided level of reliablity?

Other approaches to improve reliability in Multi-Agent Systems (MAS) by the use of replication is presented in \cite{replicatingAgents} \cite{adaptiveMASReplication} \cite{adaptiveAgentReplication}. While being adaptive to system state, the solution presented in \cite{replicatingAgents} still faces the problem of having a single point of failure due to a communication proxy. This problem is avoided in \cite{adaptiveMASReplication}, where a decentralized solution is proposed, where the number of replicas and their placement depends on the system state. %TODO need to read \cite{adaptiveMASReplication} again...
The solution proposed in \cite{adaptiveAgentReplication} involves distributed monitoring system...  %TODO need to read this again...

\cite{adaptiveCheckPointAndRep} proposes an algorithm based on replication which dynamically varies the number of replicas depending on system load. However, the algorithm reduces the number of replicas during peak hours, in order to reduce system load. Since the reliability of system decreases during higher load \cite{studyOfFailures} \cite{studyOfFailures} \cite{implicationsOfFailures}, one should increase the number of replicas to keep the desired level of reliability.

A fault-tolerant scheduling technique incorporates a replication scheme is presented in \cite{faultTolerantSchedPolicy}. While being dynamic in that failed replicas are restarted, it is static in that the user defines the number of replicas to use.

The techniques used in \cite{selfAdaptRel} \cite{dynAdaptRepl} \cite{relModelWebServices} are more dynamic and adaptive to the dynamic behaviour of distributed systems. However, reliability is defined as producing the correct result, and achieved by techniques like voting and \emph{k-modular redundancy}.
%TODO  (above sentence) However?
An adaptive approach, which adapts to changes in the execution environment is presented in \cite{imprRelAdaptRL}. In it, they present an adaptive scheduling model based on reinforcement learning, aiming at increasing the reliability. However, they assume a task's profile is available.


\section{Our contributions}
To our knowledge, no previous attempt has been made which in a fully dynamic manner ensures a certain level of reliability for long running applications. Some previous work dynamically calculates the number of replicas, but are static in that failed replicas are not restarted, while others use a static number of replicas, and dynamically restart failed one.

We propose a framework which ensures an user determined level of reliability for long running applications by the use of replication. Furthermore, the method ensures a minimized use of resource by not using more replicas than needed. This is achieved by scheduling replicas to the most reliable resources first and foremost. Furthermore, the system is continuously monitored in order to adapt the number of replicas as the state of the system changes.

The framework is not limited to a specific type of distributed environment, and its key concepts may be used in both grid and cloud systems.

%TODO
%Finally, our solution is fully distributed and thereby avoids having a single point of failure, which may be the case in grid environments with a single Resource Management System. (THIS CONTRADICTS NOT BEING LIMITED TO A SPECIFIC DISTRIBUTED ENVIRONMENT)

Our model is implemented by extending the actor-based application environment \emph{Calvin} \ref{calvin}, developed by Ericsson. While \emph{Calvin} is mainly an environment for IoT applications, it suites our purpose well. The model is evaluated by... %TODO 

The report is structured as follows: in Section \ref{ch:background_theory} all necessary background theory is provided, in Section \ref{ch:approach} we present our model and contribution in more detail, Section \ref{ch:evaluation} aims at evaluating our solution while Section \ref{ch:limitations} and \ref{ch:future_work} presents limitations in our model and future work. Section \ref{ch:conclusions} concludes the report. 

\section{Goal}
The goal of this thesis is to devise a method for dynamically ensuring a certain level of reliability for distributed applications or services. Reliability will be achieved through replication of tasks.

First, a realiability model will be designed, describing the reliability for an application running in an distributed environment.

Secondly, a framework will be designed which will automatically detect node failures and based on the reliability model spawns enough replicas to reach above the desired reliability level.

Lastly, the system will by periodically monitored in order to adapt the reliability model and the replicas needed as the properties of the system varies over time.

The model will be implemented and tested using the IoT application framework \emph{Calvin}.

\chapter{Background} \label{ch:background}
\section{Computational Environment}
\subsection{Types of distributed computing}
Distributed computing systems (DCS) are composed of a number of components or subsystems interconnected via an arbitrary communication network \cite{relModelDistSimSystem} \cite{efficientRelAnalysisAlgo}. There are a number of different types of distributed environments, e.g. grid, clusters, cloud and HDCS.

\subsubsection{Heterogeneous distributed computing systems}
Heterogeneous Distributed Computing Systems, HDCS is a system of numerous high-performance machines connected in a high-speed network. Therefore high-speed processing of heavy applications is possible \cite{algoMinExTime}. %TODO add info

The majority of distributed service systems can be viewed as CHDS, (Centralized Heterogeneous Distributed System). A CHDS consists of heterogeneous sub-systems which a managed by a centralized control center. The sub-systems have various operating platforms and are connected in diverse topological networks \cite{studyServiceRel}.

%COPIED: Most of the distributed service systems can be modeled as a CHDS. This type of distributed systems consists of heterogeneous sub-systems with various operating platforms on different computers in diverse topological networks, which are managed by a control center. The heterogeneous sub-distributed systems are composed of different types of computers with various operating systems connected by diverse topologies of networks. These sub-systems exchange data with virtual machine through system service provider interface. They are connected with virtual nodes by routers \cite{studyServiceRel}.

\subsubsection{Grid computing}
A grid is a collection of autonomous resources that are distributed geographically and across several administrative domains, and work together to achieve a common goal, i.e. to solve a single task \cite{compStudyLoadAndCloud} \cite{relAndPerfGridServices} \cite{evalOfGridRel}.

Each domain in a grid usually has a centralized scheduling service called Resource Management System (RMS) which accepts job execution requests and sends the job's tasks to the different resources for execution \cite{evalOfGridRel}.

The reliability of the cloud computing is very critical but hard to analyze due to its characteristics of massive-scale service sharing, wide-area network, heterogeneous software/hardware components and complicated interactions among them \cite{cloudServiceRel}.

%COPIED:The grid computing system has emerged as an important new field, distinguished from conventional distributed computing systems by its focus on large-scale resource sharing, innovative applications, and, in some cases, high performance orientation. \cite{hierarchicalRelModeling}.

\subsubsection{Cluster}
A cluster system is usually a number of identical units managed by a central manager. It is similar to a grid, but differ in that resources are geographically located at the same place. The resources work in parallel under supervision of a single administrative domain. From the outside it looks like a single computing resource \cite{compStudyLoadAndCloud}. 

\subsubsection{Cloud}
A cloud has been described as the next generation of grids and clusters. While it is similar to grid and clusters, for example parallel and distributed, the important difference is that cloud has multiple domains \cite{compStudyLoadAndCloud}. Machines can be geographically distributed, and software and hardware components are often heterogeneous, and therfore analyzing and predicting workload and reliability is usually very challenging \cite{surveyReliabilityDistr}. 

%COPIED: "Cloud computing is a style of computing where service is provided across the Internet using different models and layers of abstraction [4]. It refers to the applications delivered as services [5] to the mass, " \cite{faultToleranceChallenges}

%COPIED: Cloud computing is different from but related with grid computing, utility computing and transparent computing. Grid computing [1] is a form of distributed computing whereby a "super and virtual computer" composed of a cluster of networked, loosely-coupled computers acts in concert to perform very large tasks.”

\subsection{Dynamic versus static environments}
A distributed computing environment can be either static or dynamic.

In a static environment only homogeneous resources are installed \cite{compStudyLoadAndCloud}. Prior knowledge of node capacity, processing power, memory, performance and statistics of user requirements are required. Changes in load during run time is not taken into account which makes environment easy to simulate but not well suited for heterogeneous resources.

In a dynamic environment heterogeneous resources are installed \cite{compStudyLoadAndCloud}. In this scenario prior knowledge isn't enough since the requirements of the user can change during run time. Therefore run time statistics is collected and taken into account. The dynamic environment is difficult to simulate but there are algorithms which easily adopt to run time changes in load.


\section{Faults in distributed environments}
\subsection{Types of Faults}
%TODO Perhaps research more which definition is the most usual. Per Runesson, head of Computer Science at LTH uses a different definition
A fault is usually used to describe a defect at the lowest level of abstraction \cite{faultTolerantFundamentals}. A fault may cause an error which in turn may lead to a failure, which is when a system has not behaved according to its specification.

In distributed environments, especially with heterogeneous commodity hardware, several types of failures can take place, which may affect the running applications in the environment. These failure include, but are not limited to, overflow failure, timeout failure, resource missing failure, network failure, hardware failure, software failure, and database failure \cite{cloudServiceRel}. Failures are usually considered to be either \cite{evalOfGridRel}:
\begin{enumerate}
	\item Job related, or
	\item System related, or
	\item Network related.
\end{enumerate}

The possible errors in a Grid environment can be divided into the following three categorize \cite{effTaskReplMobGrid}:
\begin{itemize}
	\item Crash failure - When a correctly working server comes to a halt.
	\item Omission failure - When a server fails to respond to incoming requests and to send messages
	\item Timing failure - When a server responds correctly but beyond the specified time interval
\end{itemize}

In \cite{studyOfFailures}, almost ten years of real-world failure data of 22 high performance computing systems is studied and concluded hardware failures to be the single most common type of failure, ranging from 30 to more than 70 \% depending on hardware type, while 10 - 20 \% of the failures were software failures.

\subsection{Fault models}
When studying the reliability of distributed systems or reliability of applications running in distributed computing environments, one usually starts with specifying which fault model is used, and the model developed is then proved with respect to this fault model \cite{faultTolerantFundamentals}.

\subsubsection*{Byzantine Faults}
The Byzantine fault model allows nodes to continue interaction after failure. Correctly working nodes cannot automatically detect if a failure has occurred and even if it was known that a failure has occurred they cannot detect which nodes has failed. The systems behaviour can be inconsistent and arbitrary \cite{surveyFaultParallel}. Nodes can fail (become Byzantine) at any point of time and stop being Byzantine at any time. A Byzantine node can send no response at all or it can try to send an incorrect result. All Byzantine nodes might send the same incorrect result making it hard to identify malicious nodes \cite{selfAdaptRel}. %Kan man detektera malicious nodes? 
The Byzantine fault model is very broad since it allows failed nodes to continue interacting, therefore it is very difficult to analyse.

% COPIED: in which processors may behave in arbitrary, even malevolent, ways). System correctness was always proved with respect to a specific fault model \cite{faultTolerantFundamentals}

\subsubsection*{Fail-stop faults}
The fail-stop model, also called the crash-stop model, is in comparison to the Byzantine fault model much simpler. When a node fails it stops producing any output and stops interacting with the other nodes \cite{faultTolerantFundamentals}. This allows for the rest of the system to automatically detect when a node has failed. Due to its simplicity, it does not handle subtle failures such as memory corruption but rather failures such as a system crash or if the system hangs \cite{surveyFaultParallel}. The fail-stop model has been criticized for not representing enough real-world failures.

%COPIED: This failure model has the practical implication that avoids the use of complicated software and/or hardware replication systems for recovering tasks from failed server. Consequently, the application cannot be successfully serviced by the system if at least one task remains unprocessed at a failed server. \cite{perfRelNonMarkovian}

\subsubsection*{Fail-stutter faults}
Since the Byzantine model is very broad and complicated and the fail-stop model doesn't represent enough real-world failures, a third middle ground model has been developed. It is an extension of the fail-stop model but it also allows for performance fault, such as unexpectedly low performance of a node \cite{surveyFaultParallel}.
% fail-stutter modellen är knappt använd...

\subsubsection*{Crash-failure model}
%TODO Källa
The crash failure model is quite alike the fail-stop model with the difference that the other nodes do not automatically detect that a node has failed \cite{faultTolerantFundamentals} \cite{adaptiveAgentReplication}.

\subsection{Failure distribution}
%TODO ev. källa för att reliability för sw är mer eller mindre strikt avtagande 
%Failure rate for software on the other hand is often assumed to only drop over time since bugs are fixed and removed by time. If updates etc. are installed the failure rate might increase if new bugs was inserted, however software does not get worn out so an overall drop is expected.

%TODO definera "failures", vilka brukar inkluderas?
When modeling system reliability, failures are usually assumed to follow a Poisson process with a constant failure rate \cite{algoMaxRelEndToEndConstraint} \cite{algoMinExTime} \cite{relModelDistSimSystem} \cite{optTaskAllocationForMaxRel} \cite{perfImplPerCheckPoint} \cite{optCheckpointInterval} \cite{realTimeSchedAlgo}. For such a model to be valid, it is assumed that failures between resources are statistically independent with a constant failure rate \cite{algoMaxRelEndToEndConstraint}. 

Constant failure rates are not likely to model the actual failure scenario of a dynamic heterogeneous distributed system \cite{algoMinExTime}. The failure rate for a hardware component usually follows a bathtub shaped curve \cite{surveyReliabilityDistr}. The failure rate is usually higher in the beginning due to that the probability that a manufacture failure would affect the system is higher in the beginning of the systems lifetime. After a certain time the failure rate drops and later increases again due to that the component gets worn out.

Statistically independent failures is also not very likely to reflect the real dynamic behaviour of distributed systems \cite{surveyReliabilityDistr} \cite{cloudServiceRel}. Faults may propagate throughout the system, thereby affecting other components as well \cite{relGridSystems}. In grid environments, a sub-system may consist of resources using a common gateway to communicate with the rest of the system. In such a scenario, the resources do not use independent links \cite{optResourceAllMaxPerformance}. As failures are likely to be correlated \cite{perfImplPerCheckPoint}, the probability of failure increases with the number of components on which the job is running.

Other factors affect the likelihood of resource failures as well. Several work has concluded a relationship between failure rate and the load of the system \cite{studyOfFailures} \cite{implicationsOfFailures}. Furthermore, studies show that failures are more likely to occur during daytime than at night \cite{implicationsOfFailures} \cite{studyOfFailures}. Finally, components which have failed in the past are more likely to fail again \cite{implicationsOfFailures}.

While a Poisson process is commonly used to describe failures, \cite{studyOfFailures} showed failures are better modelled by a Weibull distribution with a shape parameter of 0.7 - 0.8. However, despite not always reflecting the true dynamic failure behaviour of a resource, a Poisson process has been experimentally shown to be reasonable useful in mathematical models \cite{experimentalFailureAssessment}.

\subsection{Failure assumptions}
% Should fit better with previous section? Here we mention some common assumptions, and in the section above we mention that some assumptions are maybe not that good...
Models describing the nature of failures in distirbuted computing environments, are usually based on certain assumptions, and are usually only valid under those assumptions. Common assumptions include \cite{relModelDistSimSystem} \cite{relModelAnalysis}  \cite{cloudServiceRel} \cite{studyServiceRel} \cite{hierarchicalRelModeling} \cite{selfAdaptRel}:
\begin{itemize}
	\item Each component in the system has only two states: operational or failed
	\item Failures of components are statistically independent
	\item The network topology is cycle free
	\item Components have a constant failure rate, i.e. the failure of a component follows a Poisson process
	\item Fully reliable network
\end{itemize}

For reliability modelling for the grid, it is common to also assume a fully reliable Resource Management System (RMS) \cite{relAndPerfGridServices} \cite{relGridServicePredConstraint}, which is an non-neglectable assumption since the RMS in this case in fact is a single point of failure.

\section{Reliability} \label{sec:theory_reliability}
Reliability in the context of software applications can have several meanings, especially for applications running in distributed systems. Often, reliability is defined as the probability that the system can run an entire task successfully \cite{taskAllocation} \cite{relModelDistSimSystem} \cite{studyServiceRel} \cite{hierarchicalRelModeling} \cite{generalAlgoRelEval} \cite{realTimeRelAnalysis} \cite{selfAdaptRel} \cite{perfRelNonMarkovian}. A similar definition of reliability, usually used for applications in distributed environments, is the probability of a software application, running in a certain environment, to perform its intended functions for a specified period of time \cite{surveyReliabilityDistr} \cite{surveyRelPrediction} \cite{relDistApplications}, and is common for application with time constraints. Finally, reliability can also be defined as the probability that a task produces the correct result \cite{surveyRelPrediction} \cite{relAndPerfGridServices} \cite{relGridServicePredConstraint} \cite{relModelWebServices} \cite{selfAdaptRel}. This definition is usually used together with the Byzantine fault model. 

\cite{surveyReliabilityDistr} defines a software application reliable if the following is achieved:
\begin{itemize}
\item Perform well in specified time t without undergoing halting state
\item Perform exactly the way it is designed
\item Resist various failures and recover in case of any failure that occurs during system execution without proceeding any incorrect result.
\item Successfully run software operation or its intended functions for a specified period of time in a specified environment.
\item Have probability that a functional unit will perform its required function for a specified interval under stated conditions.
\item Have the ability to run correctly even after scaling is done with reference to some aspects.
\end{itemize}

In this paper, we use the following definitions of reliability:
\begin{definition} \label{def:single_task_reliability}
The reliability of a process is the probability that the resource on which the process is running is functioning during the time of execution.
\end{definition}

For multi-task applications, where the tasks use more than one resource, reliability is defined as
\begin{definition} \label{def:multi_task_reliability}
The reliability of a multi-task process is the probability that the tasks being executed within a given time without experiencing any type of failure (internal or external) during the time of execution.
\end{definition}

Finally, for long running applications, where a replication scheme used, the reliability can be defined as \cite{effTaskReplMobGrid}
\begin{definition} \label{def:task_replica_reliability}
The reliability of a process, with $n$ task replicas, is the probability at least one replica is always running. This can be expressed as the probability that not all replicas fail during the time from that an actor dies until a new replica is up and running.
\end{definition}

\subsection{Modelling reliability}
In order to determine the reliability of a system, one need to take all factors affecting the reliability into account \cite{surveyReliabilityDistr}. However, including all factors if unfeasible. \cite{factorsAffectingRel} lists 32 factors affecting the reliability of software, excluding environmental factors such as hardware and link failure. 

For distributed applications, the probability of failure increases since it is dependent on more components \cite{relModelDistSimSystem}. Most models used to model the reliability of a system is based on the mean time to failure, or the mean time between failures, of components \cite{relModelAnalysis}. Conventionally, Mean-Time-To-Failure (MTTF) refers to non-repairable resources, while Mean-Time-Between-Failures (MTBF) refers to repairable objects \cite{effTaskReplMobGrid}.

\begin{definition} \label{def:mttf}
The Mean-Time-To-Failure for a component is the average time it takes for a component to fail, given that it was operational at time zero.
\end{definition}

\begin{definition} \label{def:mtbf}
The Mean-Time-Between-Failure for a component is the average time between successive failures for that component.
\end{definition}

The MTBF can be calculated as

\begin{equation} \label{eq:MTBF}
MTBF = (total\ time) / (number\ of\ failures)
\end{equation}

From equation~\ref{eq:MTBF}, a reliability function for a component can be expressed as 
\begin{equation} \label{eq:resource_reliability}
R(t) = e^{-t/MTBF}
\end{equation}

Equation~\ref{eq:resource_reliability} expresses the probability that a given resource will work for a time $t$. Correspondingly, the probability that a resource will fail during a time $t$ is
\begin{equation} \label{eq:resource_failure_prob}
F(t) = 1- e^{-t/MTBF}
\end{equation}

\iffalse
While redundancy and diversity have been employed as popular methods to attain better reliability [4][5][6][7][8][9][10], they impose extra hardware or software costs.
optimal task allocation. This method does not require additional hardware or software and improves system reliability just by using proper allocation of the tasks among the nodes [8][11][12] [13] [14] \cite{optTaskAllocationForMaxRel}
\\\\
\cite{discContRelModel}
\\\\
System reliability: Due to independence of node and path failures, system reliability can be formulated as: Rs(X) = RS’(X) * Rs” (X) \cite{optTaskAllocationForMaxRel}
\\\\
the reliability model of the subdistributed systems inherits the traditional models’ characters [19], [20], [21], [22] and has certain limitations. Those traditional models have a common assumption that the operational probabilities of the nodes and links are constant. However, this assumption is unrealistic for the grid, so this assumption was relaxed in [15] by assuming that the failures of nodes and links followed their respective Poisson processes so that their operational probabilities decrease with their working time instead of the constant values 
There are also many other reliability models for software, hardware, or small-scale distributed systems, see, e.g., [14], [23], [24], [25], [26], which cannot be directly implemented for studying grid service reliability. \cite{hierarchicalRelModeling}
\\\\
Reliability evaluation of the hypercubes has been addressed recently [7]-[9]. Najjar and Gaudiot have modeled hypercube reliability assuming that the system works as long as there is no disconnectedworking node(s) [7]. This implies that even if a task requirement is satisfied, the system is considered failed whenever there is a disconnected node(s). This reliability is not based on task requirement, but on system managemen.  [ A unified task-based dependability model for hypercube computers]
\\\\
(the processing element that runs the program under consideration) to some other nodes such that its vertices hold all the needed files for the program under consideration. \cite{relAnalysisFRA}
\\\\
We focus on the study of task allocation decision to achieve maximal distributed system reliability (DSR) under processor resource constraints and total system cost constraint. the task allocation problem has been studied to achieve various goals, such as reliability maximization, safety maximization, fault tolerance increasing, and cost minimization. Their processor reliability was computed from the processor failure rate and the elapsed execution time. As far as we know, the effects of module software reliabilities and module execution frequencies on the optimal task allocation decision have not been studied before. To pursue this topic, we divide the processor reliability into two parts in our system model: the processor hardware reliability and the module software reliability \cite{decisionModelTaskAllocation}
\fi

\subsection{Factors affecting reliability}
Reliability of a system highly depends on how the system is used \cite{surveyRelPrediction}. When modelling a system's reliability, one must take all factors into account in order to create a proper model. However, with the vast number of factors affecting a system's reliability, it is practically unfeasible. \cite{factorsAffectingRel} lists 32 factors affecting the reliability of software, excluding environmental factors such as hardware and link failure. Other environmental conditions affecting reliability include the amount of data being transmitted, available bandwidth and operation time \cite{cloudServiceRel} \cite{hierarchicalRelModeling}.

\section{Fault tolerance techniques}
Fault tolerance techniques are used to predict failures and take an appropriate action before failures actually occur
\cite{faultToleranceChallenges}. Considering the whole life-span of a software application, fault-tolerant techniques can be divided into four different categories \cite{surveyReliabilityDistr}:
\begin{enumerate}
\item Fault prevention - elimination of errors before they happen, e.g. during development phase
\item Fault removal - elimination of bugs or faults after repeated testing phases
\item Fault tolerance - provide service complying with the specification in spite of faults
\item Fault forecasting - Predicting or estimating faults at architectural level during design phase or before actual deployment
\end{enumerate}

\iffalse
\subsection{Fault life-cycle techniques}
According to \cite{softRelRoadmap} the following four technical areas can be regarded as fault life-cycle techniques.
\begin{itemize}
\item Fault prevention - The initial mechanism for a reliable system, trying to avoid faults already in the development phase. A fault which isn't committed cost nothing to fix. 
\item Fault removal - The next line of defence, trying to detect and eliminate faults by validation and validation. For instance by testing and inspection. 
\item Fault tolerance - The last line of defence against faults which are undetected by fault removal. Can be obtained by catching faults as exceptions and/or by redundancy.
\item Fault forecasting - Estimate the presence of faults during design phase or before actual deployment. 
\end{itemize}
\fi

Limited to already developed application, fault tolerance techniques can be divided into reactive and proactive techniques \cite{faultToleranceChallenges}. A reactive fault tolerant technique reacts when a failure occur and tries to reduce the effect of the failure. They therefore consists of detecting fault and failures, and recovering from them to allow computations to continue \cite{relGridSystems}. The proactive technique on the other hand tries to predict failures and proactively replace the erroneous components. 

Common fault tolerance techniques include \emph{checkpointing}, \emph{rollback recovery} and \emph{replication} \cite{relGridSystems}.

% Checkpointing, job migration, voting etc
\subsection{Checkpoint/Restart}
Fault-tolerance by the use of periodic checkpointing and rollback recovery is the most basic form of fault-tolerance \cite{surveyFaultParallel}.

Checkpointing is a fault-tolerance technique which periodically saves the state of a computation to a persistent storage \cite{relGridSystems} \cite{surveyFaultParallel}. In the case of failure, a new process can be restarted from the last saved state, thereby reducing the amount of computations needed to be redone.

%TODO
%The basics, however, are checkpoint recovery and task replication. The former is a common method for ensuring the progress of a long-running application by taking a checkpoint, i.e. saving its state on permanent storage periodically. A checkpoint recovery is an insurance policy against failures. In the event of a failure, the application can be rolled back and restarted from its last checkpoint—thereby bounding the amount of lost work to be re-computed \cite{effTaskReplMobGrid}

\subsection{Rollback recovery}
Rollback recovery is a technique in which all actions taken during execution are written to a log. At the event of failure, the process is restarted, and the log is read and the actions replayed, which will reconstruct the previous state \cite{surveyFaultParallel}. In contrast to checkpointing, rollback recovery returns the state to the most recent state, not only the last saved one. Rollback recovery can however be used in combination with checkpointing in order to decrease recovery time, not needing to replay all actions, but only those from the latest checkpoint.

\subsection{Replication}
Job replication is a commonly used fault tolerant technique, and is based on the assumptions that the probability of a single resource failing is greater than the probability of multiple resources failing simultaneously \cite{faultToleranceGrid}.

Using replication, several identification processes are scheduled on different resources and simultaneously perform the same computations \cite{relGridSystems}. With the increased redundancy, the probability of at least one replica finishing increases at the cost of more resources being used. Furthermore, the use of replication effectively protects against having a single point of failure \cite{faultToleranceGrid}.

Replication also minimizes the risk of failures affecting the execution time of jobs, since it avoids recomputation typically necessary when using checkpoint/restart techniques \cite{designFaultTolerantSched}.

There are a number of different strategies for replicating a task:
\begin{itemize}
\item Active
\item Semi-active
\item Passive
\end{itemize}

In active replication, one or several replicas are run on a other machine and receive an exact copy of the primary nodes input. From the input they perform the same calculations as if they were the primary node. The primary node is monitored for incorrect behaviour and in event that the primary node fails or behaves in an unexpected way, one of the replicas will promote itself as the primary node \cite{surveyFaultParallel}. This type of replication is feasible only if by assumption that the two nodes receives exactly the same input. Since the replica already is in an identical state as the primary node the transition will take negligible amount of time. A drawback with active replication is that all calculations are ran twice, thus a waste of computational capacity. 

Active replication can also be used in consensus algorithms such as majority voting or k-modular redundancy, where one need to determine the correct output \cite{surveyFaultParallel}. In this case, there is no primary and backup replicas, instead every replica acts as a primary.

Semi-Active replication is very similar to active replication but with the difference that decisions common for all replicas are taken by one site.

Passive replication is the case when a second machine, typically in idle or power off state has a copy of all necessary system software as the primary machine. If the primary machine fails the "spare" machine takes over, which might incur some interrupt of service. This type of replication is only suitable for components that has a minimal internal state, unless additional checkpointing is employed.

A replica, whether active, semi-active or passive replication is used, is defined as \cite{effTaskReplMobGrid}:
\begin{definition} \label{def:replica}
The term replica or task replica is used to denote an identical copy of the original task
\end{definition}

While replication increases the system load, is may help improving performance by reducing task completion time \cite{improvingPerformanceReplication}.

\subsubsection*{Consensus}
Replication is usually used to implement some form of consensus algorithm. Consensus problem can be viewed as a form of agreement. A consensus algorithm lets several replicas execute in parallel, independent of each other and afterwards they vote for which result is correct. These algorithms are usually use the Byzantine fault model, where a resource can crash or produce an incorrect result.

Based on achieving consensus two different redundancy (replication) strategies can be identified, traditional and progressive consensus \cite{selfAdaptRel}. In traditional redundancy an odd number of replicas are executes simultaneously and afterwards they vote for which result is correct. The result with the highest number of votes are considered correct and consensus is reached. %TODO is consensus guaranteed?

In progressive redundancy on the other hand the number of replicas needed is minimized. Assume that with traditional redundancy $k \in {3,5,7...}$ replicas is executed, progressive redundancy only executes $(k+1)/2$ replicas and reaches consensus if all replicas return the same result. If some replica return a deviant result an additional number of replicas is executed until enough replicas has return the same result, i.e. consensus is reached. 

Furthermore \cite{selfAdaptRel} present a third strategy, an iterative redundancy alternative which focus is more on reaching a required level of reliability in comparison to reaching a certain level of consensus.


\section{Load balancing}
The term load balancing is generally used for the process of transferring load from overloaded nodes to under loaded nodes and thus improving the overall performance. Load balancing techniques for a distributed environment must take two tasks into account, one part is the resource allocation and the other is the task scheduling. 
%Efficient load balancing will ensure that resources are easily available on demand and efficiently utilized under condition of high/low load, that energy is saved at low load and that the cost of resources is reduced \cite{compStudyLoadAndCloud}.

The load balancing algorithms can be divided into three categorize based on the initiation of the process:
\begin{itemize}
\item Sender Initiated - An overloaded node send requests until it find a proper node which can except its load.
\item Receiver Initiated - An under loaded node sends a message for requests until it finds an overloaded node.
\item Symmetric - A combination of sender initiated and receiver initiated. 
\end{itemize}

Load balancing is often divided into two categorize, namely static and dynamic algorithms. The difference is that the dynamic algorithm takes into account the nodes previous states and performance whilst the static doesn't. The static load balancing simply looks at things like processing power and available memory which might lead to the disadvantage that the selected node gets overloaded \cite{perfAnalysisLoadCloud}

A dynamic load balancing can work in two ways, either distributed or non-distributed. In the distributed case the load balancing algorithm is run on all the nodes and the task of load  balancing is shared among them. This implies that each node has to communicate with all the others, effecting the overall performance of the network. % true also for static load balancing? i.e. distr/non-distr

In the non-distributed case the load balancing algorithms is done by only a single node or a group-node. Non-distributed load balancing can be run in semi-distributed form, where the nodes are grouped into clusters and each such cluster has a central node performing the load balancing. Since there is only one load balancing node the number of messages between the nodes are decreased drastically but instead we get the disadvantage of the central node becoming a single-point of failure and a bottleneck in the system. Therefore this centralized form of load balancing is only useful for small networks \cite{perfAnalysisLoadCloud}.

\iffalse
We will briefly mention some dynamic load balancing algorithms: % skip static load balancing algorithms or not?

\begin{itemize}
\item Central Queue Algorithm A non-distributed algorithm where the central manager maintains a cyclic FIFO-queue. Whenever a new activity arrives the manager inserts it into the queue and whenever a new request arrives it simply picks the first activity in the queue. 

\item Local Queue Algorithm When a new task is created on the main host it will be allocated on under-loaded nodes. Afterwards all new tasks are allocated locally since ... \cite{perfAnalysisLoadCloud}. The algorithm is receiver initiated since when a node is under-loaded it randomly sends requests to remote load managers

\item Ant Colony Optimization Algorithm As the name implies this algorithm is inspired by the behavior of real ants to find a optimal solution. Whenever an ant find food it moves back to the colony while leaving "markers", i.e. laying pheromone on the way. When more ants find the same place the path the pheromone will become denser. 
%Förtydliga om det ska ingå i rapporten.

\item Honey Bee Foraging Algorithm This algorithm is quite similar to the Ant Colony Optimization algorithm. When bees find food they return to the bee's colony and use special dance movements for informing the other bees of how much food there is and where it's located. When the forager bees find more food a more energetic dance takes place. This phenomenon can be applied to servers, when an overloaded server receives a request it redirects it to other under loaded servers.

% De fyra ovan är från \cite{perfAnalysisLoadCloud}

% Static algorithms: (?)
\item Bidding An overloaded node request bids from other nodes. The node with the best bid (i.e. lowest load) wins the job.
\item Max-Min 
\item Min-Min 
\end{itemize}

\fi

\section{Task scheduling}
Task scheduling is the process of mapping tasks to available resource. A scheduling algorithm can be divided into three simple steps \cite{optSchedCloud}:

\begin{enumerate}
	\item Collect the available resources
	\item Based on task requirements and resource parameters choose resources to schedule the task on
	\item Send the task to the selected resources
\end{enumerate}

Based on which requirements and parameters are considered in step 2 above a task scheduling algorithm can achieve different goals. They can aim at maximizing the total reliability, minimizing the overall system load and meeting all the tasks deadlines \cite{schedulingSurvey}. 

Task scheduling algorithms can be divided into static and dynamic algorithms depending on if the scheduling mapping is based on pre-defined parameters or if the parameters might change during runtime \cite{schedReplicas}. 

Many studies have been recently done to improve reliability by proper task allocation in distributed systems, but they have only considered some system constraints such as processing load, memory capacity, and communication rate \cite{optTaskAllocationForMaxRel}. In fact finding an optimal solution and maximizing the overall system reliability at the same time is a NP-hard problem \cite{optTaskAllocationForMaxRel} \cite{taskAllocationSwarm} \cite{schedulingSurvey}.

%COPIED: 
%One of the drawbacks with this type of scheduling is that its efficiency will have an impact on the cloud environment’s performance.

%COPIED:
%The problem of finding an optimal task allocation with maximum system reliability has been shown to be NP-hard; thus, existing approaches to finding exact solutions are limited to the use in problems of small size. This paper presents a hybrid particle swarm optimization (HPSO) algorithm for finding the near-optimal task allocation within reasonable time. \cite{taskAllocationSwarm}.

\section{Monitoring}
Heartbeat-based monitors in which lack of a timely heartbeat message from the target indicates failure.
Test-based monitors which send a test message to the target and wait for a reply. Messages may range from OSlevel pings and SNMP queries to application level testing, e.g., a test query to a database object.
End-to-end monitors which emulate actual user requests. Such monitors can identify that a problem is somewhere along the request path but not its precise location.
Error logs of different types that are often produced by software components. Some error messages can be modelled as monitors which alert when the error message is produced.
Statistical monitors which track auxiliary system attributes such as load, throughput, and resource utilization at various measurement points, and alarm if the values fall outside historical norms.
Diagnostic tools such as filesystem and memory checkers (e.g., fsck) that are expensive to run continuously, but can be invoked on demand. [Probabilistic Model-Driven Recovery in Distributed Systems]

%COPIED:
%Globus [9] provides a heartbeat service to monitor running processes to detect faults  \cite{effTaskReplMobGrid}

%COPIED
%Nodes can alternate between working correctly and being crashed in our model. Hence, the status of a node is modeled by a state machine with two states, failed and working. Failed nodes do not send messages nor do they perform any computation. Working nodes execute faithfully the diagnosis procedure. In this section, we derive lower bounds on the diagnostic latency, start-up time, and state holding time achievable by any heartbeat-based diagnosis algorithm in completelyconnected networks. The maximum time between two consecutive heartbeats arriving from a continuously working node at any other node in the system sets a limit on how early failed nodes can be identified by the absence of a heartbeat. \cite{distDiagnosis}


\chapter{Approach} \label{ch:approach}

\section{System model}
In this section, we describe the system and application models employed in this work.

\subsection{Computational environment}
In this paper we assume that all resources are within the same cluster, with low latency connections between them. The nodes consist of heterogeneous hardware and interconnected nodes as shown in figure~\ref{fig:computational_environment}. We refer to a computational resource as a \emph{node}. Due to the heterogenousity, nodes will have a varying behaviour in terms of failure, i.e. varying failure rates.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/computational_environment.pdf}
\caption{Computational environment, interconnected nodes} \label{fig:computational_environment}
\end{figure}

\subsection{Storage of data}
All information, such as how many replicas are currently executing, and on which nodes they are running, are globally available. This is achieved by the use of Distributed Hash Table (DHT), further explained in ~\ref{ch:calvin_storage}. The use of DHT efficiently avoids having a single-point-of-failure which would have been the case with having a single database.

%\subsection{Monitoring}
%Heart beat monitoring detects when a connectivity to a node is lost. In the case of lost connectivity, the node is assumed dead.

%COPIED: "“The most basic form of monitoring is a simple heartbeat system. A monitor process listens for periodic messages from the monitored components. The message simply indicates that the component continues to function correctly enough to send messages. " \cite{surveyFaultParallel}.

\subsection{Application model}
The fault-tolerant framework presented in this paper is general and may be used in various contexts. However, it is of special value for long running applications in a dynamic environment where a certain level of reliability must be met, but the reliability of the resources vary over time. Long running applications are particularly vulnerable to failure because they require many resources and usually must produce precise results \cite{relGridSystems}.

In this paper we use a simple example application in our experiments. The application can be modelled as shown in figure~\ref{fig:app_model}.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/app_model.pdf} 
\caption{An application model where a producer transmits data to a task $A$, which transforms the data, and sends the result to a consumer. $A$ may be seen as a task or service running in a cluster and requiring a certain level of reliability.}\label{fig:app_model}
\end{figure}

%TODO
COMMENT: we should have some real examples of applications which would benefit from using our framework. ?
\begin{itemize}
\item telephone system?
\item video transcoding when streaming video to mobile phone? In this case one could imagine a case where a video file is located on a server with limited processing power. The video could therefore be streamed to another server in the cluster, with more processing power, which encodes it on the fly and stream the result to the user. To keep a continuous stream of data to the user, even in the case of failure of the encoding server, one could replicate the task and stream the video to several servers which encode and transmit the results to the user. While this would increase the amount of transmitted data to the user's mobile, it would also increase the user experience.
\item Long running simulations? \cite{relModelDistSimSystem}
\end{itemize}

\iffalse

COPIED:
“Services in large-scale distributed environments (e.g., The Internet of Things [15]) are required to stay and continue operating even in the presence of malicious and unpredictable circumstances; that is, their processing capacity must not be significantly affected by the user requirements [11,8] “ \cite{imprRelAdaptRL}.
\\\\
COPIED: “Most workloads are large-scale long-running 3D scientific simulations, e.g. for nuclear stockpile stewardship. These applications perform long periods (often months) of CPU computation, interrupted every few hours by a few minutes of I/O for checkpointing. “  \cite{studyOfFailures}
\\\\
COPIED: “given the long execution times of many of the parallel applications that we are targeting – those in the scientific domain at national laboratories and supercomputing centers. “  \cite{implicationsOfFailures}. 
\\\\
COPIED: "The end result is that long-running, distributed applications are interrupted by hardware failures with increasing frequency"  \cite{surveyFaultParallel}.

\fi

\subsection{Replication scheme}
We will ensure reliability based on task replication, using active replication, where each replica receives the same input, and performs the same calculations. Figure~\ref{fig:app_model_replication} shows how the application in figure~\ref{fig:app_model} looks after replicating task $A$ 4 times.

\begin{figure}[!hbt]
\centering
\includegraphics[scale=0.5]{images/app_model_replication.pdf} 
\caption{An application model where a task or service $A$ has been replicated 4 times.}\label{fig:app_model_replication}
\end{figure}

\subsection{Fault model}
In this paper, we adapt the \emph{fail-stop} fault model, which is commonly used when presenting fault-tolerance techniques \cite{surveyFaultParallel}. Nodes in the system have one of two states: \emph{operational} or \emph{failed}. If a node fails, all running tasks on that node are dead. Furthermore, after a node has died, it will be restarted. However, the tasks that were being executed before it died will not be restarted when the node is restarted. The reason for why a node died is irrelevant. Our model do not care whether a link died, or it was a hardware failure. 

Like \cite{selfAdaptRel}, we assume failures depends on the nodes, not the jobs running on them and the computations they perform.

\subsubsection{Failure distribution}
%TODO do we want to assume Poisson process with constant failure rates? Maybe we should start with this and then later on use a more sophisticated model which for example takes the time a day into acocunt...
We assume failures follow a Poisson process, as this seems to be widely accepted in the research community. However, while most assumes constant failure rates, our failure distributions are dynamic and monitoring of the system resources and events allows for the framework to be self-adaptive and dynamic in terms of resource failure rates.

The Poisson distribution is defined as follows:

\begin{equation} \label{eq:Poisson}
P(k failures) = \dfrac{\lambda^k \cdot e^{-\lambda}}{k!}
\end{equation}

where $\lambda$ is the failure rate, i.e. the average number of failures occurring in time \emph{t}. In our case \emph{t} represents the time it take to detect a node failure and spawn a new replica. From \emph{t} and \emph{MTBF} we can calculate the failure rate $\lambda$ as $\lambda = t/MTBF$. In our case \emph{k} will always equal zero since we are interesting in knowing the probability that no failure occur during time \emph{t}. 

However, when a system first is set up and started, no knowledge about the failure rates of the system is known. Therefore, when the system is set up, we do start with assuming constant failure rates. As time goes and the time of failure for nodes are stored, one will get a more precise value for the MTBF for the nodes in the system. The time it takes to detect node failure and spawn a new replica is further discussed in ~\ref{sec:time_t}.

\subsection{Reliability model}
Using replication of a task, and reliability defined as in definition~\ref{def:task_replica_reliability}, the probability that a task $T$ with $n$ replicas is successful during a time $t$, corresponding to at least one replica survives, i.e. not all fail, can be expressed as 

\begin{equation} \label{eq:task_reliability}
R_{T}(t) =  1 - \prod\limits_{k=1}^n F_{Rep_k}(t)
\end{equation}

where $F_{Rep_k}(t)$ is the probability that replica $k$ fails during time $t$. Equation~\ref{eq:task_reliability} corresponds to definition~\ref{def:task_replica_reliability}, i.e. at least one replica is always running.

Since we assume tasks themselves do not fail unless the resources they use fail, the reliability of a task replica is dependent on the reliability of the resources it uses. Limiting the model to node failures, either in terms of hardware failure or lost connectivity, equation~\ref{eq:task_reliability} can with equation~\ref{eq:resource_reliability} be re-written as

\begin{equation} \label{eq:task_reliability_2}
R_{T}(t) = 1 - \prod\limits_{k=1}^m  F_{Res_k}(t)
\end{equation}

where $F_{Res_1}(t) \cdots F_{Res_m}(t)$ are the failure probability functions of the $m$ resources on which the $n$ replicas are running. Using this model, reliability is only increased through replication if the replicas are scheduled on separate nodes.

Given a time $t$, a desired reliability level $\lambda$, and assuming the replicas are running on separate nodes, we get

\begin{equation} \label{eq:desired_rel}
\lambda \leq 1 - \prod\limits_{k=1}^n  F_{Res_k}(t)
\end{equation}

which must be fulfilled by the system. Assuming that failure rates differ among resources, fulfilling equation~\ref{eq:desired_rel} is a scheduling problem, since the number of replicas needed is dependent on which resources the replicas are executed on.

The scheduling problem to fulfil a reliability $\lambda$ refers to selecting $n$ nodes on which to place $n$ replicas such as the reliability level of the task, expressed in equation~\ref{eq:task_reliability_2}, exceeds $\lambda$.

\subsection{Expressing time t} \label{sec:time_t}
The time $t$ used in \ref{eq:desired_rel} consists of the time it takes from that a failure happened, until a new replica is operational. $t$ can therefore be expressed as 

\begin{equation} \label{eq:rep_time}
	t = Tf + Tr
\end{equation}

where $Tf$ is the time to detect that a node has failed, and $Tr$ is the time it takes to spawn a new replica.

\subsubsection{Failure detection time}
The time to detect a failed node depends on the frequency of the heartbeats.

\subsubsection{Replication time}
The time it takes to replicate a task depends on the time to find out on which nodes current replicas are running, and sending a replicate response to one of these nodes. Since the reliability is dependent on which nodes the replicas are running on, not solely on the number of replicas, the receiving node must also find a node which has no replica already, and then send a request to that node including the current state of the task to replicate. %TODO show with a graph.

\begin{definition}
Latency = propagation time + transmission time + queueing time + processing delay
\end{definition}

\begin{definition}
Propagation time = distance / propagation speed
\end{definition}

\begin{definition}
Handshake time = (ACK size / bandwidth) + (SYN+ACK size / bandwidth) + (ACK size / bandwidth)	
\end{definition}

\begin{definition}
Transmission time = package size / bandwidth
\end{definition}

\begin{definition}
Package size: handshake + message size
Handshake size = ACK + (SYN+ACK) + ACK = 
\end{definition}

\begin{definition}
Replication time = “node dead discovery time” + “time to retrieve data from storage” + “time to send data to node” + “time to replicate”
\end{definition}

\subsection{Scheduling algorithm}
A simple greedy scheduling, similar to one presented in \cite{effTaskReplMobGrid}, which fulfills equation~\ref{eq:desired_rel} is shown below:

\begin{algorithmic}
\State $n\gets 0$
\State $nodes\gets available\ nodes$\Comment{sorted by reliability, highest first}
\While {$\lambda \geq \prod\limits_{k=1}^n  R_{Res_k}(t)$}
	\State $node\gets nodes.pop$\Comment{take the node with highest reliability}
	\State{$replica\gets new\ replica$}
	\Call{place replica on node}{$replica$, $node$}
	\State $n\gets n + 1$
\EndWhile
\end{algorithmic}

\subsection{Self-adapting model}
Adaption refers to changing the behaviour depending on the changing state of the system. For a distributed system, resources most be continously monitored in order to adapt to changing behaviour \cite{imprRelAdaptRL}.

To ensure a certain level of reliability, the framework must ensure both that the current state of the system is taken into account when calculating the number of replicas needed. Furthermore, the system and the running jobs must be continously monitored, and the number of replicas must be increased is the reliability of a running job decreases below the desired level, or, the number of replicas must be decreased in order not to waste resources.

When monitoring the system, onc must monitor all parameters affecting the reliability. In our case, we consider only the nodes' mean-time-between-failures, the time to detect node failures and the time it takes to spawn a new replica.

The replication time is stored per actor type. Since the state of different types of actors vary, and thereby the size of the states, the time to send the state from one node to another will also vary. 

% TODO machine learning section?
TODO: If we have time:
Furthermore, we will take node CPU usage, and the time of day of the event into account in our fault model. This is based on that resource failure depends on both system load and time of day \cite{implicationsOfFailures} \cite{studyOfFailures}.

\iffalse
COPIED: “adaptation to changing conditions is achieved by both adaptive scheduling and adaptive execution” \cite{evalOfGridRel}.

COPIED:
"the distributed systems require consistent and iterative monitoring for valuation resources’ behaviors and processing requirements. Therefore, an autonomous, scalable and highly dynamic learning approach is deserved \cite{imprRelAdaptRL}

%TODO predict future MTBF? machine learning? cpu, memory usage, network load, etc.

COPIED:
“The individual component can be monitored in real time and updates the parameters dynamically for the exponential distribution. The monitored information is simple: just the number of failures over the total running time of this component which has actually been recorded by log files in today’s grids. Such a dynamic updating scheme can further validate the exponential assumption, though we may relax the above assumption somewhat to allow reasonable or gradual change in the failure rate (such as wear out) because, during a short enough period of service time, the parameter cannot change too much and using the latest value should be a good approximation “ \cite{hierarchicalRelModeling}.

COPIED: “in the proposed system, we need to determine the degree of over-provisioning or job replicas as small as possible in order to minimize the system overhead “ \cite{designFaultTolerantSched}.
\fi

\section{Implementation} \label{sec:implementation}
For implementing our model, we use an actor based application environment called \emph{Calvin} \cite{calvin}.

\subsection{Calvin} Calvin is a light-weight actor-based application environment for IoT applications and was developed by \emph{Ericsson} and made open source in the summer of 2015. While Calvin is an application environment for IoT applications, is suites well for implementation of our model.

\subsection{Storage} \label{ch:calvin_storage}
Calvin uses the Kademlia implementation of a DHT. This enables any data stored by a node is distributed and available for all nodes in the network. DHT will not be further explained in this thesis.

\subsubsection{Actor model}
An application in Calvin consist of a set of connected \emph{actors}. An actor usually represents part of a device, service or computation. Actors communicate by sending data on their outports to other actors inports.  %TODO

\subsubsection{Runtimes}
The Calvin framework use a concept of \emph{runtimes}. A mesh of connected runtimes makes up the distributed execution environment on which one can deploy application. A runtime is a self-managed container for application actors and provides data transport between runtimes and actors. Each runtime has a \emph(storage), all the information stored in storage is first stored locally and later flushed, i.e. distributed in a multicast approach.

\subsubsection{Monitoring}
The runtimes in the Calvin framework use a heart-beat technique to detect runtime connectivity.

\subsubsection{Actor replication}
We extended the Calvin framework to allow a fanout/fanin connectivity model where actors can have multiple producers (fanin) and multiple consumers (fanout). Furthermore, we implemented functionality for dynamically connecting and disconnecting actors, which allowed for dynamically adding and removing replicas of an actor.

\subsection{Node Resource Reporter}
we have added an actor which automatically deploys on a runtime at runtime started-up. It reports the nodes CPU usage once every second to the runtime which distributed it to all connected nodes. Each node has a resource manager which stores all nodes usages.

\subsection{Resource Manager}
Besides storing the connected nodes usages the resource manager stores information about node failures. If a node fails a timestamp and the failed node's usage is stored, among with the time it took to replicate a new replica for each of the actors which were running on the failed node.

Due to its continuously communication with other nodes it fits great as a monitoring system. If a node stops receiving updates from a node we can assume that it has failed. But since there already is a built-in heartbeat system we will use that to detect node failures.

\chapter{Evaluation} \label{ch:evaluation}

\section{Computational environment}
In order to evaluate our model using Calvin, we set up an execution environment consisting of 7 servers, and started a runtime on each server. One of these runtimes was stable, while each of the other runtimes were continously killed and restarted, simulating a node failure, after which it was restarted. 

\section{Application}
The application used in the experiments is of simplest form, consisting only of one consuming  and one producing actor. The producing actors produced integer numbers and sent those to the consuming actor which printed the values to standard out. Since the consuming actor is on a stable server, only the producing ones were to be replicated by the system.

%REPLICATION TIME:
%An average of the last "self.history" (default is 5) times. We consider these values to be most relevant. 

\section{Replication time}
%TODO

\section{Node lost discovery time}
%TODO

\section{Reliability level}
In this experiment, each runtime was given psudeo-random numbers according to a normal distribution with mean of 30 and a standard deviation of 4. the 6 runtimes all had the same mtbf, 30 seconds. 

After the application was started, the the reliability level of the nodes on which replicas were running was periodically measured. The desired level of reliability for the producing actors were 0.999.

The test was considered successful if the average reliability level during the duration of the test exceeded 0.999.

\subsection{Result}
PUT THIS IN RESULT CHAPTER?

The experiment ran for XXX minutes, and figure ~\ref{fig:exp_reliability_level} shows a 5-second average reliability during this period. Figure ~\ref{fig:exp_reliability_level_replicas} shows the number of replicas used over time. The average reliability for the duration of the whole test was XXX, above the desired level of 0.999.

\section{Optimal number of replicas}
Similar to the previous experiment, each runtime was given psudeo-random numbers according to a normal distribution. However, in this experiment, the mean value varied among the runtimes. The mean values given to the different nodes are presented in table ~\ref{table:exp_nodes_means}, and all had a standard deviation of 4. Since nothing is known about the nodes when the test starts, it was assumed the mean-time-between-failure were 20. 

After the application was started, the the reliability level of the nodes on which replicas were running was periodically measured. The desired level of reliability for the producing actors were 0.999.

The test was considered successful if over time, the number of replicas used decreased, as the more reliable nodes were chosen, and the average reliability for the duration of the test exceeded 0.999.

\subsection{Result}
PUT THIS IN RESULT CHAPTER?
it was assumed with mean of 30 and a standard deviation of 4. the 6 runtimes all had the same mtbf, 30 seconds. 

The experiment ran for XXX minutes, and figure ~\ref{fig:exp_opt_replicas_reliability_level} shows a 5-second average reliability during this period. Figure ~\ref{fig:exp_opt_replicas_node_replicas} shows the number of replicas per node.

\section{?}

Discuss the total average reliability level, does the current level undergo the required during a node failure? We can have a reliability high enough so we can afford losing the most reliable node, but what if we lose the second most reliable node before we replaces the most reliable? We can't assure that the reliability always is above the required level. In our model we have chosen to have a reliability high enough so the required reliability is held even if the most reliable node is lost. From performance metrics we have found a typical replication time of XXX ms, which is almost negligible compared to MTBF, i.e. the risk of losing the second most reliable node before we have replicated a new replica.


\chapter{Limitations} \label{ch:limitations}
%TODO
Our model is obviously limited in that we do not account for link failures. However, the reliability model could easily be extended to include link failure probabilities, which then need to be taken into account in the scheduling algorithm as well.

Furthermore, using a fail-stop fault model, we assume that when a node dies all other nodes are aware of this. This is clearly limiting, as in the case of a link failure, a node may become unavailable for one node while being available for another.

%MOVE?
In case of link failure two nodes might get unavailable of each other. Each node will assume that the other has failed and try to restart it while replicating new replicas until required reliability is achieved. Therefore, from a system perspective, there is a unnecessary high reliability which result in an unnecessary high impact on the network load.
%Since our system in a dynamic way keeps track of the total reliability during runtime and kills unnecessary replicas this type of link failure isn't a problem? (TODO)

Our definition of reliability excludes the possibility of tasks calculating incorrect results, which is the case when using a Byzantine fault model. Since already using active replication to ensure reliability, it would be trivial to extend it to use techniques such as majority voting or \emph{k-modular redundancy} in order to extend the reliability definition to also include that the job calculates the correct result.

Our primary objective is to ensure a certain level of reliability. By using active replication, we require a lot more resources, which puts an extra burden on the system. In addition, the extra load on the system may affect the execution time, thus decreasing task performance.

Our model is yet to be evaluated on highly unreliable system during extreme load. In this case, due to the unreliability of the system, the number of replicas needed to ensure the desired reliability level will increase. This will further increase the system load, thereby decreasing the reliability of the system even further. This may turn into a vicious circle.

\subsection{Reliability model}
\label{sec:limitations_reliability_model}
\iffalse
COPIED:

Many engineers and researchers base their reliability models on the assumption that components of a system fail in a statistically independent manner. This assumption is often violated in practice because environmental and system specific factors contribute to correlated failures, which can lower the reliability of a fault tolerant system. A simple method to quantify the impact of correlation on system reliability is needed to encourage models explicitly incorporating correlated failures. Previous approaches to model correlation are limited to systems consisting of two or three components or assume that the majority of the subsets of component failures are statistically independent. \cite{discContRelModel}

the interrelationship constraint between task modules has not been taken into account in calculation of distributed system reliability (DSR). In distributed simulation, the LPs simulation advances are bound by synchronization constraint, which will affect the executive time of system components. \cite{relModelDistSimSystem}

We do not take into account parameters such as X and Y [TODO].

Furthermore failure data which they have collected for their experiments through in-house testing cannot be compared with failures that can occur under actual operational environment \cite{surveyReliabilityDistr}

A lot of researchers are of the view that service providers must provide some other detail to compute reliability of both type of services i.e. atomic service and composite service. Such as, external services it uses, how service are glued together in composite service, how frequently they call each other, and flow graph describing behavior of service [2]. Service Oriented Reliability Model (SORM) [18] computed reliability of atomic and composite services exploiting distinct technique \cite{surveyReliabilityDistr}

Given the failure probability Pfik of each one of the mi replicas Ti k of task Ti , the new failure probability P fi0 for task Ti is:
0 mi Y
Pfi =Pfi· Pfik. (2) k=1
The above corresponds to the probability of the event “all the replicas and the original task fail”. Respectively, the success probability is equal to the probability of the event “the original task or at least one of its replicas executes successfully”. The number of replicas issued depends on the failure probabilities of the original tasks and on the desired fault tolerance level in the Grid infrastructure.  \cite{effTaskReplMobGrid}
\fi

\chapter{Future Work} \label{ch:future_work}
%TODO
Replication impose extra burden on the system as additional resources are needed and computational power is wasted. By combining checkpointing techniques with active replication, the number of replicas needed could possible be decreased. (\cite{adaptiveCheckPointAndRep} combines checkpointing and replication)
\\\\
Predict future failures - machine learning (many parameters could be taken into account), and when the probability of failure reaches above a certain threshold, the running tasks on that node could be migrated. A high reliability in failure prediction allows for fewer replicas to be needed.
\\\\
Implement consensus in Calvin system

\subsection{Extended model}
Reliability of server’s availability and scalability (such as File Server, DB servers, Web servers, and email servers, etc.), communication infrastructure, and connecting devices. \cite{surveyReliabilityDistr}

For measure reliability according to the dynamic definition (Definition \ref{def:dyn_reliability}) we will use the following formula:

\begin{equation} \label{eq:overall_reliability}
R(t) = R_{1}(t) \cdot R_{2}(t) \cdots R_{n}(t)
\end{equation}
where $R_{k}(t)$ is the probability that factor $k$ is free from failures during time $t$. Some factors to consider are software (the program itself), OS (the device executing the program), hardware, network, electrical supply and load. The factors can be divided into static and dynamic factors. The static factors are those which does not change that frequently, such as electrical supply or hardware/software while the dynamic factors are those changing more frequently, for instance the current load (or load average for last 5 minutes etc). 


\chapter{Conclusions} \label{ch:conclusions}

\begin{thebibliography}{50}

\bibitem{taskAllocation}
	Sol M. Shatz, Jia-Ping Wang and Masanori Goto,
	\emph{Task Allocation for Maximizing Reliability of Distributed Computer Systems},
	Computers, IEEE Transactions on, Volume 41  Issue 9,
	Sep 1992
	
\bibitem{surveyReliabilityDistr}
	Waseem Ahmed and Yong Wei Wu,
	\emph{A survey on reliability in distributed systems},
	Journal of Computer and System Sciences Volume 78 Issue 8,
	December 2013, Pages 1243–1255
	
\bibitem{surveyRelPrediction}
	A. Immonen, E. Niemelä,
	\emph{Survey of reliability and availability prediction methods from the viewpoint of software architecture},
	Software \& Systems Modeling, p.49-65,
	February 2008

\bibitem{relDistApplications}
	C. A. Tănasie, S. Vîntutis, A. Grigorivici,
	\emph{Reliability in Distributed Software Applications},
	Informatica Economică vol. 15, no. 4/2011,

\bibitem{relGridSystems}
	Christopher Dabrowski,
	\emph{Reliability in grid computing systems},
	Concurrency Computation: Practice Experience,
	2009

\bibitem{compStudyLoadAndCloud}
	Mayanka Katyal and Atul Mishra,
	\emph{A Comparative Study of Load Balancing Algorithms in Cloud Computing Environmen},
	International Journal of Distributed and Cloud Computing, Volume 1 Issue 2,
	2013
	
\bibitem{relAndPerfGridServices}
	Y. Dai, G. Levitin
	\emph{Reliability and Performance of Tree-Structured Grid Services},
	IEEE TRANSACTIONS ON RELIABILITY, VOL. 55, NO. 2, 
	June 2006

\bibitem{surveyFaultParallel}
	Michael Treaster,
	\emph{A Survey of Fault-Tolerance and Fault-Recovery Techniques in Parallel Systems},
	Cornell University Library,
	Jan 2005

\bibitem{faultTolerantFundamentals}
	F. C. Gärtner,
	\emph{Fundamentals of Fault-Tolerant Distributed Computing in Asynchronous Environments},
	ACM Computing Surveys Vol. 31 Issue 1 p. 1-26, 
	March 1999 

\bibitem{gridWorkflow}
	Soonwook Hwang and Carl Kesselman,
	\emph{Grid Workflow: A Flexible Failure Handling Framework for the Grid},
	High Performance Distributed Computing, 2003. Proceedings. 12th IEEE International Symposium on, p. 	126-137, 
	June 2003

\bibitem{perfAnalysisLoadCloud}
	Prashant D. Maheta , Kunjal Garala and Namrata Goswami,
	\emph{A Performance Analysis of Load Balancing Algorithms in Cloud Environment},
	Computer Communication and Informatics (ICCCI), 2015 International Conference on,
	Jan 2015
	
\bibitem{taskSchedulingReplication}
	Shuli Wang et. al.
	\emph{A Task Scheduling Algorithm Based on Replication for Maximizing Reliability on Heterogeneous Computing Systems},
	Parallel \& Distributed Processing Symposium Workshops (IPDPSW), 2014 IEEE International, p. 1562 1571, 
	May 2014

\bibitem{softRelRoadmap}
	Michael R. Lyu
	\emph{Reliability Engineering: A Roadmap},
	Future of Software Engineering, FOSE ’07, p. 153-170,
	23–25 May 2007
	
\bibitem{dynAdaptRepl}
	Guessoum Z. et. al.
	\emph{Dynamic and Adaptive Replication for Large-Scale Reliable Multi-agent Systems},
	Lecture Notes in Computer Science pp 182-198,
	April 2003

\bibitem{algoMaxRelEndToEndConstraint}
	F. Cao , M. M. Zhu,
	\emph{Distributed workflow mapping algorithm for maximized reliability under end-to-end delay constraint},
	The Journal of Supercomputing, Vol. 66, Issue 3, p. 1462-1488,
	December 2013

\bibitem{algoMinExTime}
	A. Dogan, F. Özüner,
	\emph{Matching and Scheduling Algorithms for Minimising Execution Time and Failure Probability of Applications in Heterogeneous Computing},
	IEEE Transactions on parallel and distributed systems, Vol. 13, No.3,
	March 2002

\bibitem{relModelDistSimSystem}
	H. Wan, H. Z. Huang, J. Yang, Y. Chen,
	\emph{Reliability model of distributed simulation system},
	Quality, Reliability, Risk, Maintenance, and Safety Engineering (ICQR2MSE), 2011 International Conference,
	June 2011

\bibitem{relModelAnalysis}
	C. S. Raghavendra, S. V. Makam,
	\emph{Reliability Modeling and Analysis of Computer Networks},
	IEEE Transactions on Reliability  Vol. 35,  Issue. 2,
	June 1986

\bibitem{cloudServiceRel}
	Y. Dai, B. Yang, J. Dongarra, G. Zhang
	\emph{Cloud Service Reliability: Modeling and Analysis},
	%TODO published?

\bibitem{optTaskAllocationForMaxRel}
	H. R. Faragardi, R. Shojaee, M. A. Keshtkar, H. Tabani,
	\emph{Optimal task allocation for maximizing reliability in distributed real-time systems},
	Computer and Information Science (ICIS), 2013 IEEE/ACIS 12th International Conference,
	June 2013

\bibitem{perfImplPerCheckPoint}
	A. J. Oliner, R. K. Sahoo, J. E. Moreira, M. Gupta,
	\emph{Performance Implications of Periodic Checkpointing on Large-scale Cluster Systems},
	Parallel and Distributed Processing Symposium, 2005. Proceedings. 19th IEEE International,
	April 2005

\bibitem{studyOfFailures}
	B. Schroeder, G. Gibson,
	\emph{A large-scale study of failures in high-performance computing systems},
	IEEE Transactions on Dependable and Secure Computing (Volume:7,  Issue: 4),
	November 2010

\bibitem{implicationsOfFailures}
	Y. Zhang, M. S. Squillante, A. Sivasubramaniam, R. K. Sahoo,
	\emph{Performance Implications of Failures in Large-Scale Cluster Scheduling},
	Job Scheduling Strategies for Parallel Processing, Volume 3277 of the series Lecture Notes in Computer Science pp 233-252,
	2005

\bibitem{discContRelModel}
	L. Fiondella, L. Xing,
	\emph{Discrete and continuous reliability models for systems with identically distributed correlated components},
	Reliability Engineering \& System Safety p. 1-10,
	Jan 2015

\bibitem{effTaskReplMobGrid}
	Antonios Litke et. al.,	
	\emph{Efficient task replication and management for adaptive fault tolerance in Mobile Grid environments},
	Future Generation Computer Systems Vol 23 Issue 2 p. 163-178,
	February 2007

\bibitem{realTimeSchedAlgo}
Real-time fault-tolerant scheduling algorithm for distributed computing systems

\bibitem{distDiagnosis}
Distributed Diagnosis in Dynamic Fault Environments

\bibitem{optCheckpointInterval}
A higher order estimate of the optimum checkpoint interval for restart dumps

\bibitem{factorsAffectingRel}
An analysis of factors affecting software reliability

\bibitem{SLASched}
SLA-aware Resource Scheduling for Cloud Storage

\bibitem{algoOptTimeMaxRel}
An Algorithm for Optimized Time, Cost, and Reliability in a Distributed Computing System

\bibitem{imprRelAdaptRL}
Improving reliability in resource management through adaptive reinforcement learning for distributed systems

\bibitem{selfAdaptRel}
Self-Adapting Reliability in Distributed Software Systems

\bibitem{schedReplicas}
Scheduling Fault-Tolerant Distributed Hard Real-Time Tasks Independently of the Replication Strategies

\bibitem{calvin}
Calvin - Merging Cloud and IoT

\bibitem{taskAllocationSwarm}
Task allocation for maximizing reliability of a distributed system using hybrid particle swarm optimization

\bibitem{optResourceAllMaxPerformance}
Optimal Resource Allocation for Maximizing Performance and Reliability in Tree-Structured Grid Services 

\bibitem{matchSchedAlgoMinFailure}
Matching and Scheduling Algorithms for Minimizing Execution Time and Failure Probability of Applications in Heterogeneous Computing

\bibitem{safetyRelTaskAllocation}
Safety and Reliability Driven Task Allocation in Distributed Systems

\bibitem{improvedTaskAllMaxRel}
Improved Task-Allocation Algorithms to Maximize Reliability of Redundant Distributed Computing Systems 

\bibitem{designFaultTolerantSched}
Design of a Fault-Tolerant Scheduling System for Grid Computing

\bibitem{evalReplicationSched}
Evaluation of replication and rescheduling heuristics for grid systems with varying resource availability

\bibitem{faultTolerantSchedPolicy}
Fault-Tolerant Scheduling Policy for Grid Computing Systems

\bibitem{adaptiveCheckPointAndRep}
Adaptive Task Checkpointing and Replication: Toward Efficient Fault-Tolerant Grids

\bibitem{decisionModelTaskAllocation}
The decision model of task allocation for constrained stochastic distributed systems

\bibitem{perfRelNonMarkovian}
Performance and Reliability of Non-Markovian Heterogeneous Distributed Computing Systems

\bibitem{hierarchicalRelModeling}
A Hierarchical Modeling and Analysis for Grid Service Reliability

\bibitem{relAnalysisFRA}
Reliability analysis of distributed systems based on a fast reliability algorithm

\bibitem{relGridServicePredConstraint}
Reliability and Performance of Star Topology Grid Service with Precedence Constraints on Subtask Execution

\bibitem{relModelWebServices}
A Software Reliability Model for Web Services

\bibitem{studyServiceRel}
A study of service reliability and availability for distributed systems

\bibitem{efficientRelAnalysisAlgo}
Efficient algorithms for reliability analysis of distributed computing systems

\bibitem{evalOfGridRel}
Evaluating the reliability of computational grids from the end user’s point of view

\bibitem{generalAlgoRelEval}
A Generalized Algorithm for Evaluating Distributed-Program Reliability

\bibitem{realTimeRelAnalysis}
Real-Time Distributed Program Reliability Analysis

\bibitem{collaborativeReliability}
Collaborative Reliability Prediction of Service-Oriented Systems

\bibitem{faultToleranceGrid}
Fault Tolerance Techniques in Grid Computing Systems

\bibitem{faultToleranceChallenges}
Fault Tolerance Challenges, Techniques and Implementation in Cloud Computing

\bibitem{improvingPerformanceReplication}
Improving Performance via Computational Replication on a Large-Scale Computational Grid

\bibitem{adaptiveMASReplication}
Adaptive Replication in Fault-Tolerant Multi-Agent Systems

\bibitem{replicatingAgents}
Improving Fault-Tolerance by Replicating Agents

\bibitem{adaptiveAgentReplication}
Towards Reliable Multi-Agent Systems: An Adaptive Replication Mechanism

\bibitem{replicationManagement}
Fault Tolerant Algorithm for Replication Management in Distributed Cloud System

\bibitem{experimentalFailureAssessment}
Experimental Assessment of Workstation Failures and Their Impact on Checkpointing Systems

\bibitem{schedulingSurvey}
A Survey on Scheduling and the Attributes of Task Scheduling in the Cloud

\bibitem{optSchedCloud}
Scheduling Optimization in Cloud Computing

\end{thebibliography}

\begin{appendices}
\chapter{A}

\end{appendices}









\iffalse

% Below is the report template found online

\chapter[Short on Formatting]{Formatting}
Avoid empty spaces between \textit{chapter}-\textit{section}, \textit{section}-\textit{sub-section}. For instance, a very brief summary of the chapter would be one way of bridging the chapter heading and the first section of that chapter.
\section{Page Size and Margins}
Use A4 paper, with the text margins given in Table \ref{tab:margins}.
\begin{table}[!hbt]
\centering
\caption{Text margins for A4.}\label{tab:margins}
\begin{tabular}{cc}
\hline
\textbf{margin} & \textbf{space} \\
\hline 
top &  3.0cm\\ 

bottom & 3.0cm \\ 
 
left (inside) & 2.5cm \\ 

right (outside) & 2.5cm \\ 

binding offset & 1.0cm \\ 
\hline 
\end{tabular} 
\end{table}

\section{Typeface and Font Sizes}
The fonts to use for the reports are \textbf{TeX Gyre Termes} (a \textbf{Times New Roman} clone) for serif fonts, \textsf{\textbf{TeX Gyre Heros}} (a \textsf{\textbf{Helvetica}} clone) for sans-serif fonts, and finally \texttt{\textbf{TeX Gyre Cursor}} (a \texttt{\textbf{Courier}} clone) as mono-space font. All these fonts are included with the TeXLive 2013 installation. Table \ref{tab:fonts} lists the most important text elements and the associated fonts.
\begin{table}[!hbt]
\caption{Font types, faces and sizes to be used.}\label{tab:fonts}

 \begin{tabular}{ l c c c}
\hline 
\textbf{Element} & \textbf{Face} & \textbf{Size}  & \textbf{\LaTeX size}  \\ 
\hline 
{\huge \textbf{Ch. label}} & {\huge \textbf{serif, bold}} & \thefontsize\huge & \verb+\huge+ \\ 
{\Huge \textbf{Chapter}} & {\Huge \textbf{serif, bold}} & \thefontsize\Huge & \verb+\Huge+ \\ 
{\LARGE \textsf{\textbf{Section}}} & {\Large \textsf{\textbf{sans-serif, bold}}} & \thefontsize\LARGE &  \verb+\LARGE+  \\ 
{\Large \textsf{\textbf{Subsection}}} & {\Large \textsf{\textbf{sans-serif, bold}}} & \thefontsize\Large & \verb+\Large+ \\ 
{\large \textsf{\textbf{Subsubsection}}} & {\Large \textsf{\textbf{sans-serif, bold}}} & \thefontsize\large &  \verb+\large+ \\ 
Body & serif & \thefontsize\normalsize & {\footnotesize \verb+\normalsize+} \\
%{\footnotesize Footnote} & serif  & \thefontsize\footnotesize & {\footnotesize \verb+\footnotesize+} \\
{\footnotesize \textsc{Header}} & {\footnotesize \textsc{serif, SmallCaps}} & \thefontsize\footnotesize &  \\
Footer (page numbers) & serif, regular & \thefontsize\normalsize &  \\
\hline
\textbf{Figure label} & \textbf{serif, bold} & \thefontsize\normalsize & \\
Figure caption & serif, regular & \thefontsize\normalsize & \\
\textsf{In figure} & \textsf{sans-serif} & \textit{any} & \\
\textbf{Table label} & \textbf{serif, bold} & \thefontsize\normalsize & \\
Table caption and text & serif, regular & \thefontsize\normalsize & \\
\texttt{Listings} & \texttt{mono-space} & $\le$ \thefontsize\normalsize & \\
\hline 
\end{tabular} 
\end{table}

\subsection{Headers and Footers}
Note that the page headers are aligned towards the outside of the page (right on the right-hand page, left on the left-hand page) and they contain the section title on the right and the chapter title on the left respectively, in \textsc{SmallCaps}. The footers contain only page numbers on the exterior of the page, aligned right or left depending on the page. The lines used to delimit the headers and footers from the rest of the page are $0.4 pt$ thick, and are as long as the text.

\subsection{Chapters, Sections, Paragraphs}
Chapter, section, subsection, etc. names are all left aligned, and numbered as in this document. 

Chapters always start on the right-hand page, with the label and title separated from the rest of the text by a $0.4 pt$ thick line.

Paragraphs are justified (left and right), using single line spacing. Note that the first paragraph of a chapter, section, etc. is not indented, while the following are indented.

\subsection{Tables}
Table captions should be located above the table, justified, and spaced 2.0cm from left and right (important for very long captions). Tables should be numbered, but the numbering is up to you, and could be, for instance:
\begin{itemize}
\item \textbf{Table X.Y} where X is the chapter number and Y is the table number within that chapter. (This is the default in \LaTeX. More on {\LaTeX} can be found on-line, including whole books, such as \cite{goossens93}.) or
\item \textbf{Table Y} where Y is the table number within the whole report
\end{itemize}
As a recommendation, use regular paragraph text in the tables, bold headings and avoid vertical lines (see Table \ref{tab:fonts}). 

\subsection{Figures}
Figure labels, numbering, and captions should be formed similarly to tables. As a recommendation, use vector graphics in figures (Figure \ref{fig:vectorg}), rather than bitmaps (Figure \ref{fig:rasterg}). Text within figures usually looks better with sans-serif fonts.
\begin{figure}[!hbt]
\centering
\includegraphics[scale=2.5]{images/examplepic1.pdf} 
\caption{A PDF vector graphics figure. Notice the numbering and placement of the caption. The caption text is indented 2.0cm from both left and right text margin.}\label{fig:vectorg}
\end{figure}

\begin{figure}[!hbt]
\centering
\includegraphics[scale=2.5]{images/examplepic3.jpg} 
\caption{A JPEG bitmap figure. Notice the bad quality of such an image when scaling it. Sometimes bitmap images are unavoidable, such as for screen dumps.}\label{fig:rasterg}
\end{figure}
For those interested in delving deeper into the design of graphical information display, please refer to books such as \cite{Tufte:1986, few2012show}.

\section{Mathematical Formulae and Equations}
You are free to use in-text equations and formulae, usually in \textit{italic serif} font. For instance: $S = \sum_i a_i$. We recommend using numbered equations when you do need to refer to the specific equations:
\begin{equation}
E = \int_0^{\delta} P(t) dt \quad \longleftrightarrow \quad E = m c^2
\end{equation}
The numbering system for equations should be similar to that used for tables and figures.

\section{References}
Your references should be gathered in a \textbf{References} section, located at the end of the document (before \textbf{Appendices}). We recommend using number style references, ordered as appearing in the document or alphabetically. Have a look at the references in this template in order to figure out the style, fonts and fields. Web references are acceptable (with restraint) as long as you specify the date you accessed the given link \cite{fontspec, CTAN}. You may of course use URLs directly in the document, using mono-space font, i.e. \url{http://cs.lth.se/}.

\section{Colours}
As a general rule, all theses are printed in black-and-white, with the exception of selected parts in selected theses that need to display colour images essential to describing the thesis outcome (\textit{computer graphics}, for instance).

A strong requirement is for using \textbf{black text on white background} in your document's main text. Otherwise we do encourage using colours in your figures, or other elements (i.e. the colour marking internal and external references) that would make the document more readable on screen. You may also emphasize table rows, columns, cells, or headers using white text on black background, or black text on light grey background.

Finally, note that the document should look good in black-and-white print. Colours are often rendered using monochrome textures in print, which makes them look different from on screen versions. This means that you should choose your colours wisely, and even opt for black-and-white textures when the distinction between colours is hard to make in print. The best way to check how your document looks, is to print out a copy yourself.

\chapter{Language}

You are strongly encouraged to write your report in English, for two reasons. First, it will improve your use of English language. Second, it will increase visibility for you, the author, as well as for the Department of Computer Science, and for your host company (if any).

However, note that your examiner (and supervisors) are not there to provide you with extensive language feedback. We recommend that you check the language used in your report in several ways:
\begin{description}
\item[Reference books] dedicated to language issues can be very useful. \cite{heffernan2000writing} 
\item[Spelling and grammar checkers] which are usually available in the commonly used text editing environments.
\item[Colleagues and friends] willing to provide feedback your writing.
\item[Studieverkstaden] is a university level workshop, that can help you with language related problems (see \href{http://www.lu.se/studera/livet-som-student/service-och-stod/studieverkstaden}{Studieverkstaden}'s web page).
\item[Websites] useful for detecting language errors or strange expressions, such as
\begin{itemize}
\item \url{http://translate.google.com}
\item \url{http://www.gingersoftware.com/grammarcheck/}
\end{itemize}
\end{description}

\section{Style Elements}
Next, we will just give some rough guidelines for good style in a report written in English. Your supervisor and examiner as well as the aforementioned \textbf{Studieverkstad} might have a different  take on these, so we recommend you follow their advice whenever in doubt. If you want a reference to a short style guide, have a look at \cite{shortstyleguide}.

\subsubsection{Widows and Orphans}

Avoid \textit{widows} and \textit{orphans}, namely words or short lines at the beginning or end of a paragraph, which are left dangling at the top or bottom of a column, separated from the rest of the paragraph.

\subsubsection{Footnotes}

We strongly recommend you avoid footnotes. To quote from \cite{OGSW}, \textit{Footnotes are frequently misused by containing information which should either be placed in the text or excluded altogether. They should be avoided as a general rule and are acceptable only in exceptional cases when incorporation of their content in the text  [is] not possible.} 

\subsubsection{Active vs. Passive Voice}

Generally active voice (\textit{I ate this apple.}) is easier to understand than passive voice (\textit{This apple has been eaten (by me).}) In passive voice sentences the actor carrying out the action is often forgotten, which makes the reader wonder who actually performed the action. In a report is important to be clear about who carried out the work. Therefore we recommend to use active voice, and preferably the plural form \textit{we} instead of \textit{I} (even in single author reports).

\subsubsection{Long and Short Sentences}
A nice brief list of sentence problems and solutions is given in \cite{yalesentences}. Using choppy sentences (too short) is a common problem of many students. The opposite, using too long sentences, occurs less often, in our experience.

\subsubsection{Subject-Predicate Agreement}
A common problem of native Swedish speakers is getting the subject-predicate (verb) agreement right in sentences. Note that a verb must agree in person and number with its subject. As a rough tip, if you have subject ending in \textit{s} (plural), the predicate should not, and the other way around. Hence, \textit{only one s}. Examples follow:
\begin{description}
\item[incorrect] He have to take this road.
\item[correct] He has to take this road.
\end{description}
\begin{description}
\item[incorrect] These words forms a sentence.
\item[correct] These words form a sentence.
\end{description}
\noindent In more complex sentences, getting the agreement right is trickier. A brief guide is given in  the \textit{20 Rules of Subject Verb Agreement} \cite{subjectverb}.

\chapter{Structure}
It is a good idea to discuss the structure of the report with your supervisor rather early in your writing. Given next is a generic structure that is a starting point, but by no means the absolute standard. Your supervisor should provide a better structure for the specific field you are writing your thesis in. Note also that the naming of the chapters is not compulsory, but may be a helpful guideline.
\begin{description}
\item[Introduction] should give the background of your work. Important parts to cover:
\begin{itemize}
\item Give the context of your work, have a short introduction to the area.
\item Define the problem you are solving (or trying to solve).
\item Specify your contributions. What does this particular work/report bring to the research are or to the body of knowledge? How is the work divided between the co-authors? (This part is essential to pinpoint individual work. For theses with two authors, it is compulsory to identify which author has contributed with which part, both with respect to the work and the report.)
\item Describe related work (literature study). Besides listing other work in the area, mention how is it related or relevant to your work. The tradition in some research area is to place this part at the end of the report (check with your supervisor).
\end{itemize}
\item[Approach] should contain a description of your solution(s), with all the theoretical background needed. On occasion this is replaced by a subset or all of the following:
\begin{itemize}
\item \textbf{Method}: describe how you go about solving the problem you defined. Also how do you show/prove that your solution actually works, and how well does it work.
\item \textbf{Theory}: should contain the theoretical background needed to understand your work, if necessary.
\item \textbf{Implementation}: if your work involved building an artefact/implementation, give the details here. Note, that this should not, as a rule, be a chronological description of your efforts, but a view of the result. There is a place for insights and lamentation later on in the report, in the Discussion section.
\end{itemize}
\item[Evaluation] is the part where you present the finds. Depending on the area this part contains a subset or all of the following: 
\begin{itemize}
\item \textbf{Experimental Setup} should describe the details of the method used to evaluate your solution(s)/approach. Sometimes this is already addressed in the \textbf{Method}, sometimes this part replaces \textbf{Method}.
\item \textbf{Results} contains the data (as tables, graphs) obtained via experiments  (benchmarking, polls, interviews).
\item \textbf{Discussion} allows for a longer discussion and interpretation of the results from the evaluation, including extrapolations and/or expected impact. This might also be a good place to describe your positive and negative experiences related to the work you carried out.
\end{itemize} 
Occasionally these sections are intermingled, if this allows for a better presentation of your work. However, try to distinguish between measurements or hard data (results) and extrapolations, interpretations, or speculations (discussion).
\item[Conclusions] should summarize your findings and possible improvements or recommendations.
\item[Bibliography] is a must in a scientific report. {\LaTeX} and \texttt{bibtex} offer great support for  handling references and automatically generating bibliographies.
\item[Appendices] should contain lengthy details of the experimental setup, mathematical proofs, code download information, and shorter code snippets. Avoid longer code listings. Source code should rather be made available for download on a website or on-line repository of your choosing.

\end{description}
\makebibliography{MyMSc}

\begin{appendices}
\chapter{About This Document}
The following environments and tools were used to create this document:
\begin{itemize}
\item operating system: Mac OS X 10.10.1
\item tex distribution: MacTeX-2014, \url{http://www.tug.org/mactex/}
\item tex editor: Texmaker 4.4.1 for Mac, \url{http://www.xm1math.net/texmaker/} for its XeLaTeX flow (recommended) or pdfLaTeX flow
\item bibtex editor: BibDesk 1.6.3 for Mac, \url{http://bibdesk.sourceforge.net/}
\item fonts \texttt{cslthse-msc.cls} document class): 
\begin{description}
\item{for XeLaTeX}: TeX Gyre Termes, \textsf{TeX Gyre Heros}, \texttt{TeX Gyre Cursor} (installed from the TeXLive 2013)
\item{for pdfLaTeX}: TeX Gyre font packages: tgtermes.sty, tgheros.sty, tgcursor.sty, gtxmath.sty (available through TeXLive 2013) 
\end{description} 
\item picture editor: OmniGraffle Professional 5.4.2
\end{itemize}

\noindent A list of the essential \LaTeX packages needed to compile this document follows (all except \texttt{hyperref} are included in the document class):
\begin{itemize}
\item \texttt{fontspec}, to access local fonts, needs the XeLaTeX flow
\item \texttt{geometry}, for page layout
\item \texttt{titling}, for formatting the title page
\item \texttt{fancyhdr}, for custom headers and footers
\item \texttt{abstract}, for customizing the abstract
\item \texttt{titlesec}, for custom chapters, sections, etc.
\item \texttt{caption}, for custom tables and figure captions
\item \texttt{hyperref}, for producing PDF with hyperlinks
\item \texttt{appendix}, for appendices
\item \texttt{printlen}, for printing text sizes
\item \texttt{textcomp}, for text companion fonts (e.g. bullet)
\end{itemize}

\noindent Other useful packages:
\begin{itemize}
\item \texttt{listings}, for producing code listings with syntax colouring and line numbers
\end{itemize}

\chapter{List of Changes}
\subsubsection{Since 2015/04/27}
\begin{itemize}
\item Improved the \textbf{Structure} chapter and added more detailed comments for each part.
\end{itemize}

\subsubsection{Since 2014/02/18}
\begin{itemize}
\item Added the possibility to specify two supervisors. Use either of the \verb+\supervisor{}+ or \verb+\supervisors{}{}+ commands to set the names and contacts on the first page.
\end{itemize}

\subsubsection{Since 2013/09/23}
\begin{itemize}
\item Added missing colon ":" after \textit{Examiner} on the front page. 
\end{itemize}

\subsubsection{Since 2013/08/30}
\begin{itemize}
\item Changed fonts from Garamond (Times New Roman), Helvetica (Arial), Courier (Source Code Pro) to Tex Gyre fonts, namely Termes, Heros, Cursor, which are freely available with TexLive 2013 installation. These are all clones of Times New Roman, Helvetica and Courier, respectively. Garamond is problematic on some systems, being a non-freely available font.
\item Corrected the \textit{Face} column in Table \ref{tab:fonts} to correctly depict the font face.
\end{itemize}

\subsubsection{Since 2013/02/22}
\begin{itemize}
\item Number of words required in the abstract changed to 150 (from 300).
\end{itemize}

\subsubsection{Since 2013/02/15}
\begin{itemize}
\item Made a separate document class, for clarity.
\item made it work with pdfLaTeX and garamond.sty, in addition to XeLaTeX and true type fonts. It is up to the user to get the hold of the garamond.zip from \url{http://gael-varoquaux.info/computers/garamond/index.html}.
\end{itemize}
\end{appendices}

\fi

\end{document}