\documentclass[11pt,twoside]{eitExjobb}
%%\documentclass[11pt,twoside,final]{eitExjobb}
% Use final for the final version that will be printed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\usepackage[Text,Num]{LUfonts}%% LU fonts (local file)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Nicer fonts
%\usepackage{pxfonts}
%\usepackage{mathpazo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ÅÄÖ
\usepackage[T1]{fontenc}
%% Packages used in the thesis
\usepackage{color}
\usepackage{graphpap}
\unitlength=1mm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{microtype}
\usepackage{algorithm, algorithmicx}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{cite}

\usepackage{amsmath}

\crefname{app}{Appendix}{Appendices}
\crefname{procedure}{Algorithm}{Procedure}

\crefname{exp}{experiment}{experiment}
\Crefname{exp}{Experiment}{Experiment}

\newtheorem{definition}{Definition}[chapter]
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

\renewcommand{\algorithmicrequire}{\textbf{Precondition:}}
\renewcommand{\algorithmicensure}{\textbf{Input:}}

\floatstyle{ruled}
\newfloat{function}{htbp}{lok}
\floatname{function}{Function}

\crefname{function}{function}{functions}
\Crefname{function}{Function}{Functions}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\newcommand{\loslabel}[1]{\makebox[3cm][l]{\textbf{#1}\ }}
\newenvironment{listofsymbols}{\begin{list}{}{\renewcommand{\makelabel}{\loslabel}}}{\end{list}}

\newcommand{\abbrlabel}[1]{\makebox[3cm][l]{\textbf{#1}\ }}
\newenvironment{abbreviations}{\begin{list}{}{\renewcommand{\makelabel}{\abbrlabel}}}{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%\supervisor{Björn Landfeldt, \href{mailto:bjorn.landfeldt@eit.lth.se}{\texttt{bjorn.landfeldt@eit.lth.se}}}{}
%\examiner{Christian Nyberg, \href{mailto:christian.nyberg@eit.lth.se}{\texttt{christian.nyberg@eit.lth.se}}}

%%% Title page
\Title{Dynamic Fault Tolerance and Task Scheduling in Distributed Systems}
\Author{{Philip Ståhl\\\texttt{ada10pst@student.lu.se}} \\
		Jonatan Broberg\\\texttt{elt11jbr@student.lu.se}}
\Date{\today}
\Advisor{Stefan Höst}
\Company{Mobile and Pervasive Computing Institute (MAPCI)\\Lund University}
\MakeTitlePage
%%%%% Page numbering for front pages
\frontmatter

%%%%% Abstract
\chapter*{Abstract}
Ensuring a predefined level of reliability for applications running in distributed environments is a complex task. Having multiple identical copies of a task increases redundancy and thereby the reliability. Due to varying properties of the often heterogeneous and vast number of components used in distributed environments, using static analysis of the environment to determine how many copies are needed to reach a certain level of reliability is insufficient. Instead, the system should dynamically adapt the number of copies as the properties of the system changes. In this thesis, we present a dynamic fault tolerant model and task scheduling, which ensures a predefined reliability by replicating tasks. Reliability is ensured over time by detecting failures, and dynamically creating new copies. Furthermore, the resources used are kept to a minimum by using the optimal number of task copies. Finally, the model was implemented using Ericsson's IoT-environment Calvin, thus providing a platform which can be used for further research and experiments.

%% ToC
\tableofcontents
%\cleardoublepage
\listoffigures
\listoftables

\chapter*{List of Symbols}
\begin{listofsymbols}
\item[$\lambda$] Failure rate
\item[$\sigma$] Standard deviation
\item[$\mu$] Mean value
\item[$f(t)$] Probability of a replica failing within time $t$
\item[$F(t)$] Probability of a node failing within time $t$
\item[$P(k)$] Probability of k failures
\item[$R(t)$] Probability of not experience a failure within a time $t$
\item[$R_{\textrm{req}}$] Required reliability
\item[$t$] Time from a failure happening to the system is restored
\item[$t_{\textrm{create}}$] Time for creating a new replica
\item[$t_{\textrm{d}}$] Time to detect a failure
\item[$t_{\textrm{de-serialize state}}$] Time for de-serializing a task's state
\item[$t_{\textrm{get state}}$] Time to get a task's state 
\item[$t_{\textrm{h}}$] Time between sending heartbeats
\item[$t_{\textrm{R}}$] Total time for having a new replica operational
\item[$t_{\textrm{r}}$] Time for replicating a task to another node
\item[$t_{\textrm{replication msg}}$] Time for sending all information needed for starting a new replica
\item[$t_{\textrm{response}}$] Time for sending a response
\item[$t_{\textrm{send reply}}$] Time for sending a reply
\item[$t_{\textrm{serialize state}}$] Time for serializing a task's state
\item[$t_{\textrm{timeout}}$] Timeout time for listening for heartbeats
\item[$t_{\textrm{transmit state}}$] Time for transmitting a task's state
\item[$t_{\textrm{query storage}}$] Time for querying the storage and receiving a response
\end{listofsymbols}

\chapter*{Abbreviations}
\begin{abbreviations}
\item[CHDS] Centralized Heterogeneous Distributed Systems
\item[DHT] Distributed Hash Table
\item[HDCS] Heterogeneous Distributed Computing Systems
\item[IoT] Internet of Things
\item[MAS] Multi-Agent System
\item[MTBF] Mean Time Between Failures
\item[MTTF] Mean Time To Failure
\item[RMS] Resource Management System
\item[RSD] Relative Standard Deviation
\item[TCP] Transmission Control Protocol
\item[UDP] User Datagram Protocol
\item[VM] Virtual Machine
\end{abbreviations}


\cleardoublepage
%%%%% Page numbering for the main thesis
\mainmatter
%%%%%

\chapter{Introduction} \label{ch:introduction} 
\section{Background and Motivation} \label{sec:introduction_backgroud_motivation}
Ensuring a predefined level of reliability is of major concern for many cloud service providers. Cloud computing, and distributed computing in general, is growing rapidly, and users demand more services and higher availability and reliability. Therefore, providing fault tolerant systems and improving reliability by more sophisticated task scheduling algorithms, have become very important and gained plenty of research interest.

Distributed systems often consist of heterogeneous hardware and any of the, often vast number of, components may fail at any time. Consequently, as computational work is distributed across multiple resources, the overall reliability of the application decreases. To cope with this issue, a fault tolerant design or error handling mechanism needs to be in place. Ensuring reliability raises complexity to the resource allocation decisions and fault tolerance mechanisms in highly dynamic distributed systems. For cloud service providers, it is necessary that this increased complexity is taken care of without putting extra burden on the user. The system should therefore ensure these properties in a seamless way.

In some cases, vendors, such as carrier providers, are obliged by law to achieve a specific level of availability or reliability. In such cases, quality aspects such as latency and resource usage may need to be sacrificed to reach the required level. By using static and dynamic analysis of the infrastructure and the mean-time-between-failure for the available resources, a statistical model can be created to verify that the required level is reached. However, due to the dynamic properties of distributed environments, such a model must also be dynamic. 

Furthermore, fault tolerance is of particular importance for long running applications or services where a predefined level of reliability is required. Despite fulfilling the required level at the time of deployment, as the state of the system changes, the required reliability may no longer be fulfilled, and actions must be taken to still reach the required level.

One way of increasing the reliability is by replicating application tasks, i.e. creating identical copies. Having identical copies, also called replicas, results in increased redundancy, and allows for continued execution of the application, despite the event of failure of a task replica. Seamlessly being able to continue the execution or computations performed, without losing any data is of particular interest for stream processing applications.

A drawback of replication is the extra resources needed. If replicating a task \emph{n} times with every replica performing the same computation on the same input, $n$ times as many resources are needed, hence a great deal of computational resources are wasted. Dynamic analysis of the system and an adaptive scheduling technique could help in determining on which resources replicas should be placed for optimal resource usage and load balancing.

In this master thesis, a model with dynamic fault tolerance is presented, which ensures the required reliability is met by replicating tasks. By dynamically adapting to changing properties of the system and available resources, it also ensures that the optimal number of replicas are used.

\section{Related work} \label{sec:introduction_related_work}
The interest in reliability for distributed systems has gained increased attention recently~\cite{replicationManagement}. Due to that heterogeneous resources often compose distributed environments such as cloud and grid systems, ensuring reliability is a complex task~\cite{cloudServiceRel, surveyReliabilityDistr}.

A large number of scheduling techniques have been designed, aiming at maximizing reliability of computational jobs in distributed environments under various constraints such as meeting task deadlines or minimizing execution time~\cite{algoOptTimeMaxRel, optTaskAllocationForMaxRel, taskAllocation, taskAllocationSwarm, algoMaxRelEndToEndConstraint, algoMinExTime, schedReplicas}. Maximizing reliability is for these algorithms a secondary concern, while meeting the constraints is the primary. In contrast, \cite{optResourceAllMaxPerformance, matchSchedAlgoMinFailure, safetyRelTaskAllocation, improvedTaskAllMaxRel} have developed models which focuses on increasing the reliability. Common for these scheduling techniques are that while they try to maximize reliability, they do not ensure that a certain level of reliability is met. Furthermore, the algorithms are usually static in the way that they do not account for the dynamic behavior of distributed systems. In addition, they make assumptions such as known execution times of tasks, which make them unsuitable for long running applications or services without a known execution time.

Plenty of work has been done in the area of designing fault tolerant systems by using checkpoint/restart techniques \cite{adaptiveCheckPointAndRep, IEEEfaultTolerantSys, faultTolerantDeadlock}. These techniques rely on the notion of a stable storage, such as a hard-drive, which is persistent even in the case of a system failure. Checkpoint techniques are usually employed for applications or jobs where the computations executed take a very long time. For such computations, a substantial amount of computational resources are wasted if the job has to redo all computations in case of a failure.

Some previous attempts at designing fault tolerant systems by the use of replication have been made~\cite{designFaultTolerantSched, evalReplicationSched, taskSchedulingReplication, effTaskReplMobGrid, relGridServicePredConstraint}. In \cite{evalReplicationSched}, a static number of replicas is used for every application being deployed. Furthermore, they do not guarantee that all replicas are deployed, instead they use a best-effort approach, where replicas are deployed only if resources are available. In contrast to~\cite{evalReplicationSched}, \cite{ effTaskReplMobGrid, taskSchedulingReplication, designFaultTolerantSched} dynamically determine the number of replicas based on the state of the system. However, they are all static in that failed replicas are not restarted, and do therefore not ensure the reliability is met over time. Furthermore, while~\cite{designFaultTolerantSched} dynamically determines the number of replicas to use, the scheduling decisions on which resources to put the replicas is done after the number of replicas has been determined. As the reliability vary between resources in a heterogeneous environment, the number of replicas needed depends on which resources that are used. Therefore, determining the number of replicas needed and where to place them should be a joint process.

A quite old but still relevant work is found in~\cite{dynAdaptRepl} in which a framework for dynamically replicating tasks in a Multi-Agent Systems (MAS) is presented. The authors introduce a software architecture which can act as support for building reliable MAS. Since the available resources often are limited, they say that it is not feasible to replicate all components. Therefore, they introduce the concept of criticality for each agent, which is allowed to evolve during runtime. An agent's criticality is calculated using the CPU usage and communication activity of the agent, and is used to determine the number of replicas, along with a predefined minimum number of replicas. The proposed solution also allows for dynamically adapting the number of replicas and the replication strategy itself (passive/active) in order to maximize the reliability of the agents based on available resources. 

Other approaches to improve reliability in MAS by the use of replication are presented in~\cite{replicatingAgents, adaptiveMASReplication, adaptiveAgentReplication}. While being adaptive to system state, the solution presented in~\cite{replicatingAgents} still faces the problem of having a single-point-of-failure due to a communication proxy. This problem is avoided in~\cite{adaptiveMASReplication}, where a decentralized solution is proposed, and where the number of replicas and their placement depends on the system state. However, instead of ensuring a given level of reliability is met, they aim at maximizing the reliability and availability based on available resources.

A dynamic and adaptive algorithm, which dynamically varies the number of replicas depending on system load is presented in~\cite{adaptiveCheckPointAndRep}. The proposed algorithm does not ensure a certain reliability. Instead, it reduces the number of replicas during peak hours, in order to reduce system load. Since the reliability of a system decreases during higher load~\cite{studyOfFailures, implicationsOfFailures}, the number of replicas should be increased instead of decreased in order to stay above the required level of reliability.

A fault tolerant scheduling technique incorporating a replication scheme is presented in~\cite{faultTolerantSchedPolicy}. While being dynamic in that failed replicas are restarted, it is static in that the user defines the number of replicas to use, hence it does not ensure a specific level of reliability is met.

The techniques used in~\cite{selfAdaptRel, dynAdaptRepl, relModelWebServices} are more dynamic and adaptive to the dynamic behavior of distributed systems. However, reliability is defined as producing the correct result, and is achieved by techniques like \emph{majority voting} and \emph{k-modular redundancy}. An adaptive approach, which adapts to changes in the execution environment is presented in~\cite{imprRelAdaptRL}. They present an adaptive scheduling model based on reinforcement learning, aiming at increasing the reliability. However, they assume that a task's profile, including execution time, is available.

\section{Our contributions} \label{sec:introduction_contributions}
To our knowledge, no previous attempt has been made which in a fully dynamic manner ensures a predefined level of reliability for long running applications or services. Some previous work dynamically calculates the number of replicas, but are static in that failed replicas are not restarted, while others use a static number of replicas, and dynamically restart failed ones.

We propose a framework which ensures a user determined level of reliability by the use of replication. Furthermore, the method ensures a minimized use of resources by not using more replicas than needed, and by minimizing the number of resources needed. This is achieved by placing replicas on the most reliable resources first and foremost. Finally, the system is periodically monitored in order to adapt to changing system properties.

The framework is not limited to a specific type of distributed environment, nor the reliability model used. The reliability model and the task placement decisions are easily replaced to consider more parameters, or to include load balancing. 

Our model is implemented using the actor-based application environment \emph{Calvin} \cite{calvin}, developed by Ericsson. While \emph{Calvin} is mainly an environment for Internet of Things (IoT) applications, it suites our purpose well. The model is evaluated by running a set of experiments in a small cluster. Furthermore, the implementation provides a platform for further research and experiments.

\section{Goal} \label{sec:introduction_goals}
The goal of this thesis was to devise a method for dynamically ensuring a predefined level of reliability for distributed applications or services by dynamically replicating tasks. The goal was further to implement the method and provide a flexible and extensible platform, which can be used for further research and experiments.

First, a reliability model was designed, describing the reliability of the available resources, and for applications using these resources.

Secondly, a framework was designed which automatically detects node failures and based on the reliability of the available resources creates enough replicas to reach the required reliability level. Furthermore, the system and its running applications are periodically monitored in order to adapt the resources' reliability and the number of replicas needed as the properties of the system vary over time.

Lastly, the model was implemented and tested using the IoT application framework \emph{Calvin}.

The report is structured as follows: in \cref{ch:background} all necessary background theory is provided, in \cref{ch:design} we present our model and contribution in more detail, \cref{ch:evaluation} presents an evaluation of our solution and in \cref{ch:discussion} the solution is discussed. \Cref{ch:future_work} presents future work and \cref{ch:conclusions} concludes the report.

\chapter{Background} \label{ch:background}
In this chapter we provide all the necessary background theory to fully understand the rest of the report.
\section{Computational Environment} \label{sec:background_comp_env}
The computational environment used in this thesis is distributed computing, i.e. several resources working together towards a common goal.

\subsection{Types of distributed computing} \label{subsec:background_types_of_distr_comp}
Distributed Computing Systems (DCS) are composed of a number of components or subsystems interconnected via an arbitrary communication network~\cite{relModelDistSimSystem, efficientRelAnalysisAlgo}. There are a number of different types of distributed environments, e.g. grids, clusters, clouds and heterogeneous distributed computing systems.

\subsubsection{Grid computing}
A grid is a collection of autonomous resources, that are distributed geographically and across several administrative domains, and work together to achieve a common goal, i.e. to solve a single task~\cite{compStudyLoadAndCloud, relAndPerfGridServices, evalOfGridRel}.

Each domain in a grid usually has a centralized scheduling service called Resource Management System (RMS) which accepts job execution requests and sends the jobs' tasks to the different resources for execution~\cite{evalOfGridRel}.

The reliability of the grid computing is very critical but hard to analyze due to its characteristics of massive-scale service sharing, wide-area network, heterogeneous software/hardware components and complicated interactions among them~\cite{cloudServiceRel}.

\subsubsection{Cluster}
A cluster system is usually a number of identical units managed by a central manager. It is similar to a grid, but differ in that resources are geographically located at the same place. The resources work in parallel under supervision of a single administrative domain. From the outside it looks like a single computing resource~\cite{compStudyLoadAndCloud}.

\subsubsection{Cloud}
Cloud has been described as the next generation of grids and clusters. While it is similar to clusters, the main difference is that cloud consists of multiple domains~\cite{compStudyLoadAndCloud}. The domains can be geographically distributed, and software and hardware components are often heterogeneous. Therefore, analyzing and predicting workload and reliability are usually very challenging~\cite{surveyReliabilityDistr}.

\subsubsection{Heterogeneous distributed computing systems}
A Heterogeneous Distributed Computing System (HDCS), is a system of numerous high-performance machines connected in a high-speed network. Therefore, high-speed processing by computational heavy applications is possible~\cite{algoMinExTime}. %TODO add info

The majority of distributed service systems can be viewed as a Centralized Heterogeneous Distributed System (CHDS). A CHDS consists of heterogeneous sub-systems which are managed by a centralized control center. The sub-systems have various operating platforms and are connected in diverse topological networks~\cite{studyServiceRel}.

\subsection{Dynamic versus static environments} \label{subsec:background_dyn_stat_env}
A distributed computing environment can be either static or dynamic. In a static environment, only homogenous resources are usually installed~\cite{compStudyLoadAndCloud}. For load-balancing and scheduling algorithms, prior knowledge of node capacity, processing power, memory, performance and statistics of user requirements are required. Changes in load during runtime are not taken into account which makes the environment easy to simulate but not well suited for heterogeneous resources. Once the system has been put into place, resources are neither added nor removed from the system.

In contrast to static environments, a dynamic environment usually consists of heterogeneous resources~\cite{compStudyLoadAndCloud}. Once the system has been put into place, resources can be dynamically added or removed. In such environments, prior knowledge is therefore not enough for load-balancing and scheduling algorithms, since the requirements of the user and the available resources can change during runtime. Runtime statistics are therefore usually collected and taken into account. Dynamic environments are difficult to simulate, but algorithms exist which easily adopt to runtime changes. 

\section{Faults in distributed environments} \label{sec:background_faults_distr_env}
In distributed environments, a large number of faults can occur due to its complexity. Therefore, when modeling faults and reliability in such environments, a fault model is usually employed, describing which kinds of failures that are being considered.

\subsection{Types of Faults} \label{subsec:background_types_of_faults}
A fault is usually used to describe a defect at the lowest level of abstraction~\cite{faultTolerantFundamentals}. A fault may cause an error which in turn may lead to a failure, which is when a system has not behaved according to its specification.

In distributed environments, especially with heterogeneous commodity hardware, several types of failures can take place, which affect the running applications. Such failures include, but are not limited to, overflow, timeout, resource missing, network, hardware, software, and database failure~\cite{cloudServiceRel}. Failures are usually considered to be either~\cite{evalOfGridRel}:

\begin{itemize}
	\item Job related
	\item System related
	\item Network related.
\end{itemize}

In~\cite{studyOfFailures}, almost ten years of real-world failure data of 22 high performance computing systems was studied and concluded hardware failures to be the single most common type of failure, ranging from 30 to more than 70 percent depending on hardware type, while 10 to 20 percent of the failures were software failures.


\subsection{Fault models} \label{subsec:background_fault_models}
When studying the reliability of distributed systems or applications running in distributed computing environments, one usually starts with specifying which fault model that is used. A reliability model is then designed, and validated with respect to this fault model~\cite{faultTolerantFundamentals}. In the fault models described in this section, and in the rest of the report, a computational resource is referred to as a node.

\subsubsection{Byzantine fault model} \label{subsub:background_byzantine}
The Byzantine fault model allows nodes to continue interaction after failure. Correctly functioning nodes cannot automatically detect if a failure has occurred. Even if it was known that a failure occurred, nodes cannot detect which node that has failed. 

Furthermore, the system's behavior can be inconsistent and arbitrary~\cite{surveyFaultParallel}. Nodes can fail (become Byzantine) at any point of time and stop being Byzantine at any time. A Byzantine node can send no response at all, or it can send an incorrect result. All Byzantine nodes might send the same incorrect result, thereby making it hard to identify malicious nodes~\cite{selfAdaptRel}. The Byzantine fault model is very broad since it allows failed nodes to continue interacting, but it is therefore also very difficult to simulate and to analyze.

\subsubsection{Fail-stop fault model} \label{subsub:background_fail_stop}
The fail-stop model, also called the crash-stop model, is in comparison to the Byzantine fault model much simpler. When a node fails it stops producing any output and stops interacting with the other nodes~\cite{faultTolerantFundamentals}. This allows for the rest of the system to automatically detect when a node has failed. Due to its simplicity, it does not handle subtle failures such as memory corruption but rather failures such as system crashes~\cite{surveyFaultParallel}.

\subsubsection{Crash-failure fault model}
The crash failure model is quite alike the fail-stop model with the difference that nodes do not automatically detect the failure of a node~\cite{faultTolerantFundamentals, adaptiveAgentReplication}.

\subsubsection{Fail-stutter fault model}
Since the Byzantine model is very broad and complicated and the fail-stop model doesn't represent enough real-world failures, a third middle ground model has been developed. It is an extension of the fail-stop model but differ in that it also allows for performance fault, such as unexpectedly low performance of a node~\cite{surveyFaultParallel}.

\subsection{Failure distribution} \label{subsec:background_failure_distribution}
Models describing the nature of failures in distributed computing environments are usually based on certain assumptions and are usually only valid under those assumptions. Commonly made assumptions about failures and the computational environment are \cite{relModelDistSimSystem, relModelAnalysis, cloudServiceRel, studyServiceRel, hierarchicalRelModeling, selfAdaptRel}:
\begin{itemize}
	\item Each component in the system has only two states: \emph{operational} or \emph{failed}
	\item Failures of components are statistically independent
	\item Components have a constant failure rate, i.e. the failure of a component follows a Poisson process
	\item Fully reliable network
\end{itemize}

When modeling reliability for grid systems, it is common to also assume a fully reliable Resource Management System (RMS)~\cite{relAndPerfGridServices, relGridServicePredConstraint}, which is a non-negligible assumption since the RMS is a single-point-of-failure.

Constant failure rates are not likely to model the actual failure scenario of a dynamic heterogeneous distributed system~\cite{algoMinExTime}. The failure rate for a hardware component often follows a bathtub shaped curve~\cite{surveyReliabilityDistr}. The failure rate is usually higher in the beginning due to that the probability that a manufacture failure would affect the system is higher in the beginning of the system's lifetime. After stabilizing, the failure rate drops and later increases again due to that the component gets worn out.

Statistically independent failures are also not very likely to reflect the real dynamic behavior of distributed systems~\cite{surveyReliabilityDistr, cloudServiceRel}. Faults may propagate throughout the system, thereby affecting other components as well~\cite{relGridSystems}. In grid environments, a sub-system may consist of resources using a common gateway to communicate with the rest of the system. In such a scenario, the resources do not use independent links~\cite{optResourceAllMaxPerformance}. As failures are likely to be correlated~\cite{perfImplPerCheckPoint}, the probability of failure increases with the number of resources a task uses.

Other factors also affect the likelihood of resource failures. Several studies have concluded a relationship between failure rate and the load of the system~\cite{studyOfFailures, implicationsOfFailures}. Furthermore, \cite{studyOfFailures, implicationsOfFailures} also show that failures are more likely to occur during daytime than at night, which may be a consequence of the system load being higher during daytime. In addition, components which have failed in the past are more likely to fail again~\cite{implicationsOfFailures}.

While a Poisson process is commonly used to describe the probabilities of failures, \cite{studyOfFailures} shows that failures are better modelled by a Weibull distribution with a shape parameter of 0.7 - 0.8. However, despite not always reflecting the true dynamic failure behavior of a resource, a Poisson process has been experimentally shown to be reasonably useful in mathematical models~\cite{experimentalFailureAssessment}. The Poisson distribution expresses the probability of $k$ failures during a specific period of time, and is defined as follows:

\begin{equation} \label{eq:Poisson}
P(k \mbox{ failures}) = \dfrac{\lambda^k \cdot e^{-\lambda}}{k!}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}


\end{thebibliography}


%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%
\chapter{Test Appendix}


\end{document}
